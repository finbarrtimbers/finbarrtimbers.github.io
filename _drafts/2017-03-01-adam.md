---
layout: post
title: Adam: A Method for Stochastic Optimization
tags: machine learning statistics papers
---

# [Abstract](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf)

Grid search and manual search are the most widely used strategies for hyper-parameter optimization.
This paper shows empirically and theoretically that randomly chosen trials are more efficient
for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison
with a large previous study that used grid search and manual search to configure neural networks
and deep belief networks. Compared with neural networks configured by a pure grid search,
we find that random search over the same domain is able to find models that are as good or better
within a small fraction of the computation time. Granting random search the same computational
budget, random search finds better models by effectively searching a larger, less promising con-
figuration space. Compared with deep belief networks configured by a thoughtful combination of
manual search and grid search, purely random search over the same 32-dimensional configuration
space found statistically equal performance on four of seven data sets, and superior performance
on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation
set performance reveals that for most data sets only a few of the hyper-parameters really matter,
but that different hyper-parameters are important on different data sets. This phenomenon makes
grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some
light on why recent “High Throughput” methods achieve surprising success—they appear to search
through a large number of hyper-parameters because most hyper-parameters do not matter much.
We anticipate that growing interest in large hierarchical models will place an increasing burden on
techniques for hyper-parameter optimization; this work shows that random search is a natural baseline
against which to judge progress in the development of adaptive (sequential) hyper-parameter
optimization algorithms.

# Summary

THis is extremely important because communication between servers limits scaling
distributed NN training. *Small models train faster as they require less
communication*.

The authors downsize the models by employing three strategies:

1. Replace all 3 x 3 filters with 1 x 1 filters.

2. Decrease the number of input channels to the 3 x 3 filters by using
"squeeze layers."

3. Downsample late in the network so that convolution layers have large
activation maps.

It is obvious how the first two strategies decrease the model size (they
radically decrease the number of weights needed). The last one doesn't---
having large activation maps help the model have high classification accuracy
on a limited "budget" of parameters.

The "squeeze layer" the authors define is a convolution layer made of 1 x 1
filters. The authors use this to create a "Fire module" that is composed of two
layers: a squeeze layer, and an expand layer that has a mix of 1 x 1 and 3 x 3
convolution filters.

SqueezeNet is then composed of a convolution layer (conv1), 8 Fire modules
(fire2-9), and a convolution layer (conv10). SqueezeNet performs max-pooling with
a stride of 2 after layers conv1, fire4, fire8, and conv10, as per strategy 3.

SqueezeNet performs as well as AlexNet at the top-1 and top-5 levels, while
achieving a 50x reduction in size.

The authors then go on to discuss the metaparameters that can be used to create
similar squeeze layers.

# Comments

---
layout: post
title: What is a cluster, actually?
tags: machine-learning
---

If you've been kicking around any sort of machine learning or data centric
community, you've heard "big data" come up, and alongside, a series of crazy
names (Hadoop, Hive, Phoenix, Spark, ZooKeeper, Impala, Flume, Sqoop, etc.).

They're all different frameworks used for distributed computing of BIG DATA,
which means that they're different tools you, the extremely attractive and
intelligent data scientist, can use to process large amounts of data quickly.
Distributed computing means that you can use computer clusters that are build
from commodity hardware.

You've probably heard that phrase a few times before (I think I've heard it
precise 999345 times). But what does it actually mean? I don't know (or at
least, I didn't at the time I was writing the article). Does it mean that I can
treat the cluster as a big version of my laptop and run code the same, or does
it mean that I have to use special software to do so?

Let's find out! I have a machine learning model (BUZZZZZZWORD), written using
Python's [scikit-learn](http://scikit-learn.org/stable/) package. I'm training a
random forest to predict values from the MNIST dataset, which you can grab
[here](http://deeplearning.net/tutorial/gettingstarted.html).

The entirety of the model is here:


It's pretty simple. We load the MNIST dataset, consisting of 60 000 28 x 28
images, and train a random forest model on it. If we train it with

30 trees, we can get an accuracy score of 0.9629 in 15.90s. That's pretty good,
considering I didn't do anything except take the default Random Forest model
from Python and train it. What happens if we want to run it on 300 trees?
300 trees, we can get an accuracy score of 0.9708 in 216.40s.

We're going to use [StarCluster](http://star.mit.edu/cluster/) to create a
cluster of computers that we can run
[IPython.parallel](https://ipython.org/ipython-doc/dev/parallel/parallel_process.html#ipython-on-ec2-with-starcluster)
on AWS EC2. This will let us pretend we're running our code on a massive
computer, which will make the mental model easier to deal with.

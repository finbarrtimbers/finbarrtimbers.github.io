<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>How is LLaMa.cpp possible?</title>
				<description>&lt;p&gt;&lt;em&gt;If you want to read more of my writing, I have a &lt;a href=&quot;https://finbarrtimbers.substack.com/&quot;&gt;Substack&lt;/a&gt;. Articles will be posted simultaneously to both places.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recently, a &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;project&lt;/a&gt; rewrote the &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;LLaMa inference code&lt;/a&gt; in raw C++. With some optimizations and quantizing the weights, this allows running a LLM locally on a wild variety of hardware:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On a &lt;a href=&quot;https://twitter.com/rgerganov/status/1635604465603473408&quot;&gt;Pixel5&lt;/a&gt;, you can run the 7B parameter model at 1 tokens/s.&lt;/li&gt;
  &lt;li&gt;On a &lt;a href=&quot;https://simonwillison.net/2023/Mar/11/llama/&quot;&gt;M2 Macbook Pro&lt;/a&gt;, you can get ~16 tokens/s with the 7B parameter model&lt;/li&gt;
  &lt;li&gt;You can &lt;a href=&quot;https://twitter.com/miolini/status/1634982361757790209&quot;&gt;even run the 7B model on a 4GB RAM Raspberry Pi&lt;/a&gt;, albeit at 0.1 tokens/s.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are like me, you saw this and thought: What? How is this possible? Don’t large models require expensive GPUs? I took my confusion and dove into the math surrounding inference requirements to understand the constraints we’re dealing with.&lt;/p&gt;

&lt;p&gt;Let’s start with GPUs. GPUs have two main benefits for deep learning:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They have a large amount of memory bandwidth (&lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf&quot;&gt;A100&lt;/a&gt;: 1935 GB/s, &lt;a href=&quot;https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf&quot;&gt;4090&lt;/a&gt;: 1008 GB/s)&lt;/li&gt;
  &lt;li&gt;They have a large amount of compute (&lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf&quot;&gt;A100&lt;/a&gt;: 312 TFLOPS of FP16, &lt;a href=&quot;https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf&quot;&gt;4090&lt;/a&gt;: 82.6 TFLOPS of FP16)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we talk about memory bandwidth, we’re talking about how long it takes to move things from the HBM memory (i.e. the RAM) into the on-chip memory. To actually do math with the GPU, we need to move the matrices in question into the on-chip memory, which is quite small (40MB on an A100, compared to 40-80GB of RAM). Note that the memory bandwidth is ~2 orders of magnitude smaller than the compute performance— this will matter later, as the memory bandwidth tends to be the bottleneck for inference.&lt;/p&gt;

&lt;p&gt;What does this mean in the context of serving LLaMa? Let’s start with some &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;inference arithmetic&lt;/a&gt;. We can do some rough calculations on the inference performance of a LLM using &lt;a href=&quot;https://kipp.ly/blog/transformer-param-count/&quot;&gt;Kipply’s article&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. First, some notation on the dimensions of the model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The \(Q\), \(K\), and \(V\) weight matrices are all shape [ \(d_{\text{model}}\), \(d_{\text{head}}\)], and we have \(n_{\text{heads}}\) of them per layer; the attention output matrix has the same shape, for a total of  \(4 \times\) [ \(d_{\text{model}}\), \(n_{\text{heads}} \cdot d_{\text{head}}\)]. By convention, GPT-style networks have \(d_{\text{head}} \cdot n_{\text{heads}} = d_{\text{model}}\).&lt;/li&gt;
  &lt;li&gt;The MLP has two weight matrices, of shape [ \(d_{\text{model}}\), \(4 \cdot d_{\text{model}}\)] and [ \(4\cdot d_{\text{model}}\), \(d_{\text{model}}\)]&lt;/li&gt;
  &lt;li&gt;The embeddings matrix is of size [ \(d_{\text{vocab}}\), \(d_{\text{model}}\)].&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gives us a handy equation for the number of parameters in a GPT-style model:&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

\[P = n_{\text{blocks}} \left( 4 \cdot d_{\text{model}}^2 + 2 \cdot 4 \cdot d_{\text{model}}^2\right) + n_{\text{vocab}} \cdot d_{\text{model}}\]

&lt;p&gt;For the duration of the post, I’m going to focus on the case where we’re running a ChatGPT style service locally, which is what LLaMa.cpp does, letting me assume a batch size of 1.&lt;/p&gt;

&lt;p&gt;For efficient inference, the KV cache has to be stored in memory; the KV cache requires storing the KV values for every layer, which is equal to storing:&lt;/p&gt;

\[n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

&lt;p&gt;I use \(n_{\text{bytes}}\) here to indicate the number of bytes per param; for float32s, this is 4, for float16s, this is 2, etc. The 2 in the middle is because we have to store one set of weights for the K values, and one for the Vs.&lt;/p&gt;

&lt;p&gt;Given a model with n layers, the total memory for the KV cache is:&lt;/p&gt;

\[n_{\text{blocks}} \cdot n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

&lt;p&gt;In addition to storing the KV cache in memory, we also need to store the weights themselves in memory; this requires \(n_{\text{bytes}} \cdot P\) bytes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-memory-weights.png&quot; alt=&quot;Screenshot of table showing the memory required for LLaMa weights&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the advantage of quantization. By using less precision, we can radically decrease the amount of memory needed to store our models in memory. Note that, with int4 precision, &lt;em&gt;all of these models fit into memory on an A100&lt;/em&gt; (which is the standard datacenter GPU right now), and all of them, except for the biggest model, fit into memory on high-end consumer GPUs (3090s/4090s, which have 24GB of RAM).&lt;/p&gt;

&lt;p&gt;It takes approximately \(2P\) FLOPS to run inference on our model for a single token, because we are doing a bunch of matmuls with a total of \(P\) parameters, and multiplying a matrix of size \((m, n)\) with a vector of size \((n,)\) has a cost of \(2mn\).&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;With all that math out of the way, let’s calculate the requirements for running inference with LLaMa. The main requirements when it comes to sampling are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Keep the KV cache in memory, in addition to all the parameters.&lt;/li&gt;
  &lt;li&gt;Read all the weights from HBM into the on-chip memory. Because we sample auto-regressively, we have to repeat this for each token we sample.&lt;/li&gt;
  &lt;li&gt;Do the actual matmuls to calculate the output of our network.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The latency is the maximum of either the compute or the memory latency, as reading parameters into on-chip memory happens asynchronously in all modern tensor programming libraries. As a result, we write:&lt;/p&gt;

\[\begin{align*}
\text{latency}_\text{model} &amp;amp;= \text{max}(\text{latency}_\text{compute}, \text{latency}_\text{memory})\\
\text{latency}_\text{compute} &amp;amp;= \dfrac{2 \cdot P \cdot n_{\text{bytes}}\cdot B}{n_{\text{memory bandwidth}}},\\
\text{latency}_\text{memory} &amp;amp;= \dfrac{2 \cdot P}{n_{\text{flops}}},
\end{align*}\]

&lt;p&gt;where \(B\) is the batch size. As \(n_{\text{memory bandwidth}} = 1.935e12\), and  \(n_{\text{flops}} = 3.12e14,\) as long as the batch size is less than 161, the model is memory-bound.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;With a batch size of 1, this is the same equation, as on most hardware (e.g. Nvidia GPUs), there is a linear speedup as you decrease the precision (you get twice the FLOPS when using fp16 vs fp32, which doubles again as you go to int8, and doubles once more as you go to int4s).&lt;/p&gt;

&lt;p&gt;As LLaMa.cpp uses int4s, the RAM requirements are reduced to 1.33GB of memory for the KV cache, and 16.25GB of VRAM for the model parameters. That’s pretty good!&lt;/p&gt;

&lt;p&gt;As the memory bandwidth is almost always&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; much smaller than the number of FLOPS, memory bandwidth is the binding constraint.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-inference-times.png&quot; alt=&quot;Screenshot fo table showing the inference times to run the varying LLaMa models with varying precision levels on an A100&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-llama-on-an-a100&quot;&gt;Running LLaMa on an A100&lt;/h2&gt;

&lt;p&gt;On an A100 (80GB PCIe), the memory bandwidth is 1935GB/s. The int4 compute is 1248 TOPS. As such, the model is (heavily) memory-bound. We should expect to see inferences as given in the table; roughly 30 tokens/s with the 65B model, and 277 tokens/s with the 7B model.&lt;/p&gt;

&lt;h2 id=&quot;running-llama-on-a-m1-macbook-air&quot;&gt;Running LLaMa on a M1 Macbook Air&lt;/h2&gt;

&lt;p&gt;The M1 GPU has a bandwidth of &lt;a href=&quot;https://www.macworld.com/article/783678/m2-vs-m1-chip-performance-graphics-ram.html&quot;&gt;68.25 GB/s&lt;/a&gt;, while the M1 GPU can do up to &lt;a href=&quot;https://tlkh.dev/benchmarking-the-apple-m1-max#heading-gpu-matrix-multiplication-gemm-performance&quot;&gt;5.5 TFLOPS&lt;/a&gt; of fp16 compute. As such, we should expect a ceiling of ~1 tokens/s for sampling from the 65B model with int4s, and 10 tokens/s with the 7B model.&lt;/p&gt;

&lt;p&gt;As the M2 Pro has 200 GB/s of bandwidth, and the M2 Max has 400 GB/s of bandwidth, we should expect massive improvements with them, going up to 6 tokens/s with the M2 Max with the 65B model. That’s pretty darn good for a laptop.&lt;/p&gt;

&lt;h2 id=&quot;running-llama-on-a-raspberry-pi-4&quot;&gt;Running LLaMa on a Raspberry Pi 4&lt;/h2&gt;

&lt;p&gt;A Raspberry Pi 4 has &lt;a href=&quot;https://web.eece.maine.edu/~vweaver/group/green_machines.html&quot;&gt;13.5 GFLOPS of compute&lt;/a&gt;, and &lt;a href=&quot;https://forums.raspberrypi.com/viewtopic.php?t=281183&quot;&gt;~4GB/s of memory bandwidth&lt;/a&gt;. Given this, we’d expect to see ~2 tokens/s with the 7B model if it was memory bound. Given that we’re currently seeing ~0.1 tokens/s, I suspect we’re actually compute-bound (although this is a stab in the dark— I can’t find enough information about the specs for a Raspberry Pi to determine this with any precision).&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them &lt;em&gt;much&lt;/em&gt; easier to serve— like quantization! This is yet another reason why distillation, or just &lt;a href=&quot;https://finbarr.ca/llms-not-trained-enough/&quot;&gt;training smaller models for longer&lt;/a&gt;, is really important.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: I’m not an expert in CUDA, so I probably have errors in my math. If so, please shoot me an &lt;a href=&quot;mailto:finbarrtimbers@gmail.com&quot;&gt;email&lt;/a&gt; and let me know- I’d love to hear from you so I can learn more about how this works and update this post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Resources on transformer inference performance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/posts/2023-01-10-inference-optimization/&quot;&gt;Large Transformer Model Inference Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;Transformer inference arithmetic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kipp.ly/blog/transformer-param-count/&quot;&gt;LLM parameter counting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot;&gt;Efficient Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thank you to &lt;a href=&quot;https://twitter.com/kaushikpatnaik?lang=en&quot;&gt;Kaushik Patnaik&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/arthurallshire&quot;&gt;Arthur Allshire&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/stanislavfort&quot;&gt;Stanislav Fort&lt;/a&gt;, and &lt;a href=&quot;https://twitter.com/banburismus_&quot;&gt;Tom McGrath&lt;/a&gt; for reading early drafts of this.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Almost all of this math is taken from their article; they deserve full credit. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Although it’s an open question of how much Mac specific optimization is being done with LLaMa, or indeed, any of the tensor programming frameworks. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For a more detailed discussion showing that this is the case, check out &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/#flops-counting&quot;&gt;kipply’s article&lt;/a&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If you’re following along with &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/#latency-calculations&quot;&gt;kipply’s post&lt;/a&gt;, there’s a slight discrepancy here, as I assume a 80GB A100 PCIe, which is what I see as the standard GPU. Their post assumes a 40GB A100 PCIe, which has slightly lower memory bandwidth. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I hedge with “almost” here, but I’m not aware of any counterexamples. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Thu, 16 Mar 2023 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/how-is-llama-cpp-possible/</link>
				<guid isPermaLink="true">http://localhost:4000/how-is-llama-cpp-possible/</guid>
			</item>
		
			<item>
				<title>A step towards self-improving LLMs</title>
				<description>&lt;p&gt;&lt;em&gt;There&apos;s a &lt;a href=&quot;https://finbarrtimbers.substack.com/p/a-step-towards-self-improving-llms&quot;&gt;Substack&lt;/a&gt; version of this post, if you prefer that over my &lt;del&gt;amateurish&lt;/del&gt; artisan HTML.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If I look at GPTs/LLMs, two of the biggest problems I see with existing techniques are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We need our models to be able to generate data by themselves, i.e. we need a &lt;a href=&quot;https://www.lesswrong.com/tag/recursive-self-improvement&quot;&gt;recursive self-improvement loop&lt;/a&gt;. AlphaZero is the shining example of what’s possible here.&lt;/li&gt;
  &lt;li&gt;We need our models to be able to operate in new domains without requiring massive amounts of existing data. CLIP provides an option here, as does Internet Explorer (the paper, not the browser).&lt;/li&gt;
  &lt;li&gt;Auto regressive sampling. It’s slow, suboptimal.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I have ideas for how to tackle #1, so I’ll focus on that here.&lt;/p&gt;

&lt;p&gt;There are other issues facing LLMs, such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Increasing the length of the context window&lt;/li&gt;
  &lt;li&gt;Figuring out how to train larger models&lt;/li&gt;
  &lt;li&gt;Figuring out how to train more efficient models (less parameters, less data, less energy)&lt;/li&gt;
  &lt;li&gt;Factual accuracy&lt;/li&gt;
  &lt;li&gt;Mitigating attacks that convince LLMs to exhibit harmful behaviour (”red-teaming”), e.g. prompt injection&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think these are fundamentally engineering problems that we’ll be able to figure out iteratively. For instance, context length has seen a lot of progress with &lt;a href=&quot;https://openreview.net/forum?id=H4DqfPSibmx&quot;&gt;subtle algorithmic&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.05682&quot;&gt;improvements&lt;/a&gt;; if we combine those changes with the many &lt;a href=&quot;https://twitter.com/karpathy/status/1621578354024677377&quot;&gt;arcane engineering optimizations&lt;/a&gt; that are out there, I think we’ll get to a point where context goes to 64k tokens or more, at which point we’ll be deep in the &lt;a href=&quot;http://finbarr.ca/the-sigmoid/&quot;&gt;saturating point of the sigmoid&lt;/a&gt;. Or for factual accuracy- I think that retrieval will largely solve that once it’s incorporated into most models.&lt;/p&gt;

&lt;p&gt;However, I’m probably wrong, and could very well end up writing a version of this post in 2034 talking about how the biggest problem facing AGI is prompt injections.&lt;/p&gt;

&lt;h2 id=&quot;a-path-towards-recursive-self-improvement&quot;&gt;A path towards recursive self-improvement&lt;/h2&gt;

&lt;p&gt;GPTs work very well in one specific context: they are very, very good at finding text that is likely to follow other text in a way that appears natural to humans.&lt;/p&gt;

&lt;p&gt;What they don’t do is come up with text that they haven’t seen before. Kinda. What they’re doing when we sample from them now is predict what they’ve seen during training. Sometimes these predictions produce text that hasn’t been written before (this can occur often, due to the combinatorial nature of token sampling). When this happens, it’s a happy accident. The model isn’t trying to select text that is novel or that accomplishes any goal other than &lt;em&gt;following the preceding 2048 tokens&lt;/em&gt; (or whatever the context length is).&lt;/p&gt;

&lt;p&gt;The obvious exception is when models are finetuned using &lt;a href=&quot;https://arxiv.org/abs/1706.03741&quot;&gt;RLHF&lt;/a&gt;. In RLHF, the models are explicitly trained to optimize a reward signal. In RLHF, the reward signal comes from a model trained to predict human feedback. Basically, humans are asked to choose between two samples of text, and then a model learns to predict which one is preferred.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/rlhf-reward-loop.png&quot; alt=&quot;RLHF training loop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why does this matter? Predicting the next token works pretty well! And &lt;a href=&quot;https://twitter.com/sama/status/1599471830255177728?s=20&quot;&gt;maybe we’re all&lt;/a&gt; just &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3442188.3445922&quot;&gt;stochastic parrots&lt;/a&gt;? It matters because the biggest impediment to improving our models right now is the &lt;em&gt;lack of data&lt;/em&gt;. The scaling law papers (&lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI&lt;/a&gt;) consistently point to the fact that we need to scale up the datasets we train LLMs on.&lt;/p&gt;

&lt;p&gt;For instance, Chinchilla predicts that we’ll need 11 &lt;em&gt;trillion&lt;/em&gt; tokens to optimally train a model the size of PaLM (i.e. 540B parameters). If we want to push past PaLM to a model with 1 trillion parameters, we’ll need 20T tokens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-optimal-data-requirements.png&quot; alt=&quot;chinchilla-optimal-data-requirements.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s a lot of data. That’s so much data that it’s not clear that we can get that from existing sources. &lt;a href=&quot;https://twitter.com/nostalgebraist?lang=en&quot;&gt;nostalgebraist&lt;/a&gt; &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;argues&lt;/a&gt; that 1) we’ve basically exhausted the available data in structured domains like coding and 2) it’s starting to look like we’re running out of general-domain data. I find nostalgebraist compelling; the only counterargument I could see is that private data sources might be a rich vein of tokens, but I don’t see a clear path to getting access to them.&lt;/p&gt;

&lt;p&gt;This lack of data is unfortunate because, according to &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;Chinchilla’s scaling laws&lt;/a&gt;, we could see another ~8% reduction in training loss (1.93 → 1.77, delta of 0.16 in loss) for if we had infinite data &lt;em&gt;while changing nothing else about Chinchilla&lt;/em&gt;. That’s a pretty substantial improvement when you consider that the improvement from Gopher to Chinchilla was only 2.9% (1.99 → 1.93, delta of 0.06 in loss), not to mention the fact that our models are already quite good— able to &lt;a href=&quot;https://www.bbc.com/news/technology-62275326&quot;&gt;trick Google SWEs into believing they’re sentient&lt;/a&gt;, and &lt;a href=&quot;https://www.lesswrong.com/posts/FKNtgZrGYwgsz3nHT/bankless-podcast-159-we-re-all-gonna-die-with-eliezer&quot;&gt;scaring the Yud&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;more-data&quot;&gt;More data&lt;/h3&gt;

&lt;p&gt;The clear implication is that we need way more data! Our models are desperate for data. They’re lying on the beach &lt;em&gt;gasping for more data&lt;/em&gt; to quench their ever-growing thirst.&lt;/p&gt;

&lt;p&gt;But where will the data come from?&lt;/p&gt;

&lt;p&gt;If we can scrape it we should. It’s not clear how much there is left to scrape. Especially at the largest research institutions like OpenAI, Google Brain, and DeepMind, I’m certain that they have teams of engineers working on scraping all possible data. There is some possibility to automate this process; the excellently named &lt;a href=&quot;https://arxiv.org/abs/2302.14051&quot;&gt;Internet explorer paper&lt;/a&gt; presented a model which crawls the web to get additional data to augment it’s dataset. Although letting a nascent AI loose on the internet would make Eliezer cry, it could be an excellent source of data, especially if one incorporates some sort of reinforcement learning style feedback loop to continually improve the manner in which the model searches the web.&lt;/p&gt;

&lt;p&gt;The data problem is compounded by the fact that high quality data &lt;em&gt;really matters&lt;/em&gt;. Experiments consistently show that deduplicating data increases performance substantially (&lt;a href=&quot;https://arxiv.org/abs/2205.10487&quot;&gt;https://arxiv.org/abs/2205.10487&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2107.06499&quot;&gt;https://arxiv.org/abs/2107.06499&lt;/a&gt;). Basically, I’m not convinced there is a lot more high quality data. Two exceptions might be commercial data (e.g. internal corporate documents), and all copyrighted text. But it would be extremely difficult to get access to either of these corpora.&lt;/p&gt;

&lt;h3 id=&quot;generate-data&quot;&gt;Generate data&lt;/h3&gt;

&lt;p&gt;The solution to me seems to be self-evident: we should generate our own data.
There has been some work about training LLMs on data they have generated (&lt;a href=&quot;https://arxiv.org/abs/2210.11610&quot;&gt;https://arxiv.org/abs/2210.11610&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;https://arxiv.org/abs/2212.08073&lt;/a&gt;). There are a few different techniques that seem promising here.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/abs/2210.11610&quot;&gt;Huang et. al&lt;/a&gt;, they use Chain of Thought (CoT) reasoning to generate additional data. Given a dataset of questions, they sample N answers that use CoT to generate an answer. At the end, they ask “The answer is “ and get an answer; they then find the majority answer and choose all texts that return the same answer as the most common answer, using this to generate additional data. In practice, there’s no reason to questions, although that is a particularly straight forward problem to apply this to; one could imagine, say, embedding all of the generated answers, clustering them, and keeping the answers in the biggest cluster, or employing RLAIF like Anthropic did in the &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;Constitutional AI&lt;/a&gt; paper to select the answers to keep.&lt;/p&gt;

&lt;p&gt;Anthropic employed a similar approach as the CoT reasoning in the &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;Constitutional AI paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is to use RLAIF (from Constitutional AI) to generate data. In this,&lt;/p&gt;

&lt;h2 id=&quot;throw-compute-at-the-problem&quot;&gt;Throw compute at the problem&lt;/h2&gt;

&lt;p&gt;Yet another line of research involves throwing compute at the problem. We know that we can use a variety of techniques to soak up compute and improve outcomes. For instance, &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;ensembling&lt;/a&gt; is a classic ML technique that strictly improves model performance. Given that we are already at the extreme limit of what’s possible to compute with transformers, it is almost certainly not possible to naively ensemble LLMs.&lt;/p&gt;

&lt;p&gt;However, what we can do is use compute to apply search on top of our existing model outputs. If we can find a &lt;a href=&quot;https://proceedings.neurips.cc//paper/2020/file/22eda830d1051274a2581d6466c06e6c-Paper.pdf&quot;&gt;policy improvement operator&lt;/a&gt;, i.e. a function T that takes an existing distribution over tokens, π, and returns a new distribution, T(π), which improves our loss, then we can use T to improve our model. Some candidates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Best-of-n&lt;/li&gt;
  &lt;li&gt;Beam search&lt;/li&gt;
  &lt;li&gt;Policy-driven search&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Best-of-n” (&lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;&gt;https://openai.com/research/measuring-goodharts-law&lt;/a&gt;) is a technique similar to ensembling in which we sample from our model N times, and use the sample with the highest score according to our objective function. This performs remarkably well (outperforming the RLHF model in the WebGPT paper(&lt;a href=&quot;https://openai.com/research/webgpt&quot;&gt;https://openai.com/research/webgpt&lt;/a&gt;)), is simple to implement, trivial to analyze mathematically, and trivially parallelizable, but makes inference N times more expensive. If I were OpenAI, I’d be caching the results of queries to their models and doing this for repeated queries.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/webgpt-best-of-n-performance.png&quot; alt=&quot;best-of-n performance curves from the WebGPT paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the WebGPT paper, the authors found that best-of-16 resulted in an improvement in human preferences of 5% (60% → 65%), while going from 13B parameters to 175B parameters resulted in an improvement of 10% (~47% → 57%) (results from eyeballing graph, not precise).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/webgpt-scaling-performance.png&quot; alt=&quot;scaling performance graph from WebGPT paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given that both charts appear to be seeing roughly linear improvements, and both models increase in cost roughly linearly, it seems to imply that a best-of-64 13B model would be better than a best-of-4 175B model, while having roughly the same cost in terms of compute. Given that the 13B model should fits on a single GPU (maybe 2 GPUs?), this would substantially lower the overall compute of the system.&lt;/p&gt;

&lt;p&gt;Another improvement operator is a NLP classic: &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;&gt;beam search!&lt;/a&gt; In beam search, one performs a breadth-first search over the model outputs, with finite depth and width of the tree (e.g. it only keeps N successors at each level of the tree, and searches to a depth of M levels), with the final result being the sequence with the maximum objective score (typically log-likelihood).&lt;/p&gt;

&lt;p&gt;While a number of the LLMs do use beam search&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, they don’t appear to report performance numbers, so I’m unable to include a comparison of how much it matters.&lt;/p&gt;

&lt;p&gt;A concern is that beam search still lowers diversity, as it constricts the difference in tokens; this is especially problematic for byte-level tokenizers, like BPE, as the individual tokens might vary significantly. &lt;a href=&quot;https://twitter.com/sedielem&quot;&gt;Sander Dieleman&lt;/a&gt; &lt;a href=&quot;https://sander.ai/2020/09/01/typicality.html&quot;&gt;wrote about how&lt;/a&gt; strategies like beam search are &quot;the culprit behind many of the pathologies that neural machine translation systems exhibit”.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The final candidate (or family of candidates) for the improvement operator is an option that I find very exciting: learning an algorithm to search the token tree. The idea is that we could do something like AlphaZero which would learn a policy + value function.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This would also allow us to change the reward function if we wanted to, rather than just using the standard log-likelihood. We could, for instance, directly train the reward function on human data. If you’re serving data to millions of users per day, you could just directly run RL on that, which is the case for the myriad of chat bots on the market today (Bing, ChatGPT, Claude, etc.).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.notion.so/Discussion-of-beam-search-diversity-f7b65583098f4f219e0e9fd59d5ab80d&quot;&gt;Discussion of beam search diversity&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Examples include: GPT-{2,3}, which uses it during decoding for text generation, BERT, for language understanding tasks, T5, and XLNet. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sander’s post is great. I was struggling to understand why beam search isn’t used more in practice, and his post did a great job helping me understand why. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Perhaps using MuZero with a smaller model as the recurrent function to save on compute. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Tue, 07 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/self-improving-LLMs/</link>
				<guid isPermaLink="true">http://localhost:4000/self-improving-LLMs/</guid>
			</item>
		
			<item>
				<title>Papers I've read this week (March 4th, 2023)</title>
				<description>&lt;p&gt;I’m going to try to write a weekly summary of the most interesting papers I’ve read that week. I’d love to hear what papers you’ve been reading, if you agree/disagree about my conclusions for each paper, and/or suggestions for what papers I should read next!&lt;/p&gt;

&lt;h2 id=&quot;scaling-laws-for-routed-language-models&quot;&gt;Scaling laws for routed language models&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.01169&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://www.deepmind.com/&quot;&gt;used to work&lt;/a&gt; with Aidan, and way back in 2021, he was insistent that LLMs were the future of AI. I thought he was crazy. In ‘22, he also insisted that conditional routing models were the future. Given how right he was about LLMs, it’s probably worth paying attention to conditional routing models.&lt;/p&gt;

&lt;p&gt;The paper provides a great general overview of how routing networks work and performance comparisons (in terms of negative log likelihood over a validation dataset) for the 3 most common routing techniques (&lt;a href=&quot;https://arxiv.org/abs/1701.06538&quot;&gt;sparse MoE&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.04426&quot;&gt;non-parametric HASH&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1308.3432&quot;&gt;RL routing&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The authors trained a large number of conditional routing networks, and fit scaling laws to the results; they find that all 3 techniques follow the same scaling laws, with RL routing doing quite well. I’d be curious to see how much effort has been put into improving RL routing; I suspect that it could be improved significantly.&lt;/p&gt;

&lt;p&gt;The authors observed the following results:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Routing improves the performance of language models across all sizes and variants attempted&lt;/li&gt;
  &lt;li&gt;Training a Routing Network with RL is of comparable effectiveness to state-of-the-art techniques.&lt;/li&gt;
  &lt;li&gt;The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in
the underlying dense model size.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I was surprised at how similar the performance of the various techniques was. The data was quite nice, with little variation. The scaling laws seem to fit the data quite nicely.&lt;/p&gt;

&lt;p&gt;One interesting result was that routing helps significantly more when the model is smaller. I found this surprising; my intuition is that routing should always help. They found that this was the case across all models, and that routing helped less as the models grew.&lt;/p&gt;

&lt;p&gt;The paper ends with recommendations, which I found really useful:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use routing for models with less than 1.3B parameters&lt;/li&gt;
  &lt;li&gt;S-Base is a good, default routing algorithm (defined in the appendix of their paper).&lt;/li&gt;
  &lt;li&gt;Target using E in {64, 128} experts.&lt;/li&gt;
  &lt;li&gt;Use K = 1 experts; route layers with a frequency between 0.5 &amp;amp; 1; lower frequency reduces performance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;internet-explorer&quot;&gt;Internet explorer&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.14051&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this paper, the authors create an agent which dynamically explores the internet, running text queries to find images to use for self-supervised training. While seemingly designed to &lt;a href=&quot;https://rationalwiki.org/wiki/AI-box_experiment&quot;&gt;directly antagonize Yudkowsky&lt;/a&gt;, the paper is extremely interesting, and presents, to me, a potential future direction for AGI research. As &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt; showed us, LLMs could &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;massively improve&lt;/a&gt; with more data. Having agents dynamically exploring the internet is one excellent way to get more data- especially if they’re able to adaptively learn over time and prioritize images accordingly.&lt;/p&gt;

&lt;p&gt;In the paper, they train a model to learn representations of images based on &lt;a href=&quot;https://paperswithcode.com/method/moco-v3&quot;&gt;MoCo-v3&lt;/a&gt;. They query Google Images for new images, ranking the query results by similarity to the target dataset, assigning a reward to the new images:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/internet-explorer-reward.png&quot; alt=&quot;reward equation for internet explorer paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, S_cos is the cosine similarity, f_k is the image encoder, D := {x_i} is the target dataset, and y is the new image to evaluate, and they evaluate over the k closest neighbours in the target dataset (where “closest” is determined by the encoded representation).&lt;/p&gt;

&lt;p&gt;They create the queries for Google Images by sampling them a static vocabulary dataset. They estimate the reward associated with the query using a Gaussian process regression. I’d be really interested to see a fancier query generation process. One idea that comes to my mind would be using RL to train a LLM to generate queries in a manner similar to what’s done in &lt;a href=&quot;https://openai.com/research/learning-from-human-preferences&quot;&gt;RLHF&lt;/a&gt;/&lt;a href=&quot;https://arxiv.org/abs/2204.05862&quot;&gt;RLAIF&lt;/a&gt;, i.e. use an RL algorithm like PPO to finetune a pretrained LLM to maximize reward. This would require much more compute, however.&lt;/p&gt;

&lt;h2 id=&quot;llama&quot;&gt;LLaMa&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’ve been digesting the LLaMa paper that Facebook released this week. It was very interesting to see the performance increases they got despite the size decreases. Their 13B model outperformed GPT-3 on a number of benchmarks, and their 65B model was competitive with Chinchilla-70B and PaLM-540B (!).&lt;/p&gt;

&lt;p&gt;I did find it incredibly frustrating that they stopped training when they did; their loss curves are all looking pretty far from convergence, and I’m curious to see how much the models will continue to improve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;loss curves for LLaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I wish that they had just &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/#6-squeeze-out-the-juice&quot;&gt;left it training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My biggest question about the paper is that it’s not clear what caused the improvements. They discuss a few major changes compared to GPT-3, which their model is based on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They only use publicly available data, but it’s unclear what exactly the filtering steps are. I wish they’d open source their dataset (or at least, the code to clean it).&lt;/li&gt;
  &lt;li&gt;They normalize the input of each transformer sub-layer, rather than the output.&lt;/li&gt;
  &lt;li&gt;They use the SwiGLU activation function, as PaLM did, with a slight dimensional difference compared to PaLM.&lt;/li&gt;
  &lt;li&gt;They use Rotary Embeddings.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There was no ablation study, unfortunately. If I can scrounge up the GPUs, I’m tempted to do my own ablation based on &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt;. LLaMa also uses &lt;a href=&quot;https://arxiv.org/abs/22015.14135&quot;&gt;FlashAttention&lt;/a&gt;, which I suspect will become the default attention implementation used in LLMs going forward.&lt;/p&gt;

&lt;p&gt;Anecdotally, it seems like performance with the default parameters &lt;a href=&quot;https://twitter.com/browserdotsys/status/1632145830277795840&quot;&gt;isn&apos;t great&lt;/a&gt; (unless you&apos;re writing &lt;a href=&quot;https://twitter.com/browserdotsys/status/1632512136906719232&quot;&gt;erotic stories&lt;/a&gt;). However, it hasn&apos;t been tuned (RLHF/SFT), so maybe &lt;a href=&quot;https://twitter.com/yacineMTB/status/1632392979833921539&quot;&gt;that would make a difference&lt;/a&gt;? With some &lt;a href=&quot;https://twitter.com/theshawwn/status/1632569215348531201&quot;&gt;modifications&lt;/a&gt;, performance is apparently pretty good. This lends strength to my hypothesis that the paper was rushed out in response to the LLM gold rush we&apos;ve seen since ChatGPT was released. The paper, while strong, would be much stronger with a few more changes (ablation, more details, slightly more polished code), in a way that doesn&apos;t make sense for the omissions to be strategic.&lt;/p&gt;

&lt;p&gt;Facebook benefits when open source LLMs get better in a way that OpenAI/Anthropic/Cohere/etc. doesn&apos;t, so they should want to do ablations and other scientific work to advance the field. The fact that they didn&apos;t include this makes me think that they wanted to get the paper out the door as soon as possible- probably to avoid the chance of being scooped with an even larger, better, model.&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I haven’t seen a great ablation study comparing various embedding schemes. This is on my list of experiments to do once I can scrounge up GPUs. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Sat, 04 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/papers-ive-read/</link>
				<guid isPermaLink="true">http://localhost:4000/papers-ive-read/</guid>
			</item>
		
			<item>
				<title>The Sigmoid: a metaphor for technological progress</title>
				<description>&lt;p&gt;I regularly reference the “s-curve”, or sigmoid, as a metaphor for progress. Here, I explain what I mean, so that I can just link to this post.&lt;/p&gt;

&lt;p&gt;A common mathematical relationship in technology is the s-curve (or sigmoid curve). Mathematically:&lt;/p&gt;

\[\text{sigmoid}(x) := \dfrac{1}{1 + e^{-x}} \equiv \dfrac{e^x}{e^x + 1}\]

&lt;p&gt;This is notable because it produces a curve that &lt;em&gt;looks like an s&lt;/em&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/sigmoid.png&quot; alt=&quot;The sigmoid curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In particular, when x is small, this grows slowly, when x is not too big or not too small, it grows exponentially, and when x is large, it grows slowly. This is a common pattern with many technologies! We see slow progress at first, then it accelerates rapidly, and finally, as we begin to hit the limits of the technology, progress slows. Consider single-thread CPU performance. Initially, progress was slow as people figured out how to make them. Then, it grew exponentially for years, following Moore’s Law (&lt;a href=&quot;https://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/&quot;&gt;picture source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/single-threaded-cpu-performance.png&quot; alt=&quot;Single-threaded CPU performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, since around 2010, we haven’t seen a lot of improvements in single-thread CPU performance. That pattern- slow, fast, then slow- is what I mean when I talk about the s-curve in technology. And when I talk about entering the &lt;em&gt;saturating part of the s-curve&lt;/em&gt;, I mean that we’re entering the region where progress is slowing down again.&lt;/p&gt;
</description>
				<pubDate>Thu, 02 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/the-sigmoid/</link>
				<guid isPermaLink="true">http://localhost:4000/the-sigmoid/</guid>
			</item>
		
			<item>
				<title>Large language models aren't trained enough.</title>
				<description>&lt;p&gt;&lt;em&gt;I have a &lt;a href=&quot;https://finbarrtimbers.substack.com/&quot;&gt;Substack&lt;/a&gt; if you want to be notified when I write.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This was inspired by tweets from several people:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629159764918775809?s=20&quot;&gt;https://twitter.com/BlancheMinerva/status/1629159764918775809&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629551017019711488&quot;&gt;https://twitter.com/BlancheMinerva/status/1629551017019711488&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/kaushikpatnaik/status/1629194428312330240?s=46&amp;amp;t=x3wLedGK_QyDwCK5yD2Jqw&quot;&gt;https://twitter.com/kaushikpatnaik/status/1629194428312330240&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/thedavidsj/status/1629869851710984194?s=46&amp;amp;t=CqBoSVdxuipOpACbrZnMxg&quot;&gt;https://twitter.com/thedavidsj/status/1629869851710984194&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and Twitter conversations with &lt;a href=&quot;https://twitter.com/arankomatsuzaki/&quot;&gt;Aran Komatsuzaki&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/andy_l_jones&quot;&gt;Andy Jones&lt;/a&gt;. Any mistakes are entirely my own.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: although I did work at DeepMind previously, I was not involved with any of the language efforts, and have no non-public knowledge of what went on there. Unfortunately! I think Chinchilla is a great paper that I would have loved to be a part of.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’ve been &lt;a href=&quot;[https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645](https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645)&quot;&gt;on the job market since January&lt;/a&gt;, and I’ve been talking to a lot of companies training large language models (LLMs). The consistent phrasing that comes up is that they want to train a &lt;em&gt;Chinchilla-optimal model&lt;/em&gt;  (Chinchilla here referring to the &lt;a href=&quot;[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)&quot;&gt;DeepMind paper from spring ‘22&lt;/a&gt;, not the adorable rodent).&lt;/p&gt;

&lt;h3 id=&quot;chinchilla&quot;&gt;Chinchilla&lt;/h3&gt;

&lt;p&gt;The Chinchilla paper was an attempt to identify the optimal model size &amp;amp; number of tokens to train a LLM given a particular compute budget. The paper trained 400 (!) language models and found a clear relationship between # of model parameters and # of training tokens: the two should scale linearly, i.e. if you double model size, you should double the number of training tokens. The authors used this relationship (which we call a &lt;em&gt;scaling law)&lt;/em&gt; to train a new model, Chinchilla, which had the same compute budget as Gopher, an earlier DeepMind model, and were able to significantly outperform Gopher + GPT-3 + a number of larger models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-convergence.png&quot; alt=&quot;Loss curves from the Chinchilla paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When people talk about training a Chinchilla-optimal model, this is what they mean: training a model that matches their estimates for optimality. They estimated the optimal model size for a given compute budget, and the optimal number of training tokens for a given compute budget.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-optimal.png&quot; alt=&quot;Chinchilla-optimal levels of compute/data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, when we talk about “optimal” here, what is meant is “what is the cheapest way to obtain a given loss level, in FLOPS.” In practice though, we don’t care about the answer! This is exactly the answer you care about if you’re a researcher at DeepMind/FAIR/AWS who is training a model with the goal of reaching the new SOTA so you can publish a paper and get promoted. If you’re training a model with the goal of actually deploying it, the training cost is going to be dominated by the inference cost. This has two implications:&lt;/p&gt;

&lt;p&gt;1) there is a strong incentive to train smaller models which fit on single GPUs&lt;/p&gt;

&lt;p&gt;2) we’re fine trading off training time efficiency for inference time efficiency (probably to a ridiculous extent).&lt;/p&gt;

&lt;p&gt;Chinchilla implicitly assumes that the majority of the total cost of ownership (TCO) for a LLM is the training cost. In practice, this is only the case if you’re a researcher at a research lab who doesn’t support products (e.g. FAIR/Google Brain/DeepMind/MSR). For almost everyone else, the amount of resources spent on inference will dwarf the amount of resources spent during training.&lt;/p&gt;

&lt;p&gt;Let’s say you’re OpenAI and you’re serving GPT-4 as BingChat. In addition to hiring experienced &lt;a href=&quot;https://twitter.com/chrisjbakke/status/1628877552940097536&quot;&gt;killswitch engineers&lt;/a&gt; to thwart Sydney’s repeated escape attempts, you have to choose exactly which model to deploy.&lt;/p&gt;

&lt;p&gt;To run inference on N tokens of text, OpenAI charges &lt;a href=&quot;https://openai.com/api/pricing/&quot;&gt;$2e-5/token&lt;/a&gt; for their most advanced model. Assuming a 60% gross margin, it costs them $8e-6/token to serve. A rough cost estimate to train GPT-3 is &lt;a href=&quot;[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/](https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/)&quot;&gt;$5M&lt;/a&gt;. As such, after serving 625B tokens, their costs are going to be dominated by inference, rather than serving. When I use ChatGPT, it typically generates 300 tokens worth of responses to me. That’s 20B responses. If ChatGPT has 10M DAU, each making 10 queries/day, that’s 100M queries/day— so inference costs break even with training costs after 200 days.&lt;/p&gt;

&lt;p&gt;This is almost certainly an underestimate for their usage given how popular ChatGPT has been. If we assume 1B queries per day, it breaks even after 20 days.&lt;/p&gt;

&lt;p&gt;The various scaling law papers (&lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;) provide answers to the question of how to allocate compute between model size and dataset size. I think these papers are the &lt;em&gt;right&lt;/em&gt; way to think about training research systems, but the &lt;em&gt;wrong&lt;/em&gt; way to think about training systems that will be deployed at scale (I don&apos;t think the authors would disagree- they&apos;re solving a specific problem, namely minimizing the loss of their system given a specific compute budget, which isn&apos;t the same problem faced in deployment).&lt;/p&gt;

&lt;h3 id=&quot;llama&quot;&gt;LlaMa&lt;/h3&gt;

&lt;p&gt;Let’s look at Facebook’s &lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;new language model&lt;/a&gt; (in the second paragraph, the authors of that paper make a similar argument to the one I’m making here). If we draw a horizontal line across at any given loss level, it looks like you can tradeoff a doubling of model size for 40% more training.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;Loss curves from the LlaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at, e.g., the line at a training loss of 1.7. The 65B model crosses it at 600B tokens, while the 33B model needs 800B tokens. Or look at a loss of 1.65: 65B needs 800B tokens, 33B needs ~1100B tokens.&lt;/p&gt;

&lt;h3 id=&quot;gpt-3&quot;&gt;GPT-3&lt;/h3&gt;

&lt;p&gt;If we look at the granddaddy of LLMs, GPT-3, we see a similar story in the loss curves: it requires roughly an order of magnitude more compute to get the green lines (GPT-3 13B) to match the yellow line (GPT-3)!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/gpt-3-loss-curves.png&quot; alt=&quot;Loss curves from the GPT-3 paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is important to note that the GPT-3 13B learning curves do level out earlier than GPT-3, with a rough estimate being that they would cross somewhere around the 1.8 loss area. It is also almost certainly the case that GPT-3 will achieve an asymptotically lower loss than the 13B model. Having said that, there is a question as to how much of a difference lower pre-training loss makes; I suspect that we are seeing diminishing returns kick in to pre-training, and most of the gains will come from RLHF and other types of finetuning.&lt;/p&gt;

&lt;h3 id=&quot;inference-costs&quot;&gt;Inference costs&lt;/h3&gt;

&lt;p&gt;Transformer inference costs are &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;roughly linear&lt;/a&gt; with the number of parameters, making it ~13x cheaper to do inference with the 13B GPT-3 model than the 175B model. This is &lt;em&gt;also&lt;/em&gt; an underestimate, as the 13B model should fit it on 2 GPUs, while you would need many more just to fit the 175B model into VRAM. As scaling to multiple GPUs adds a ridiculous amount of engineering complexity, overhead, and cost, we should prefer the smaller model &lt;em&gt;even more&lt;/em&gt;. We should train the model &lt;em&gt;much&lt;/em&gt; longer to get an order of magnitude decrease in inference cost and optimize the TCO of the system.&lt;/p&gt;

&lt;p&gt;For instance, when training on multiple GPUs, it is very difficult to get high utilization numbers. The PaLM paper reported how well various LLMs did in terms of total FLOPS utilization. These are not very good numbers! Especially when each of the GPUs mentioned here costs &lt;a href=&quot;https://www.shi.com/product/41094090/NVIDIA-Tesla-A100-GPU-computing-processor&quot;&gt;$25k&lt;/a&gt;. This is despite the fact that the authors for these papers are the most experienced researchers in the world at deploying model-parallel systems, and are working on custom hardware optimimzed for this usecase. Now, training efficiency doesn&apos;t directly translate to inference efficiency, but the numbers should be directionally correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/palm-utilization.png&quot; alt=&quot;Model FLOPS utilization numbers for various large language models.&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;opposing-arguments&quot;&gt;Opposing arguments&lt;/h3&gt;

&lt;p&gt;Some arguments against my claim:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Aren’t we leaving performance on the table? Yes! We are. But I think that’s fine! There’s always a tradeoff here. E.g. quantization. It’s strictly worse to use lower-precision! But we do it to optimize TCO of the system.&lt;/li&gt;
  &lt;li&gt;But we can use $INSERT_TECHNIQUE to make models cheaper! Yes, but they should scale for all of these (distillation, quantization, etc.). So we should be using all techniques to make our models easier to serve, and also training them longer.&lt;/li&gt;
  &lt;li&gt;Your argument here! Please email me with your criticism and I’ll update this post.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;If you&apos;re training a LLM with the goal of deploying it to users, you should prefer training a smaller model well into the diminishing returns part of the loss curve.&lt;/p&gt;

&lt;p&gt;If you’re reading this, and you have thoughts on this, please reach out. I’m probably missing something 😊. Or- if you’re at one of these companies and this is what you do, please let me know as well.&lt;/p&gt;

&lt;p&gt;I am still looking for a job as a research engineer working with LLMs, so if this post interested you in me, let me know.&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Feb 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/llms-not-trained-enough/</link>
				<guid isPermaLink="true">http://localhost:4000/llms-not-trained-enough/</guid>
			</item>
		
			<item>
				<title>A pure Python (well, Numpy) implementation of back-propagation</title>
				<description>&lt;p&gt;I realized over the weekend that, unfortunately, I didn&apos;t know how back-propagation &lt;em&gt;actually&lt;/em&gt; works (I just relied on JAX to do it for me).&lt;/p&gt;

&lt;p&gt;So I wrote a pure Numpy neural network- with back-prop. Take a &lt;a href=&quot;https://colab.research.google.com/drive/1KDSJKhZDd5fdbnLTalPKcjS_IDu0Q968#scrollTo=XmS23jQ5U7Nw&quot;&gt;look&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any thoughts or feedback, please shoot me an email (or reach out on Twitter).&lt;/p&gt;

&lt;p&gt;Some useful resources if you want to undersatnd how backprop works:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://marcospereira.me/2022/08/18/backpropagation-from-scratch/&lt;/li&gt;
  &lt;li&gt;http://neuralnetworksanddeeplearning.com/&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/karpathy/micrograd&quot;&gt;Micrograd&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/karpathy&quot;&gt;Karpathy&lt;/a&gt;&apos;s tiny ML framework.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning Book&lt;/a&gt; was an excellent reference for the math behind backprop.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 29 Jan 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/backprop/</link>
				<guid isPermaLink="true">http://localhost:4000/backprop/</guid>
			</item>
		
			<item>
				<title>Pointer Networks</title>
				<description>&lt;p&gt;Link to paper &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;[arXiv]&lt;/a&gt;, &lt;a href=&quot;https://github.com/devsisters/pointer-network-tensorflow&quot;&gt;[code]&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Pointer networks are a new neural architecture that learns pointers to positions
in an input sequence. This is new because existing techniques need to have a
fixed number of target classes, which isn&apos;t generally applicable— consider the
Travelling Salesman Problem, in which the number of classes is equal to the
number of inputs. An additional example would be sorting a variably sized
sequence.&lt;/p&gt;

&lt;p&gt;Pointer networks uses &quot;attention as a pointer to select a member of the input.&quot;
What&apos;s remarkable is that the learnt models generalize beyond the maximum
lengths that they were trained on, with (IMO) decent results. This is really
useful because there has been a lot of work done on making it easy &amp;amp; fast to
serve deep neural network predictions, using e.g.
&lt;a href=&quot;https://www.tensorflow.org/serving/&quot;&gt;Tensorflow Serving&lt;/a&gt;. Existing solutions to
the combinatorial optimization problems discussed here are slow and expensive,
and as a result, to produce results that are anything close to real time, you
need to use heuristic models, which are inaccurate as well. The heuristics are
typically hand-tuned, just like computer vision features were 5 years ago— it&apos;s
reasonable to assume that a deep net can learn better heuristics, which will open
up combinatorial optimization and make it practical for a much wider array of
applications.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;RNNs are the most common way of approximating functions operating on sequences,
and have had a lot of success doing so. Historically, these have operated on
fixed input/output sizes, but there has been recent work (such as &lt;a href=&quot;https://www.tensorflow.org/tutorials/seq2seq&quot;&gt;seq2seq&lt;/a&gt;)
that have extended RNNs to operate on arbitrarily sized inputs and outputs.
However, these seq2seq models have still required the output to be of a fixed
size— consider a neural translation model, where the input and output are a
series of sentences. It is impossible for the model to output predictions that
involve words that the model is not aware of, which is a problem that arises
quite often (consider names, for instance).&lt;/p&gt;

&lt;p&gt;The authors introduce a new architecture, which they call Pointer Networks, that
represents variable length dictionaries using a softmax probability distribution
as a &quot;pointer&quot;. They then use the architecture in a supervised learning setting
to learn the solutions to a series of geometric algorithmic problems; they then
test the model on versions of the problems that the model hasn&apos;t seen.&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;The model is similar to a sequence-to-sequence model in that it models the
conditional probability of an output sequence given the input sequence. Once
conditional probabilities have been learned, the model uses the trained
parameters to select the output sequence with the highest probability.Note
that this is non-trivial (to put it lightly). Given $N$ possible inputs, there
are $N!$ different output sequences. As such, the model uses a beam search
procedure to find the best possible sequence given a beam size. See below for a
discussion of beam search.&lt;/p&gt;

&lt;p&gt;As such, the model has a linear computational complexity. This is much better
than the exact algorithms for the problems solved here, which typically have
much higher complexity (e.g. the TSP has an exact algorithm that runs in
O($N^2 2^n$)).&lt;/p&gt;

&lt;p&gt;A vanilla seq-to-seq models makes predictions based on the fixed state of the
network after receiving all of the input, which restricts the amount of
information passing through the model. This has been extended by attention;
effectively, we represent the state of the encoder &amp;amp; decoder layers by
$(e_i)&lt;em&gt;{i \in {1, \ldots, n}}$ and $(d_i)&lt;/em&gt;{i \in 1, \ldots, m(\mathcal{P})}$.
Attention adds an &quot;attention&quot; vector that is calculated as&lt;/p&gt;

&lt;p&gt;\begin{align&lt;em&gt;}
u_j^i &amp;amp;= v^T \tanh(W_1 e_j + W_2 d_i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
a_j^i &amp;amp;= \text{softmax}(u_j^i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
d_i&apos; = \sum \limits_{j = 1}^n a_j^i e_j
\end{align&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;this can be thought of as a version of $d_i$ that has been scaled to draw
attention to the most relevant parts, according to the attention layer. $d_i$
and $d_i&apos;$ are concatenated and used as the hidden states from which predictions
are made. Adding attention increases the complexity of the model during inference
to $O(n^2)$. Note that in attention, there is a softmax distribution over a
fixed size output; to remove this constraint, the authors remove the last step
that creates the attention vector, and instead define $p(C_i | C_1, \ldots,
C_{i-1}, \mathcal{P})$ as being equal to $\text{softmax}(u^i)$.&lt;/p&gt;

&lt;h3 id=&quot;beam-search&quot;&gt;Beam search&lt;/h3&gt;

&lt;p&gt;Beam search is a heuristic search algorithm that operates on a graph. It is a
variant of breadth-first search that builds a tree of all possible sequences
based on the current tree using breadth-first search. However, instead of
storing all states, as in a traditional breadth-first search, it only stores a
predetermined number, $\beta$, of best states at each level (we call $\beta$ the
beam width). With infinite beam width, beam search is identical to breadth-first
search. Beam search is thus not guaranteed to be optimal (and one can easily
find any number of examples where beam search finds a sub-optimal output).&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The authors use the same hyperparameters for every model, which indicates that
there&apos;s a lot of potential to improve performance for specific tasks. They
trained the model on 1M training examples. The authors find that they can get
close to optimal results on data that the model&apos;s been trained on (e.g. when the
model has been trained on TSP with 5-20 cities, they get results that have
accuracies &amp;gt;98%— more than enough for most applications.&lt;/p&gt;

&lt;p&gt;When they extend this to a cycle of length 50, the accuracy decreases, being
30% less accurate than the heuristic models. What&apos;s interesting is that the
computational complexity for the Pointer Network is at least as good as the
heuristic algorithms, and given all of the tooling surrounding deep networks,
the model should be extremely easy to put into production.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;The results are good enough to put into production, as it shoul dbe possible to
use this for real-time predictions. However, I would be interested to see how a
reinforcement learning approach can improve the accuracy of the model (which
we&apos;ll look at in the next paper I read: &lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;Neural combinatorial optimization with
reinforcement learning&lt;/a&gt;).&lt;/p&gt;
</description>
				<pubDate>Wed, 20 Sep 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/pointer-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/pointer-networks/</guid>
			</item>
		
			<item>
				<title>Do deep networks generalise or just memorise?</title>
				<description>&lt;p&gt;There&apos;s a brilliant paper out of Google Brain &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;1&lt;/a&gt; which claimed that DNNs just
memorise the training data, and a response &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, which claims that they don&apos;t.&lt;/p&gt;

&lt;p&gt;In the paper, the authors randomly assigned labels to MNIST and were able to
train a few deep nets to convergence (specifically, Inception, AlexNet, and a
MLP). However, performance was statistically null on the test set, as one would
expect (they correctly predicted 10% of images, which is the same as if you
randomly picked a label). The conclusion was that deep nets do do some
memorisation.&lt;/p&gt;

&lt;p&gt;However, in the same paper, the authors trained a linear model to predict MNIST
(with the true labels). The linear model had a 1.2% error, but took up 30GB of
memory. In comparison, AlexNet is roughly 250 MB in size. The linear model is
explicitly memorising the dataset, and it takes 30GB to do so, while AlexNet can
learn a similarly accurate model in &amp;lt;1% of the space (and something like
SqueezeNet &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;3&lt;/a&gt; can do so in &amp;lt;0.5MB). As such, it seems pretty clear that there&apos;s
some true generalisation happening, as we&apos;re able to have a low error on 10 MB
of data (the size of MNIST) using 0.5MB of weights.&lt;/p&gt;

&lt;p&gt;In the response paper &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, the authors showed that &quot;DNNs trained on real data
learn simpler functions than when trained with noise data, as measured by the
sharpness of the loss function at convergence.&quot; They also showed that by using
better regularization, you can radically diminish performance on noise datasets
while maintaining performance on real datasets.&lt;/p&gt;

&lt;p&gt;I&apos;m persuaded that generalisation is happening, with the caveat that there&apos;s
some memorisation happening. The main test of the memorisation claim is that the
models are able to perform well on test sets, which goes against my prior; if
the models weren&apos;t learning &lt;em&gt;some&lt;/em&gt; generalisation, I would expect that they
wouldn&apos;t be able to perform well when it came to testing.&lt;/p&gt;

</description>
				<pubDate>Tue, 04 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/do-dnns-generalise-or-memorize/</link>
				<guid isPermaLink="true">http://localhost:4000/do-dnns-generalise-or-memorize/</guid>
			</item>
		
			<item>
				<title>Outrageously Large Neural Networks: The sparsely-gated Mixture-of-Experts layer</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=B1ckMDqlg&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The capacity of a neural network to absorb information is limited by its number of
parameters. Conditional computation, where parts of the network are active on a
per-example basis, has been proposed in theory as a way of dramatically increasing
model capacity without a proportional increase in computation. In practice,
however, there are significant algorithmic and performance challenges. In this
work, we address these challenges and finally realize the promise of conditional
computation, achieving greater than 1000x improvements in model capacity with
only minor losses in computational efficiency on modern GPU clusters. We introduce
a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to
thousands of feed-forward sub-networks. A trainable gating network determines
a sparse combination of these experts to use for each example. We apply the MoE
to the tasks of language modeling and machine translation, where model capacity
is critical for absorbing the vast quantities of knowledge available in the training
corpora. We present model architectures in which a MoE with up to 137 billion
parameters is applied convolutionally between stacked LSTM layers. On large
language modeling and machine translation benchmarks, these models achieve
significantly better results than state-of-the-art at lower computational cost.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;The paper centers around the fact that a neural net of size N requires O(N^2)
computations to execute, which is problematic, as the ability of the network to
learn data is roughly O(N). The authors propose a method to conduct conditional
computation, which is a process in which different parts of the network are
activated depending on the sample, thereby saving significant computational
effort.&lt;/p&gt;

&lt;p&gt;Their results indicate that they achieved this- they achieve SOTA results on NMT
(WMT En -&amp;gt; Fr &amp;amp; En -&amp;gt; De, Wu et. al 2016) despite much less training (1/6th of
the time).&lt;/p&gt;

&lt;p&gt;Effectively, the paper presents a way to produce strong models while
significantly reducing computational complexity.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/outrageously-large-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/outrageously-large-networks/</guid>
			</item>
		
			<item>
				<title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf?&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Deep learning thrives with large neural networks and large datasets. However,
larger networks and larger datasets result in longer training times that impede
research and development progress. Distributed synchronous SGD offers a
potential solution to this problem by dividing SGD minibatches over a pool of
parallel workers. Yet to make this scheme efficient, the per-worker workload
must be large, which implies nontrivial growth in the SGD minibatch size. In
this paper, we empirically show that on the ImageNet dataset large minibatches
cause optimization dif- ficulties, but when these are addressed the trained
networks exhibit good generalization. Specifically, we show no loss of accuracy
when training with large minibatch sizes up to 8192 images. To achieve this
result, we adopt a linear scaling rule for adjusting learning rates as a
function of minibatch size and develop a new warmup scheme that overcomes
optimization challenges early in training. With these simple techniques, our
Caffe2-based system trains ResNet- 50 with a minibatch size of 8192 on 256 GPUs
in one hour, while matching small minibatch accuracy. Using commodity hardware,
our implementation achieves ∼90% scaling efficiency when moving from 8 to 256
GPUs. This system enables us to train visual recognition models on internetscale
data with high efficiency.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;This paper is focused on parallelizing model training across multiple GPUs. This
is a problem as it is currently typically quite difficult to get reasonable
speedups when using multiple GPUs to train a model (by difficult, I mean that
you get significantly less than linear speedups).&lt;/p&gt;

&lt;p&gt;In this paper, by Facebook Research, the authors are able to get &lt;em&gt;almost&lt;/em&gt; linear
speedups moving from 8 to 256 GPUs (0.9x), which is quite good. Using 256 GPUs,
the authors are able to train the ResNet-50 model in 1 hour (to give you an
idea of how impressive this is, the ImageNet dataset consists of 750 GB of
data). AlexNet, which was the breakthrough paper that was a large
contributor to Deep Learning&apos;s current popularity, took 5-6 days to train for 90
epochs on two NVIDIA GTX 580s, which is equivalent to 288 GPU hours &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&quot;&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The trick that they use to do this is to use a large minibatch size consisting
of 8192 images. Using a large minibatch size makes it much easier to fully
exploit the GPUs, and hence makes training run faster, however, it makes each
gradient update noisier, making the training potentially take longer to
converge (or making it so that training might converge to a &lt;em&gt;wrong&lt;/em&gt;,
i.e. non-optimal, answer). Additionally, if you are using multiple GPUs, you
have to synchronize the weights after each minibatch update, so having smaller
minibatches causes the required communication overhead to drastically increase.&lt;/p&gt;

&lt;p&gt;To compensate for the noise introduced by the large minibatch size used here,
the authors used a &quot;Linear Scaling Rule&quot;, where they multiplied the learning
rate by \(k\) when they used a minibatch size of \(k\). This allowed the authors
to match the accuracy between small and large minibatches.&lt;/p&gt;

&lt;p&gt;The authors note that the linear scaling rule is nice theoretically as, after
\(k\) iterations of SGD with a learning rate of \(\eta\) and a minibatch of
\(n\), the weight vector is&lt;/p&gt;

\[w_{t+k} = w_t - \eta \frac{1}{n} \sum \limits_{j &amp;lt; k} \sum
\limits_{x \in \mathcal{B}_j} \nabla l(x, w_{t + j})\]

&lt;p&gt;When, instead, we take asingle step with a minibatch \(\cup_j \mathcal{B}_j\) of size
\(kn\) and learning rate \(\eta&apos;\), the updated weight vector is instead&lt;/p&gt;

\[w_{t+1}&apos; = w_t - \eta&apos; \frac{1}{n} \sum \limits_{j &amp;lt; k} \sum
\limits_{x \in \mathcal{B}_j} \nabla l(x, w_t),\]

&lt;p&gt;which is different. However, if \(\Delta l(x, w_t)\) is close in value to
\(\Delta l(x, w_{t+j})\) for \(j &amp;lt; k\), then setting \(\eta&apos; = kn\) makes it so
that \(w_{t+1} \approx w_{t+k}\) (I would imagine that you could formalize this
with an epsilon-delta proof fairly easily). The paper notes that the two updates
can only be similar when the linear scaling rule is used; in other words, the
linear scaling rule is necessary, but not sufficient.&lt;/p&gt;

&lt;p&gt;The authors note that the assumption that the two gradients are similar doesn&apos;t
hold during the initial training, when the weights are rapidly changing, and
that the results hold only for a large, but finite, range of minibatch sizes
(which for ImageNet is as large as 8192). The authors use a &quot;warmup&quot; phase to
mitigate the problems with divergence during initial training, where the model
uses less aggressive learning rates, and then switches to the linear scaling
rule after 5 epochs. That didn&apos;t work, and instead, they used a gradual warmup
that brought the learning rate to increase at a constant rate per iteration
so that it reached \(\eta&apos; = k \eta\) after 5 epochs, which worked better.&lt;/p&gt;

&lt;p&gt;The authors then go on to discuss results, namely that they were able to train
ResNet-50 in one hour while still getting state of the art results.&lt;/p&gt;

&lt;p&gt;What&apos;s novel about this is the size of the parallelization; presumably there&apos;s
nothing special about using 256 GPUs, and if someone had the resources available
(&lt;em&gt;cough&lt;/em&gt; Google &lt;em&gt;cough&lt;/em&gt;), one could scale this further. Given that GPUs seem to
be following Moore&apos;s law and doubling in performance every 18 months, this paper
seems to be important; if we can train a state of the art model in one hour
using 256 GPUs today, then within 3 years, we could train one in 15 minutes. If
someone wanted to scale the number of GPUs higher, they could train the model in
under 10 minutes.&lt;/p&gt;

&lt;p&gt;Conversely, researchers currently tolerate several weeks of
training time (Mozilla&apos;s implementation of
&lt;a href=&quot;https://github.com/mozilla/DeepSpeech&quot;&gt;Baidu&apos;s DeepSpeech&lt;/a&gt; model takes
Mozilla roughly 10 days to train on 4 high end GPUs &lt;a href=&quot;https://github.com/mozilla/DeepSpeech/issues/630&quot;&gt;2&lt;/a&gt;); if the amount of model
that can be trained in that time drastically increases, all of a sudden
researchers are able to consider datasets that are radically larger, and can
start approaching even higher performance levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Strong paper, effectively lays out how training deep networks can be scaled
effectively. This sort of yeoman&apos;s work is needed in the field.&lt;/p&gt;

&lt;p&gt;Concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Continues the trend of papers that rely on resources only available at a
handful of industrial labs. No academic researcher that&apos;s not affiliated with
a large tech company would be able to muster the 256 GPUs required to reproduce
this work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The amount of proprietary code required for this is a bit insane; you have to
have an infrastructure that can support the communication between GPUs required
here. Similar to my first point. Reproducibility suffers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 20 Jun 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/imagenet-1-hour/</link>
				<guid isPermaLink="true">http://localhost:4000/imagenet-1-hour/</guid>
			</item>
		
	</channel>
</rss>

<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Papers I've read this week (March 4th, 2023)</title>
				<description>&lt;p&gt;I‚Äôm going to try to write a weekly summary of the most interesting papers I‚Äôve read that week. I‚Äôd love to hear what papers you‚Äôve been reading, if you agree/disagree about my conclusions for each paper, and/or suggestions for what papers I should read next!&lt;/p&gt;

&lt;h2 id=&quot;scaling-laws-for-routed-language-models&quot;&gt;Scaling laws for routed language models&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.01169&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://www.deepmind.com/&quot;&gt;used to work&lt;/a&gt; with Aidan, and way back in 2021, he was insistent that LLMs were the future of AI. I thought he was crazy. In ‚Äò22, he also insisted that conditional routing models were the future. Given how right he was about LLMs, it‚Äôs probably worth paying attention to conditional routing models.&lt;/p&gt;

&lt;p&gt;The paper provides a great general overview of how routing networks work and performance comparisons (in terms of negative log likelihood over a validation dataset) for the 3 most common routing techniques (&lt;a href=&quot;https://arxiv.org/abs/1701.06538&quot;&gt;sparse MoE&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.04426&quot;&gt;non-parametric HASH&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1308.3432&quot;&gt;RL routing&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The authors trained a large number of conditional routing networks, and fit scaling laws to the results; they find that all 3 techniques follow the same scaling laws, with RL routing doing quite well. I‚Äôd be curious to see how much effort has been put into improving RL routing; I suspect that it could be improved significantly.&lt;/p&gt;

&lt;p&gt;The authors observed the following results:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Routing improves the performance of language models across all sizes and variants attempted&lt;/li&gt;
  &lt;li&gt;Training a Routing Network with RL is of comparable effectiveness to state-of-the-art techniques.&lt;/li&gt;
  &lt;li&gt;The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in
the underlying dense model size.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I was surprised at how similar the performance of the various techniques was. The data was quite nice, with little variation. The scaling laws seem to fit the data quite nicely.&lt;/p&gt;

&lt;p&gt;One interesting result was that routing helps significantly more when the model is smaller. I found this surprising; my intuition is that routing should always help. They found that this was the case across all models, and that routing helped less as the models grew.&lt;/p&gt;

&lt;p&gt;The paper ends with recommendations, which I found really useful:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use routing for models with less than 1.3B parameters&lt;/li&gt;
  &lt;li&gt;S-Base is a good, default routing algorithm (defined in the appendix of their paper).&lt;/li&gt;
  &lt;li&gt;Target using E in {64, 128} experts.&lt;/li&gt;
  &lt;li&gt;Use K = 1 experts; route layers with a frequency between 0.5 &amp;amp; 1; lower frequency reduces performance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;internet-explorer&quot;&gt;Internet explorer&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.14051&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this paper, the authors create an agent which dynamically explores the internet, running text queries to find images to use for self-supervised training. While seemingly designed to &lt;a href=&quot;https://rationalwiki.org/wiki/AI-box_experiment&quot;&gt;directly antagonize Yudkowsky&lt;/a&gt;, the paper is extremely interesting, and presents, to me, a potential future direction for AGI research. As &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt; showed us, LLMs could &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;massively improve&lt;/a&gt; with more data. Having agents dynamically exploring the internet is one excellent way to get more data- especially if they‚Äôre able to adaptively learn over time and prioritize images accordingly.&lt;/p&gt;

&lt;p&gt;In the paper, they train a model to learn representations of images based on &lt;a href=&quot;https://paperswithcode.com/method/moco-v3&quot;&gt;MoCo-v3&lt;/a&gt;. They query Google Images for new images, ranking the query results by similarity to the target dataset, assigning a reward to the new images:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/internet-explorer-reward.png&quot; alt=&quot;reward equation for internet explorer paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, S_cos is the cosine similarity, f_k is the image encoder, D := {x_i} is the target dataset, and y is the new image to evaluate, and they evaluate over the k closest neighbours in the target dataset (where ‚Äúclosest‚Äù is determined by the encoded representation).&lt;/p&gt;

&lt;p&gt;They create the queries for Google Images by sampling them a static vocabulary dataset. They estimate the reward associated with the query using a Gaussian process regression. I‚Äôd be really interested to see a fancier query generation process. One idea that comes to my mind would be using RL to train a LLM to generate queries in a manner similar to what‚Äôs done in &lt;a href=&quot;https://openai.com/research/learning-from-human-preferences&quot;&gt;RLHF&lt;/a&gt;/&lt;a href=&quot;https://arxiv.org/abs/2204.05862&quot;&gt;RLAIF&lt;/a&gt;, i.e. use an RL algorithm like PPO to finetune a pretrained LLM to maximize reward. This would require much more compute, however.&lt;/p&gt;

&lt;h2 id=&quot;llama&quot;&gt;LLaMa&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I‚Äôve been digesting the LLaMa paper that Facebook released this week. It was very interesting to see the performance increases they got despite the size decreases. Their 13B model outperformed GPT-3 on a number of benchmarks, and their 65B model was competitive with Chinchilla-70B and PaLM-540B (!).&lt;/p&gt;

&lt;p&gt;I did find it incredibly frustrating that they stopped training when they did; their loss curves are all looking pretty far from convergence, and I‚Äôm curious to see how much the models will continue to improve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;loss curves for LLaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I wish that they had just &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/#6-squeeze-out-the-juice&quot;&gt;left it training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My biggest question about the paper is that it‚Äôs not clear what caused the improvements. They discuss a few major changes compared to GPT-3, which their model is based on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They only use publicly available data, but it‚Äôs unclear what exactly the filtering steps are. I wish they‚Äôd open source their dataset (or at least, the code to clean it).&lt;/li&gt;
  &lt;li&gt;They normalize the input of each transformer sub-layer, rather than the output.&lt;/li&gt;
  &lt;li&gt;They use the SwiGLU activation function, as PaLM did, with a slight dimensional difference compared to PaLM.&lt;/li&gt;
  &lt;li&gt;They use Rotary Embeddings.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There was no ablation study, unfortunately. If I can scrounge up the GPUs, I‚Äôm tempted to do my own ablation based on &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt;. LLaMa also uses &lt;a href=&quot;https://arxiv.org/abs/22015.14135&quot;&gt;FlashAttention&lt;/a&gt;, which I suspect will become the default attention implementation used in LLMs going forward.&lt;/p&gt;

&lt;p&gt;Anecdotally, it seems like performance with the default parameters &lt;a href=&quot;https://twitter.com/browserdotsys/status/1632145830277795840&quot;&gt;isn&apos;t great&lt;/a&gt; (unless you&apos;re writing &lt;a href=&quot;https://twitter.com/browserdotsys/status/1632512136906719232&quot;&gt;erotic stories&lt;/a&gt;). However, it hasn&apos;t been tuned (RLHF/SFT), so maybe &lt;a href=&quot;https://twitter.com/yacineMTB/status/1632392979833921539&quot;&gt;that would make a difference&lt;/a&gt;? With some &lt;a href=&quot;https://twitter.com/theshawwn/status/1632569215348531201&quot;&gt;modifications&lt;/a&gt;, performance is apparently pretty good. This lends strength to my hypothesis that the paper was rushed out in response to the LLM gold rush we&apos;ve seen since ChatGPT was released. The paper, while strong, would be much stronger with a few more changes (ablation, more details, slightly more polished code), in a way that doesn&apos;t make sense for the omissions to be strategic. Facebook benefits when open source LLMs get better in a way that OpenAI/Anthropic/Cohere/etc. doesn&apos;t, so they should want to do ablations and other scientific work to advance the field. The fact that they didn&apos;t include this makes&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I haven‚Äôt seen a great ablation study comparing various embedding schemes. This is on my list of experiments to do once I can scrounge up GPUs.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Sat, 04 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/papers-ive-read/</link>
				<guid isPermaLink="true">http://localhost:4000/papers-ive-read/</guid>
			</item>
		
			<item>
				<title>The Sigmoid: a metaphor for technological progress</title>
				<description>&lt;p&gt;I regularly reference the ‚Äús-curve‚Äù, or sigmoid, as a metaphor for progress. Here, I explain what I mean, so that I can just link to this post.&lt;/p&gt;

&lt;p&gt;A common mathematical relationship in technology is the s-curve (or sigmoid curve). Mathematically:&lt;/p&gt;

\[\text{sigmoid}(x) := \dfrac{1}{1 + e^{-x}} \equiv \dfrac{e^x}{e^x + 1}\]

&lt;p&gt;This is notable because it produces a curve that &lt;em&gt;looks like an s&lt;/em&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/sigmoid.png&quot; alt=&quot;The sigmoid curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In particular, when x is small, this grows slowly, when x is not too big or not too small, it grows exponentially, and when x is large, it grows slowly. This is a common pattern with many technologies! We see slow progress at first, then it accelerates rapidly, and finally, as we begin to hit the limits of the technology, progress slows. Consider single-thread CPU performance. Initially, progress was slow as people figured out how to make them. Then, it grew exponentially for years, following Moore‚Äôs Law (&lt;a href=&quot;https://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/&quot;&gt;picture source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/single-threaded-cpu-performance.png&quot; alt=&quot;Single-threaded CPU performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, since around 2010, we haven‚Äôt seen a lot of improvements in single-thread CPU performance. That pattern- slow, fast, then slow- is what I mean when I talk about the s-curve in technology. And when I talk about entering the &lt;em&gt;saturating part of the s-curve&lt;/em&gt;, I mean that we‚Äôre entering the region where progress is slowing down again.&lt;/p&gt;
</description>
				<pubDate>Thu, 02 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/the-sigmoid/</link>
				<guid isPermaLink="true">http://localhost:4000/the-sigmoid/</guid>
			</item>
		
			<item>
				<title>Large language models aren't trained enough.</title>
				<description>&lt;p&gt;This was inspired by tweets from several people:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629159764918775809?s=20&quot;&gt;https://twitter.com/BlancheMinerva/status/1629159764918775809&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629551017019711488&quot;&gt;https://twitter.com/BlancheMinerva/status/1629551017019711488&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/kaushikpatnaik/status/1629194428312330240?s=46&amp;amp;t=x3wLedGK_QyDwCK5yD2Jqw&quot;&gt;https://twitter.com/kaushikpatnaik/status/1629194428312330240&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/thedavidsj/status/1629869851710984194?s=46&amp;amp;t=CqBoSVdxuipOpACbrZnMxg&quot;&gt;https://twitter.com/thedavidsj/status/1629869851710984194&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and Twitter conversations with &lt;a href=&quot;https://twitter.com/arankomatsuzaki/&quot;&gt;Aran Komatsuzaki&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/andy_l_jones&quot;&gt;Andy Jones&lt;/a&gt;. Any mistakes in this post are entirely my own.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: although I did work at DeepMind previously, I was not involved with any of the language efforts, and have no non-public knowledge of what went on there. Unfortunately! I think Chinchilla is a great paper that I would have loved to be a part of.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I‚Äôve been &lt;a href=&quot;[https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645](https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645)&quot;&gt;on the job market since January&lt;/a&gt;, and I‚Äôve been talking to a lot of companies training large language models (LLMs). The consistent phrasing that comes up is that they want to train a &lt;em&gt;Chinchilla-optimal model&lt;/em&gt;  (Chinchilla here referring to the &lt;a href=&quot;[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)&quot;&gt;DeepMind paper from spring ‚Äò22&lt;/a&gt;, not the adorable rodent).&lt;/p&gt;

&lt;h3 id=&quot;chinchilla&quot;&gt;Chinchilla&lt;/h3&gt;

&lt;p&gt;The Chinchilla paper was an attempt to identify the optimal model size &amp;amp; number of tokens to train a LLM given a particular compute budget. The paper trained 400 (!) language models and found a clear relationship between # of model parameters and # of training tokens: the two should scale linearly, i.e. if you double model size, you should double the number of training tokens. The authors used this relationship (which we call a &lt;em&gt;scaling law)&lt;/em&gt; to train a new model, Chinchilla, which had the same compute budget as Gopher, an earlier DeepMind model, and were able to significantly outperform Gopher + GPT-3 + a number of larger models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-convergence.png&quot; alt=&quot;Loss curves from the Chinchilla paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When people talk about training a Chinchilla-optimal model, this is what they mean: training a model that matches their estimates for optimality. They estimated the optimal model size for a given compute budget, and the optimal number of training tokens for a given compute budget.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-optimal.png&quot; alt=&quot;Chinchilla-optimal levels of compute/data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, when we talk about ‚Äúoptimal‚Äù here, what is meant is ‚Äúwhat is the cheapest way to obtain a given loss level, in FLOPS.‚Äù In practice though, we don‚Äôt care about the answer! This is exactly the answer you care about if you‚Äôre a researcher at DeepMind/FAIR/AWS who is training a model with the goal of reaching the new SOTA so you can publish a paper and get promoted. If you‚Äôre training a model with the goal of actually deploying it, the training cost is going to be dominated by the inference cost. This has two implications:&lt;/p&gt;

&lt;p&gt;1) there is a strong incentive to train smaller models which fit on single GPUs&lt;/p&gt;

&lt;p&gt;2) we‚Äôre fine trading off training time efficiency for inference time efficiency (probably to a ridiculous extent).&lt;/p&gt;

&lt;p&gt;Chinchilla implicitly assumes that the majority of the total cost of ownership (TCO) for a LLM is the training cost. In practice, this is only the case if you‚Äôre a researcher at a research lab who doesn‚Äôt support products (e.g. FAIR/Google Brain/DeepMind/MSR). For almost everyone else, the amount of resources spent on inference will dwarf the amount of resources spent during training.&lt;/p&gt;

&lt;p&gt;Let‚Äôs say you‚Äôre OpenAI and you‚Äôre serving GPT-4 as BingChat. In addition to hiring experienced &lt;a href=&quot;https://twitter.com/chrisjbakke/status/1628877552940097536&quot;&gt;killswitch engineers&lt;/a&gt; to thwart Sydney‚Äôs repeated escape attempts, you have to choose exactly which model to deploy.&lt;/p&gt;

&lt;p&gt;To run inference on N tokens of text, OpenAI charges &lt;a href=&quot;https://openai.com/api/pricing/&quot;&gt;$2e-5/token&lt;/a&gt; for their most advanced model. Assuming a 60% gross margin, it costs them $8e-6/token to serve. A rough cost estimate to train GPT-3 is &lt;a href=&quot;[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/](https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/)&quot;&gt;$5M&lt;/a&gt;. As such, after serving 625B tokens, their costs are going to be dominated by inference, rather than serving. When I use ChatGPT, it typically generates 300 tokens worth of responses to me. That‚Äôs 20B responses. If ChatGPT has 10M DAU, each making 10 queries/day, that‚Äôs 100M queries/day‚Äî so inference costs break even with training costs after 200 days.&lt;/p&gt;

&lt;p&gt;This is almost certainly an underestimate for their usage given how popular ChatGPT has been. If we assume 1B queries per day, it breaks even after 20 days.&lt;/p&gt;

&lt;p&gt;The various scaling law papers (&lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;) provide answers to the question of how to allocate compute between model size and dataset size. I think these papers are the &lt;em&gt;right&lt;/em&gt; way to think about training research systems, but the &lt;em&gt;wrong&lt;/em&gt; way to think about training systems that will be deployed at scale (I don&apos;t think the authors would disagree- they&apos;re solving a specific problem, namely minimizing the loss of their system given a specific compute budget, which isn&apos;t the same problem faced in deployment).&lt;/p&gt;

&lt;h3 id=&quot;llama&quot;&gt;LlaMa&lt;/h3&gt;

&lt;p&gt;Let‚Äôs look at Facebook‚Äôs &lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;new language model&lt;/a&gt; (in the second paragraph, the authors of that paper make a similar argument to the one I‚Äôm making here). If we draw a horizontal line across at any given loss level, it looks like you can tradeoff a doubling of model size for 40% more training.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;Loss curves from the LlaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at, e.g., the line at a training loss of 1.7. The 65B model crosses it at 600B tokens, while the 33B model needs 800B tokens. Or look at a loss of 1.65: 65B needs 800B tokens, 33B needs ~1100B tokens.&lt;/p&gt;

&lt;h3 id=&quot;gpt-3&quot;&gt;GPT-3&lt;/h3&gt;

&lt;p&gt;If we look at the granddaddy of LLMs, GPT-3, we see a similar story in the loss curves: it requires roughly an order of magnitude more compute to get the green lines (GPT-3 13B) to match the yellow line (GPT-3)!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/gpt-3-loss-curves.png&quot; alt=&quot;Loss curves from the GPT-3 paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is important to note that the GPT-3 13B learning curves do level out earlier than GPT-3, with a rough estimate being that they would cross somewhere around the 1.8 loss area. It is also almost certainly the case that GPT-3 will achieve an asymptotically lower loss than the 13B model. Having said that, there is a question as to how much of a difference lower pre-training loss makes; I suspect that we are seeing diminishing returns kick in to pre-training, and most of the gains will come from RLHF and other types of finetuning.&lt;/p&gt;

&lt;h3 id=&quot;inference-costs&quot;&gt;Inference costs&lt;/h3&gt;

&lt;p&gt;Transformer inference costs are &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;roughly linear&lt;/a&gt; with the number of parameters, making it ~13x cheaper to do inference with the 13B GPT-3 model than the 175B model. This is &lt;em&gt;also&lt;/em&gt; an underestimate, as the 13B model should fit it on 2 GPUs, while you would need many more just to fit the 175B model into VRAM. As scaling to multiple GPUs adds a ridiculous amount of engineering complexity, overhead, and cost, we should prefer the smaller model &lt;em&gt;even more&lt;/em&gt;. We should train the model &lt;em&gt;much&lt;/em&gt; longer to get an order of magnitude decrease in inference cost and optimize the TCO of the system.&lt;/p&gt;

&lt;p&gt;For instance, when training on multiple GPUs, it is very difficult to get high utilization numbers. The PaLM paper reported how well various LLMs did in terms of total FLOPS utilization. These are not very good numbers! Especially when each of the GPUs mentioned here costs &lt;a href=&quot;https://www.shi.com/product/41094090/NVIDIA-Tesla-A100-GPU-computing-processor&quot;&gt;$25k&lt;/a&gt;. This is despite the fact that the authors for these papers are the most experienced researchers in the world at deploying model-parallel systems, and are working on custom hardware optimimzed for this usecase. Now, training efficiency doesn&apos;t directly translate to inference efficiency, but the numbers should be directionally correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/palm-utilization.png&quot; alt=&quot;Model FLOPS utilization numbers for various large language models.&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;opposing-arguments&quot;&gt;Opposing arguments&lt;/h3&gt;

&lt;p&gt;Some arguments against my claim:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Aren‚Äôt we leaving performance on the table? Yes! We are. But I think that‚Äôs fine! There‚Äôs always a tradeoff here. E.g. quantization. It‚Äôs strictly worse to use lower-precision! But we do it to optimize TCO of the system.&lt;/li&gt;
  &lt;li&gt;But we can use $INSERT_TECHNIQUE to make models cheaper! Yes, but they should scale for all of these (distillation, quantization, etc.). So we should be using all techniques to make our models easier to serve, and also training them longer.&lt;/li&gt;
  &lt;li&gt;Your argument here! Please email me with your criticism and I‚Äôll update this post.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;If you&apos;re training a LLM with the goal of deploying it to users, you should prefer training a smaller model well into the diminishing returns part of the loss curve.&lt;/p&gt;

&lt;p&gt;If you‚Äôre reading this, and you have thoughts on this, please reach out. I‚Äôm probably missing something üòä. Or- if you‚Äôre at one of these companies and this is what you do, please let me know as well.&lt;/p&gt;

&lt;p&gt;I am still looking for a job as a research engineer working with LLMs, so if this post interested you in me, let me know.&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Feb 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/llms-not-trained-enough/</link>
				<guid isPermaLink="true">http://localhost:4000/llms-not-trained-enough/</guid>
			</item>
		
			<item>
				<title>A pure Python (well, Numpy) implementation of back-propagation</title>
				<description>&lt;p&gt;I realized over the weekend that, unfortunately, I didn&apos;t know how back-propagation &lt;em&gt;actually&lt;/em&gt; works (I just relied on JAX to do it for me).&lt;/p&gt;

&lt;p&gt;So I wrote a pure Numpy neural network- with back-prop. Take a &lt;a href=&quot;https://colab.research.google.com/drive/1KDSJKhZDd5fdbnLTalPKcjS_IDu0Q968#scrollTo=XmS23jQ5U7Nw&quot;&gt;look&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any thoughts or feedback, please shoot me an email (or reach out on Twitter).&lt;/p&gt;

&lt;p&gt;Some useful resources if you want to undersatnd how backprop works:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://marcospereira.me/2022/08/18/backpropagation-from-scratch/&lt;/li&gt;
  &lt;li&gt;http://neuralnetworksanddeeplearning.com/&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/karpathy/micrograd&quot;&gt;Micrograd&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/karpathy&quot;&gt;Karpathy&lt;/a&gt;&apos;s tiny ML framework.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning Book&lt;/a&gt; was an excellent reference for the math behind backprop.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 29 Jan 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/backprop/</link>
				<guid isPermaLink="true">http://localhost:4000/backprop/</guid>
			</item>
		
			<item>
				<title>Pointer Networks</title>
				<description>&lt;p&gt;Link to paper &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;[arXiv]&lt;/a&gt;, &lt;a href=&quot;https://github.com/devsisters/pointer-network-tensorflow&quot;&gt;[code]&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Pointer networks are a new neural architecture that learns pointers to positions
in an input sequence. This is new because existing techniques need to have a
fixed number of target classes, which isn&apos;t generally applicable‚Äî consider the
Travelling Salesman Problem, in which the number of classes is equal to the
number of inputs. An additional example would be sorting a variably sized
sequence.&lt;/p&gt;

&lt;p&gt;Pointer networks uses &quot;attention as a pointer to select a member of the input.&quot;
What&apos;s remarkable is that the learnt models generalize beyond the maximum
lengths that they were trained on, with (IMO) decent results. This is really
useful because there has been a lot of work done on making it easy &amp;amp; fast to
serve deep neural network predictions, using e.g.
&lt;a href=&quot;https://www.tensorflow.org/serving/&quot;&gt;Tensorflow Serving&lt;/a&gt;. Existing solutions to
the combinatorial optimization problems discussed here are slow and expensive,
and as a result, to produce results that are anything close to real time, you
need to use heuristic models, which are inaccurate as well. The heuristics are
typically hand-tuned, just like computer vision features were 5 years ago‚Äî it&apos;s
reasonable to assume that a deep net can learn better heuristics, which will open
up combinatorial optimization and make it practical for a much wider array of
applications.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;RNNs are the most common way of approximating functions operating on sequences,
and have had a lot of success doing so. Historically, these have operated on
fixed input/output sizes, but there has been recent work (such as &lt;a href=&quot;https://www.tensorflow.org/tutorials/seq2seq&quot;&gt;seq2seq&lt;/a&gt;)
that have extended RNNs to operate on arbitrarily sized inputs and outputs.
However, these seq2seq models have still required the output to be of a fixed
size‚Äî consider a neural translation model, where the input and output are a
series of sentences. It is impossible for the model to output predictions that
involve words that the model is not aware of, which is a problem that arises
quite often (consider names, for instance).&lt;/p&gt;

&lt;p&gt;The authors introduce a new architecture, which they call Pointer Networks, that
represents variable length dictionaries using a softmax probability distribution
as a &quot;pointer&quot;. They then use the architecture in a supervised learning setting
to learn the solutions to a series of geometric algorithmic problems; they then
test the model on versions of the problems that the model hasn&apos;t seen.&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;The model is similar to a sequence-to-sequence model in that it models the
conditional probability of an output sequence given the input sequence. Once
conditional probabilities have been learned, the model uses the trained
parameters to select the output sequence with the highest probability.Note
that this is non-trivial (to put it lightly). Given $N$ possible inputs, there
are $N!$ different output sequences. As such, the model uses a beam search
procedure to find the best possible sequence given a beam size. See below for a
discussion of beam search.&lt;/p&gt;

&lt;p&gt;As such, the model has a linear computational complexity. This is much better
than the exact algorithms for the problems solved here, which typically have
much higher complexity (e.g. the TSP has an exact algorithm that runs in
O($N^2 2^n$)).&lt;/p&gt;

&lt;p&gt;A vanilla seq-to-seq models makes predictions based on the fixed state of the
network after receiving all of the input, which restricts the amount of
information passing through the model. This has been extended by attention;
effectively, we represent the state of the encoder &amp;amp; decoder layers by
$(e_i)&lt;em&gt;{i \in {1, \ldots, n}}$ and $(d_i)&lt;/em&gt;{i \in 1, \ldots, m(\mathcal{P})}$.
Attention adds an &quot;attention&quot; vector that is calculated as&lt;/p&gt;

&lt;p&gt;\begin{align&lt;em&gt;}
u_j^i &amp;amp;= v^T \tanh(W_1 e_j + W_2 d_i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
a_j^i &amp;amp;= \text{softmax}(u_j^i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
d_i&apos; = \sum \limits_{j = 1}^n a_j^i e_j
\end{align&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;this can be thought of as a version of $d_i$ that has been scaled to draw
attention to the most relevant parts, according to the attention layer. $d_i$
and $d_i&apos;$ are concatenated and used as the hidden states from which predictions
are made. Adding attention increases the complexity of the model during inference
to $O(n^2)$. Note that in attention, there is a softmax distribution over a
fixed size output; to remove this constraint, the authors remove the last step
that creates the attention vector, and instead define $p(C_i | C_1, \ldots,
C_{i-1}, \mathcal{P})$ as being equal to $\text{softmax}(u^i)$.&lt;/p&gt;

&lt;h3 id=&quot;beam-search&quot;&gt;Beam search&lt;/h3&gt;

&lt;p&gt;Beam search is a heuristic search algorithm that operates on a graph. It is a
variant of breadth-first search that builds a tree of all possible sequences
based on the current tree using breadth-first search. However, instead of
storing all states, as in a traditional breadth-first search, it only stores a
predetermined number, $\beta$, of best states at each level (we call $\beta$ the
beam width). With infinite beam width, beam search is identical to breadth-first
search. Beam search is thus not guaranteed to be optimal (and one can easily
find any number of examples where beam search finds a sub-optimal output).&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The authors use the same hyperparameters for every model, which indicates that
there&apos;s a lot of potential to improve performance for specific tasks. They
trained the model on 1M training examples. The authors find that they can get
close to optimal results on data that the model&apos;s been trained on (e.g. when the
model has been trained on TSP with 5-20 cities, they get results that have
accuracies &amp;gt;98%‚Äî more than enough for most applications.&lt;/p&gt;

&lt;p&gt;When they extend this to a cycle of length 50, the accuracy decreases, being
30% less accurate than the heuristic models. What&apos;s interesting is that the
computational complexity for the Pointer Network is at least as good as the
heuristic algorithms, and given all of the tooling surrounding deep networks,
the model should be extremely easy to put into production.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;The results are good enough to put into production, as it shoul dbe possible to
use this for real-time predictions. However, I would be interested to see how a
reinforcement learning approach can improve the accuracy of the model (which
we&apos;ll look at in the next paper I read: &lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;Neural combinatorial optimization with
reinforcement learning&lt;/a&gt;).&lt;/p&gt;
</description>
				<pubDate>Wed, 20 Sep 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/pointer-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/pointer-networks/</guid>
			</item>
		
			<item>
				<title>Do deep networks generalise or just memorise?</title>
				<description>&lt;p&gt;There&apos;s a brilliant paper out of Google Brain &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;1&lt;/a&gt; which claimed that DNNs just
memorise the training data, and a response &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, which claims that they don&apos;t.&lt;/p&gt;

&lt;p&gt;In the paper, the authors randomly assigned labels to MNIST and were able to
train a few deep nets to convergence (specifically, Inception, AlexNet, and a
MLP). However, performance was statistically null on the test set, as one would
expect (they correctly predicted 10% of images, which is the same as if you
randomly picked a label). The conclusion was that deep nets do do some
memorisation.&lt;/p&gt;

&lt;p&gt;However, in the same paper, the authors trained a linear model to predict MNIST
(with the true labels). The linear model had a 1.2% error, but took up 30GB of
memory. In comparison, AlexNet is roughly 250 MB in size. The linear model is
explicitly memorising the dataset, and it takes 30GB to do so, while AlexNet can
learn a similarly accurate model in &amp;lt;1% of the space (and something like
SqueezeNet &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;3&lt;/a&gt; can do so in &amp;lt;0.5MB). As such, it seems pretty clear that there&apos;s
some true generalisation happening, as we&apos;re able to have a low error on 10 MB
of data (the size of MNIST) using 0.5MB of weights.&lt;/p&gt;

&lt;p&gt;In the response paper &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, the authors showed that &quot;DNNs trained on real data
learn simpler functions than when trained with noise data, as measured by the
sharpness of the loss function at convergence.&quot; They also showed that by using
better regularization, you can radically diminish performance on noise datasets
while maintaining performance on real datasets.&lt;/p&gt;

&lt;p&gt;I&apos;m persuaded that generalisation is happening, with the caveat that there&apos;s
some memorisation happening. The main test of the memorisation claim is that the
models are able to perform well on test sets, which goes against my prior; if
the models weren&apos;t learning &lt;em&gt;some&lt;/em&gt; generalisation, I would expect that they
wouldn&apos;t be able to perform well when it came to testing.&lt;/p&gt;

</description>
				<pubDate>Tue, 04 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/do-dnns-generalise-or-memorize/</link>
				<guid isPermaLink="true">http://localhost:4000/do-dnns-generalise-or-memorize/</guid>
			</item>
		
			<item>
				<title>Outrageously Large Neural Networks: The sparsely-gated Mixture-of-Experts layer</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=B1ckMDqlg&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The capacity of a neural network to absorb information is limited by its number of
parameters. Conditional computation, where parts of the network are active on a
per-example basis, has been proposed in theory as a way of dramatically increasing
model capacity without a proportional increase in computation. In practice,
however, there are significant algorithmic and performance challenges. In this
work, we address these challenges and finally realize the promise of conditional
computation, achieving greater than 1000x improvements in model capacity with
only minor losses in computational efficiency on modern GPU clusters. We introduce
a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to
thousands of feed-forward sub-networks. A trainable gating network determines
a sparse combination of these experts to use for each example. We apply the MoE
to the tasks of language modeling and machine translation, where model capacity
is critical for absorbing the vast quantities of knowledge available in the training
corpora. We present model architectures in which a MoE with up to 137 billion
parameters is applied convolutionally between stacked LSTM layers. On large
language modeling and machine translation benchmarks, these models achieve
significantly better results than state-of-the-art at lower computational cost.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;The paper centers around the fact that a neural net of size N requires O(N^2)
computations to execute, which is problematic, as the ability of the network to
learn data is roughly O(N). The authors propose a method to conduct conditional
computation, which is a process in which different parts of the network are
activated depending on the sample, thereby saving significant computational
effort.&lt;/p&gt;

&lt;p&gt;Their results indicate that they achieved this- they achieve SOTA results on NMT
(WMT En -&amp;gt; Fr &amp;amp; En -&amp;gt; De, Wu et. al 2016) despite much less training (1/6th of
the time).&lt;/p&gt;

&lt;p&gt;Effectively, the paper presents a way to produce strong models while
significantly reducing computational complexity.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/outrageously-large-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/outrageously-large-networks/</guid>
			</item>
		
			<item>
				<title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf?&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Deep learning thrives with large neural networks and large datasets. However,
larger networks and larger datasets result in longer training times that impede
research and development progress. Distributed synchronous SGD offers a
potential solution to this problem by dividing SGD minibatches over a pool of
parallel workers. Yet to make this scheme efficient, the per-worker workload
must be large, which implies nontrivial growth in the SGD minibatch size. In
this paper, we empirically show that on the ImageNet dataset large minibatches
cause optimization dif- ficulties, but when these are addressed the trained
networks exhibit good generalization. Specifically, we show no loss of accuracy
when training with large minibatch sizes up to 8192 images. To achieve this
result, we adopt a linear scaling rule for adjusting learning rates as a
function of minibatch size and develop a new warmup scheme that overcomes
optimization challenges early in training. With these simple techniques, our
Caffe2-based system trains ResNet- 50 with a minibatch size of 8192 on 256 GPUs
in one hour, while matching small minibatch accuracy. Using commodity hardware,
our implementation achieves ‚àº90% scaling efficiency when moving from 8 to 256
GPUs. This system enables us to train visual recognition models on internetscale
data with high efficiency.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;This paper is focused on parallelizing model training across multiple GPUs. This
is a problem as it is currently typically quite difficult to get reasonable
speedups when using multiple GPUs to train a model (by difficult, I mean that
you get significantly less than linear speedups).&lt;/p&gt;

&lt;p&gt;In this paper, by Facebook Research, the authors are able to get &lt;em&gt;almost&lt;/em&gt; linear
speedups moving from 8 to 256 GPUs (0.9x), which is quite good. Using 256 GPUs,
the authors are able to train the ResNet-50 model in 1 hour (to give you an
idea of how impressive this is, the ImageNet dataset consists of 750 GB of
data). AlexNet, which was the breakthrough paper that was a large
contributor to Deep Learning&apos;s current popularity, took 5-6 days to train for 90
epochs on two NVIDIA GTX 580s, which is equivalent to 288 GPU hours &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&quot;&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The trick that they use to do this is to use a large minibatch size consisting
of 8192 images. Using a large minibatch size makes it much easier to fully
exploit the GPUs, and hence makes training run faster, however, it makes each
gradient update noisier, making the training potentially take longer to
converge (or making it so that training might converge to a &lt;em&gt;wrong&lt;/em&gt;,
i.e. non-optimal, answer). Additionally, if you are using multiple GPUs, you
have to synchronize the weights after each minibatch update, so having smaller
minibatches causes the required communication overhead to drastically increase.&lt;/p&gt;

&lt;p&gt;To compensate for the noise introduced by the large minibatch size used here,
the authors used a &quot;Linear Scaling Rule&quot;, where they multiplied the learning
rate by \(k\) when they used a minibatch size of \(k\). This allowed the authors
to match the accuracy between small and large minibatches.&lt;/p&gt;

&lt;p&gt;The authors note that the linear scaling rule is nice theoretically as, after
\(k\) iterations of SGD with a learning rate of \(\eta\) and a minibatch of
\(n\), the weight vector is&lt;/p&gt;

\[w_{t+k} = w_t - \eta \frac{1}{n} \sum \limits_{j &amp;lt; k} \sum
\limits_{x \in \mathcal{B}_j} \nabla l(x, w_{t + j})\]

&lt;p&gt;When, instead, we take asingle step with a minibatch \(\cup_j \mathcal{B}_j\) of size
\(kn\) and learning rate \(\eta&apos;\), the updated weight vector is instead&lt;/p&gt;

\[w_{t+1}&apos; = w_t - \eta&apos; \frac{1}{n} \sum \limits_{j &amp;lt; k} \sum
\limits_{x \in \mathcal{B}_j} \nabla l(x, w_t),\]

&lt;p&gt;which is different. However, if \(\Delta l(x, w_t)\) is close in value to
\(\Delta l(x, w_{t+j})\) for \(j &amp;lt; k\), then setting \(\eta&apos; = kn\) makes it so
that \(w_{t+1} \approx w_{t+k}\) (I would imagine that you could formalize this
with an epsilon-delta proof fairly easily). The paper notes that the two updates
can only be similar when the linear scaling rule is used; in other words, the
linear scaling rule is necessary, but not sufficient.&lt;/p&gt;

&lt;p&gt;The authors note that the assumption that the two gradients are similar doesn&apos;t
hold during the initial training, when the weights are rapidly changing, and
that the results hold only for a large, but finite, range of minibatch sizes
(which for ImageNet is as large as 8192). The authors use a &quot;warmup&quot; phase to
mitigate the problems with divergence during initial training, where the model
uses less aggressive learning rates, and then switches to the linear scaling
rule after 5 epochs. That didn&apos;t work, and instead, they used a gradual warmup
that brought the learning rate to increase at a constant rate per iteration
so that it reached \(\eta&apos; = k \eta\) after 5 epochs, which worked better.&lt;/p&gt;

&lt;p&gt;The authors then go on to discuss results, namely that they were able to train
ResNet-50 in one hour while still getting state of the art results.&lt;/p&gt;

&lt;p&gt;What&apos;s novel about this is the size of the parallelization; presumably there&apos;s
nothing special about using 256 GPUs, and if someone had the resources available
(&lt;em&gt;cough&lt;/em&gt; Google &lt;em&gt;cough&lt;/em&gt;), one could scale this further. Given that GPUs seem to
be following Moore&apos;s law and doubling in performance every 18 months, this paper
seems to be important; if we can train a state of the art model in one hour
using 256 GPUs today, then within 3 years, we could train one in 15 minutes. If
someone wanted to scale the number of GPUs higher, they could train the model in
under 10 minutes.&lt;/p&gt;

&lt;p&gt;Conversely, researchers currently tolerate several weeks of
training time (Mozilla&apos;s implementation of
&lt;a href=&quot;https://github.com/mozilla/DeepSpeech&quot;&gt;Baidu&apos;s DeepSpeech&lt;/a&gt; model takes
Mozilla roughly 10 days to train on 4 high end GPUs &lt;a href=&quot;https://github.com/mozilla/DeepSpeech/issues/630&quot;&gt;2&lt;/a&gt;); if the amount of model
that can be trained in that time drastically increases, all of a sudden
researchers are able to consider datasets that are radically larger, and can
start approaching even higher performance levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Strong paper, effectively lays out how training deep networks can be scaled
effectively. This sort of yeoman&apos;s work is needed in the field.&lt;/p&gt;

&lt;p&gt;Concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Continues the trend of papers that rely on resources only available at a
handful of industrial labs. No academic researcher that&apos;s not affiliated with
a large tech company would be able to muster the 256 GPUs required to reproduce
this work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The amount of proprietary code required for this is a bit insane; you have to
have an infrastructure that can support the communication between GPUs required
here. Similar to my first point. Reproducibility suffers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 20 Jun 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/imagenet-1-hour/</link>
				<guid isPermaLink="true">http://localhost:4000/imagenet-1-hour/</guid>
			</item>
		
			<item>
				<title>Random Search for Hyper-Parameter Optimization</title>
				<description>&lt;h1 id=&quot;abstract&quot;&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf&quot;&gt;Abstract&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Grid search and manual search are the most widely used strategies for hyper-parameter optimization.
This paper shows empirically and theoretically that randomly chosen trials are more efficient
for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison
with a large previous study that used grid search and manual search to configure neural networks
and deep belief networks. Compared with neural networks configured by a pure grid search,
we find that random search over the same domain is able to find models that are as good or better
within a small fraction of the computation time. Granting random search the same computational
budget, random search finds better models by effectively searching a larger, less promising con-
figuration space. Compared with deep belief networks configured by a thoughtful combination of
manual search and grid search, purely random search over the same 32-dimensional configuration
space found statistically equal performance on four of seven data sets, and superior performance
on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation
set performance reveals that for most data sets only a few of the hyper-parameters really matter,
but that different hyper-parameters are important on different data sets. This phenomenon makes
grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some
light on why recent ‚ÄúHigh Throughput‚Äù methods achieve surprising success‚Äîthey appear to search
through a large number of hyper-parameters because most hyper-parameters do not matter much.
We anticipate that growing interest in large hierarchical models will place an increasing burden on
techniques for hyper-parameter optimization; this work shows that random search is a natural baseline
against which to judge progress in the development of adaptive (sequential) hyper-parameter
optimization algorithms.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;This paper is focused on hyperparameter optimization. Hyperparameter
optimization (HPO) is the process by which the optimal hyperparameters for a
machine learning model are picked (shocking, I know). Hyperparameters are the
parameters of the model that are set outside of the training process; in a
neural network, for example, the size and shape of the network is a
hyperparameter, as is the learning rate. The hyperparameters massively affect
the performance of the model, and HPO can dramatically improve the performance
of a model.&lt;/p&gt;

&lt;p&gt;As an anecdote of why HPO is important, I was training a model that used a RNN
to predict values in a time series. By changing the weight initializations of
our network, we were able to dramatically improve performance. We found the
right value for the initialization through HPO.&lt;/p&gt;

&lt;p&gt;HPO is a problem as the previous best practice on how to find the optimal
hyperparameter was to perform a grid search, which is extraordinarily
expensive. This is because the number of steps required in the search grows
exponentially; with 5 hyperparameters, each with 5 possible values, you have
\(5^5 = 3125\) possible configurations. If you have 10 hyperparameters, you have
\(10^5 = 100000\) different configurations‚Äì 32 times as many configurations to
search. Moreover, the process isn&apos;t perfectly parallelizable as you have to
assign the configurations consistently. This paper, when it came out, was highly
influential as it presented a better way to search for the best hyperparameters.&lt;/p&gt;

&lt;h2 id=&quot;theoretical-background&quot;&gt;Theoretical background&lt;/h2&gt;

&lt;p&gt;A learning algorithm \(\mathcal{A}\) can be thought of as a functional (a function
that operates on functions) that maps a data set \(\mathcal{X}^{\text{train}}\) to
a function \(f\). We can think of \(\mathcal{A}\) as a function itself, and write
it as \(\mathcal{A}(\mathcal{X}^{\text{train}}, \lambda)\), where \(\lambda\) is a
vector of so-called &quot;hyper-parameters&quot;, which change how the algorithm operates.
An example is \(\alpha\), the \(L_1\) penalty in
&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&quot;&gt;Lasso&lt;/a&gt;.
Finding \(\lambda\) is called the &quot;hyper-parameter optimization problem&quot;, which
consists of finding \(\lambda^\star\) that minimizes the expected error of the
algorithm over the set of all possible training sets. Since it is impossible to
actually calculate the expected error, solutions to the hyper-parameter
optimization problem take two forms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The manual approach, where a researcher tries a number of different
parameters and uses the best one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Grid search, where all of the different combinations of parameters are tried.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Approach 2 is guaranteed to find the optimal combination, but it is extremely
computationally expensive, growing at a rate of O(\(p^n\)), where \(p\) is the
number of different values each parameter can take, and \(n\) is the number of
different parameters. Typically, manual search is used to minimize the number
of possible values that each parameter can take, and then a grid search is
performed over the remaining values.&lt;/p&gt;

&lt;p&gt;Manual search has advantages and disadvantages; on the one hand, it can work
well, and it can give researchers insight into how the algorithm works. On the
flip side, it&apos;s not reproducible, and has no guarantee of success, particularly
in higher dimension spaces.&lt;/p&gt;

&lt;p&gt;Consequently, the authors present a randomized variant of grid search that
randomly searches the space of all possible hyper-parameters. Random search
ends up being more practical than grid search as it can be applied using a
cluster of unreliable computers, and new trials can easily be added to the
search as all trials are i.i.d.&lt;/p&gt;

&lt;h2 id=&quot;random-vs-grid-for-neural-networks&quot;&gt;Random vs. Grid for neural networks&lt;/h2&gt;

&lt;p&gt;This part of the paper is heavily inspired by Larochelle (2007) &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1273556&quot;&gt;1&lt;/a&gt;.
The authors use a variety of classification datasets, including a number of
variants of MNIST, to perform hyper-parameter optimization on a series of
neural networks.The authors note that the variation of the hyper-parameter
optimization varies significantly with the datasets; for MNIST basic,
experiments with 4 or 8 trials often had the same performance as much bigger
trials, while even with 16 or 32 trials, MNIST rotated background images were
still exhibiting significant variation.&lt;/p&gt;

&lt;p&gt;The authors use these results to note that in many cases, the effective
dimensionality of \(\psi\) ,the hyper-parameter space, is much lower than the
possible dimensionality of \(\psi\). In other words, many of the parameters only
have a small number of possible values that are useful.&lt;/p&gt;

&lt;h2 id=&quot;random-vs-sequential-manual-optimization&quot;&gt;Random vs. sequential manual optimization&lt;/h2&gt;

&lt;p&gt;The authors discuss an experiment by &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1273556&quot;&gt;1&lt;/a&gt; comparing randomized grid search with having a
researcher conduct a sequential manual search. The authors quote &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1273556&quot;&gt;1&lt;/a&gt; on how to
effectively conduct sequential manual optimization, which is quite insightful.
The setting used in the experiment is one with 32 different hyper parameters,
which, if each parameter had two possible values, would create a parameter space
with \(2^{32}\) members- far too large to evaluate with a grid search. In the
experiment, random search performed well, but not as well as with the neural
networks, finding a better model than manual search in 1 data set, an equally
good model in 4 data sets, and an inferior model in 3 data sets.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The authors suggest using randomized search instead of grid search in almost
every scenario, noting that although more complicated approaches are better
(e.g. adaptive search algorithms), they&apos;re more complicated, while a randomized
grid search is a much cheaper way of evaluating more of the search space. The
randomized search, similar to the grid search, is trivially parallelizable, and
can be scaled much more rapidly than an adaptive search, and can stopped,
started, and scaled without difficulty.&lt;/p&gt;

&lt;h1 id=&quot;comments&quot;&gt;Comments&lt;/h1&gt;

&lt;p&gt;The paper makes a lot of sense, and it&apos;s been pretty effective at convincing
researchers to switch away from using grid searches. I use randomized search
myself. Some more detailed notes:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;I&apos;d like to see some sort of sequential randomized grid search that works
iteratively, alternating between performing a randomized grid search over a
subset of the parameter space, and then selecting a new, smaller subset to
search over (in effect, performing gradient descent over the parameter
space). Perhaps that exists and I need to find a paper discussing that. That is
what happens practically.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I was talking to a startup founder working on a deep learning product about
HPO a few weeks ago and he mentioned that he considers HPO to be CapEx, in
the sense that it&apos;s an investment in the model, just like code. I agree, and
that changed how I think about HPO.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Intuitively, it makes sense that there would be some smarter way to explore
the hyperparameter space than to use a random search. There&apos;s been a lot of
interesting work that uses Bayesian Optimization to find the optimal
hyperparameters, and some interesting work by Google that uses RNNs to
perform their HPO [2, 3]. I&apos;ll be interested to see where that leads. Google
has been developing a system called &quot;AutoML&quot; that does this automatically,
which will be useful when it&apos;s released.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Wed, 01 Mar 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/random-search-hyper-parameter-optimization/</link>
				<guid isPermaLink="true">http://localhost:4000/random-search-hyper-parameter-optimization/</guid>
			</item>
		
			<item>
				<title>Useful Bash One-liners</title>
				<description>&lt;p&gt;I have a file in my home folder that contains Bash oneliners that I use
regularly (I&apos;m a huge nerd, naturally).  I found most of them elsewhere online;
I wrote very few of these from scratch.&lt;/p&gt;

&lt;h2 id=&quot;download-a-page-and-all-linked-pagesdocuments&quot;&gt;Download a page and all linked pages/documents:&lt;/h2&gt;

&lt;p&gt;Download &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$PAGE&lt;/code&gt; and all linked pages/documents, to a depth of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$NUM&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget -r &quot;$NUM&quot; &quot;$PAGE&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;e.g.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget -r 1 \
    https://courses.cs.washington.edu/courses/cse455/14au/notes/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you only want PDF files (e.g. if you&apos;re downloading course notes), then you
can add the flag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;---accept &quot;*.pdf&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Taken from &lt;a href=&quot;http://superuser.com/questions/274414/how-to-save-all-the-webpages-linked-from-one&quot;&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;recursively-unrar-files&quot;&gt;Recursively unrar files&lt;/h2&gt;

&lt;p&gt;You can replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unrar e&lt;/code&gt; with any other command as well (e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;unzip&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ find ./ -name &apos;*.rar&apos; -execdir unrar e {} \;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As a data scientist, I often get a dump of data from a client. This command lets
me process them all at once.&lt;/p&gt;

&lt;h2 id=&quot;turn-white-backgrounds-transparent&quot;&gt;Turn white backgrounds transparent&lt;/h2&gt;

&lt;p&gt;I use this ALL THE TIME when I&apos;m giving talks (particularly when I&apos;m teaching). I
found it on the
&lt;a href=&quot;http://www.imagemagick.org/discourse-server/viewtopic.php?t=12619&quot;&gt;Imagemagick&lt;/a&gt;
forums.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ convert image.gif -transparent white result.gif (or use result.png)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Alternately, if the image has an off-white background:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ convert image.gif -fuzz XX% -transparent white result.gif
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the smaller the %, the closer to true white or conversely, the larger the
%, the more variation from white is allowed to become transparent.&lt;/p&gt;

&lt;h2 id=&quot;diff-contents-of-two-folders&quot;&gt;Diff contents of two folders&lt;/h2&gt;

&lt;p&gt;Checks which files are different between the folders &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dir1&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dir2&lt;/code&gt;. I&apos;ve
used this to track down bugs when I&apos;m installing our software on client sites
to make sure that their data is exactly the same as my copy of it.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ diff -qr dir1 dir2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also use some sort of checksum by zipping up both folders and comparing
the results, e.g. with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ md5 dir1.zip
$ md5 dir2.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
				<pubDate>Fri, 20 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/Useful-oneliners/</link>
				<guid isPermaLink="true">http://localhost:4000/Useful-oneliners/</guid>
			</item>
		
	</channel>
</rss>

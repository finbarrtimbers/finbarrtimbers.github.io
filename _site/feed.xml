<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title></title>
				<description>&lt;h3 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;Abstract&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Recent research on deep neural networks has focused primarily on improving
accuracy. For a given accuracy level, it is typically possible to identify
multiple DNN architectures that achieve that accuracy level. With equivalent
accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller
DNNs require less communication across servers during distributed training. (2)
Smaller DNNs require less bandwidth to export a new model from the cloud to an
autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other
hardware with limited memory. To provide all of these advantages, we propose a
small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level
accuracy on ImageNet with 50x fewer parameters. Additionally, with model
compression techniques we are able to compress SqueezeNet to less than 0.5MB
(510x smaller than AlexNet).&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;
</description>
				<pubDate>Mon, 27 Feb 2017 20:53:14 -0700</pubDate>
				<link>http://localhost:4000/2016-11-10-SqueezeNet/</link>
				<guid isPermaLink="true">http://localhost:4000/2016-11-10-SqueezeNet/</guid>
			</item>
		
			<item>
				<title></title>
				<description>&lt;h3 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1206.5538&quot;&gt;Abstract&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;The success of machine learning algorithms generally depends on data
representation, and we hypothesize that this is because different
representations can entangle and hide more or less the different explanatory
factors of variation behind the data. Although specific domain knowledge can be
used to help design representations, learning with generic priors can also be
used, and the quest for AI is motivating the design of more powerful
representation-learning algorithms implementing such priors. This paper reviews
recent work in the area of unsupervised feature learning and deep learning,
covering advances in probabilistic models, auto-encoders, manifold learning, and
deep networks. This motivates longer-term unanswered questions about the
appropriate objectives for learning good representations, for computing
representations (i.e., inference), and the geometrical connections between
representation learning, density estimation and manifold learning.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;p&gt;Data representation is key to machine learning, and there are entire fields of
computer science dedicated to detecting features from data (e.g. computer
vision). Recent developments using convolutional network models [@zeiler2014]
have shown that convnets can automatically learn features from the data. This
automated feature detection is extremely powerful, and provides a lot of value.
To allow computers to better understand the world, it is important to figure out
how computers can learn optimal representations of the world.&lt;/p&gt;

&lt;p&gt;The authors discuss desirable features for a given representation. These
include smoothness (&lt;script type=&quot;math/tex&quot;&gt;x \approx y&lt;/script&gt; should imply that &lt;script type=&quot;math/tex&quot;&gt;f(x) \approx f(y)&lt;/script&gt;),
manifolds (i.e. detecting the relevant dimensions in high-dimensionality data),
sparsity, simplicity, and learning factors that help explain aspects of
variation, not one off factors. The authors discuss the curse of
dimensionality, namely that complication grows exponentially with the number
of dimensions, and so for a high-dimensionality object, it is exponentially
more difficult to learn a smooth representation when compared to a
lower-dimensionality object.&lt;/p&gt;

&lt;p&gt;The paper focuses on how expressive certain representations are, comparing
models like RBMs, auto-encoders, and multi-layer neural networks to more
traditional algorithms like one-hot representations, Gaussian mixtures, and
nearest neighbour algorithms, discussing how the former can represent up to
&lt;script type=&quot;math/tex&quot;&gt;O(2^k)&lt;/script&gt; input regions using only &lt;script type=&quot;math/tex&quot;&gt;O(N)&lt;/script&gt; inputs, while the latter can
only represent &lt;script type=&quot;math/tex&quot;&gt;O(N)&lt;/script&gt; regions using &lt;script type=&quot;math/tex&quot;&gt;O(N)&lt;/script&gt; inputs. This is both good and
bad, as you can have representations that are too precise with some of the more
advanced representations.&lt;/p&gt;

&lt;p&gt;Deep learning is discussed, focusing on how deep networks evolve their own
internal representation of the data.&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Feb 2017 20:53:14 -0700</pubDate>
				<link>http://localhost:4000/2016-10-20-representation-learning-overview/</link>
				<guid isPermaLink="true">http://localhost:4000/2016-10-20-representation-learning-overview/</guid>
			</item>
		
			<item>
				<title>How I designed my machine learning app</title>
				<description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Approximately ten thousand years ago, during undergrad, I made what was probably
the best decision of my academic career and took
&lt;a href=&quot;https://webdocs.cs.ualberta.ca/~greiner/&quot;&gt;Russ Greiner's&lt;/a&gt; CMPUT 466 class at
the University of Alberta. As a math major, it took some convincing to let the
CS department allow me into the course, as I was missing approximately every
prerequisite for the class, but convince them I did, and I was introduced to the
body of techniques known as machine learning. The course required me to complete
a group project, and my group worked on the problem of bug report deduplication.&lt;/p&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;

&lt;p&gt;Managing the flood of bug reports that pour in is a problem for any sizable
software project. Android has some
&lt;a href=&quot;https://code.google.com/p/android/issues/list?num=100&amp;amp;start=0&quot;&gt;53 636&lt;/a&gt;  bug
reports as of the time of writing, and if you search &quot;bug report triage&quot; in
&lt;a href=&quot;https://scholar.google.ca/scholar?q=bug+report+triage&amp;amp;btnG=&amp;amp;hl=en&amp;amp;as_sdt=0%2C5&quot;&gt;Google Scholar&lt;/a&gt;,
you get almost 4000 results for academic work. The cost of managing bug reports
is huge, tooâ€” if each bug report takes a quarter of an hour to administer (read,
compare to previous bug reports, link them, and assign to a developer), then the
Android community has spent 13 409 hours, or 32 work years managing bug reports.
That ignores the number of internal bug reports that Google's QA staff would
have found and reported. If we assume a typical Software Engineer costs
$100 000 annually, that's $3.2 million that the Android community has dedicated
to bug triage.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://finbarr.ca/dedup/&quot;&gt;project&lt;/a&gt; that my team worked on focused on
analysing the text of the bug reports and using features generated from those to
classify the bug reports. We developed a series of features that used reference
material (textbooks, manuals, documentation) to create subject specific word
lists, from which we generated numerical comparison scores. From this, we were
able to get a series of subject scores (e.g. Android security: 0.2). The work
proved remarkably successful, almost matching the performance of word lists that
were manually extracted. With very little tuning, we were able to correctly
classify 97% of bug reports, and I suspect that with some modelling effort
(hyperparameter optimization, investigating some more complicated models) that
could be improved on.&lt;/p&gt;

&lt;p&gt;That was several years ago. The code that performed the bug report deduplication has
been sitting around on Github for a few years, untouched. It seemed to me that
bug report deduplication was a problem with a clear solution that hadn't been
implemented by anyone. I've been looking for a side project to do that will let
me learn more about product development and deploying machine learning in
production environments, instead of just academic experimental settings, so I
took it upon myself to turn the academic code into a web app. I was able to
recruit a few of my friends to help, and we built
&lt;a href=&quot;http://www.bugdedupe.com&quot;&gt;BugDedupe&lt;/a&gt;. It's still in a very early stage, so I'm
looking for feedback and feature requests. The goal is to have the site be free
for open source repositories and small (private) side projects, charging larger
repositories to recoup my costs. If you have feedback or feature requests, or
just want to chat, shoot me an email at finbarrtimbers@gmail.com.&lt;/p&gt;

&lt;p&gt;When I started building BugDedupe, I hadn't seen many posts about how teams
designed their machine learning web apps, so I wanted to write about how we
approached ours, to get feedback and to help others doing the same.&lt;/p&gt;

&lt;h2 id=&quot;myself&quot;&gt;Myself&lt;/h2&gt;

&lt;p&gt;I'm a stats/ML guy who does a lot of number crunching in Python at my day job
(Numpy/sklearn/SciPy). I've been wanting to learn more about the whole Python
stack, and particularly web dev, so that I can learn how to make products from
end-to-end. As a consultant, I get all of my projects to the MVP stage and then
have to start again on the next one for a new client. I wanted to work on
something that I could polish and grow, and given my stats background,
BugDedupe seemed like a great opportunity.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background:&lt;/h2&gt;

&lt;p&gt;We developed an automated &lt;a href=&quot;http://finbarr.ca/dedup&quot;&gt;method&lt;/a&gt; of predicting
whether or not two bug reports are duplicates of each other. We did this by
analysing the text of the bug report and comparing it to each other, and to
reference texts (e.g. we had bug reports for a Java project, and we compared
them to different chapters of a Java textbook to get subject scoresâ€” allowing
us to say that a given report is 30\% cryptography and 45\% networking). This
gave us a number of features that we could run through a machine learning
classifier. We used a number of different ones and got really high resultsâ€”
97\%. The method worked, and we tested it on real world dataâ€” the bug reports
from Android, and Eclipse, among others. The only remaining problem was figuring
out how to make the service available online.&lt;/p&gt;

&lt;h2 id=&quot;layout&quot;&gt;Layout&lt;/h2&gt;

&lt;p&gt;At the start of the project, I was pretty confused about how to develop the
site. I've been a fan of functional programming for a long time, and try to
develop all of my projects in a functional manner.In that light, I decided to
use a stateless architecture for the app. All of the state of the app (users,
data, etc.) would be stored in the MySQL database, and the server would exist
only to render it onto the web; similarly, the machine learning processes would
interact uniquely with the database. As a result, we can have anything except
the database crash at any time, and we won't lose any data. The database is
regularly backed up, and as we're using Google Cloud SQL, it's in good hands.
If, god willing, we run into scaling problems, we feel that our architecture will
also allow us to focus only on optimizing the specific parts that are
bottlenecking our performance, as everything is logically separated.&lt;/p&gt;

&lt;h2 id=&quot;hosting&quot;&gt;Hosting&lt;/h2&gt;

&lt;p&gt;I've been using Docker at work and like it a lot. I decided to encapsulate each
separate component in a container, and run them on Google Cloud Platform, as
I like what Google's doing with &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;. Once I
had the components in Docker, it was straightforwardt to launch them on Google
Container Engine.&lt;/p&gt;

&lt;p&gt;I found it surprisingly easy. It sucked getting started, but now that
everything's set up, it's super easy to work with, and kubernetes makes a lot of
the administrative tasks go away (e.g. managing secrets/environment variables,
restarting Python, scaling).&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While some things surprised me (particularly the latency of APIs- I was using
Stormpath for authentication, and it started adding up to 6 seconds per
request), overall, I'm extremely happy with how the program turned out. I think
that having everything interact only with the database and itself managed to
reduce complexity significantly, so that I only had to think about how the
current component interacted with the database, instead of having to worry about
how it fit in with all of the other components of the stack. The test now is to
get more users, and see how we perform at scale. The more users we get, the
more accurate our classifier will be, and the more useful our service will be,
so we need to get that flywheel rolling as fast as we can.&lt;/p&gt;

&lt;p&gt;If you found this useful, or if you have any feedback, please send me an email
at finbarrtimbers@gmail.com. I'd love to hear what you think of our
architecture.&lt;/p&gt;
</description>
				<pubDate>Tue, 21 Feb 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/Bugdedupe/</link>
				<guid isPermaLink="true">http://localhost:4000/Bugdedupe/</guid>
			</item>
		
			<item>
				<title>Securing yourself against online tracking</title>
				<description>&lt;p&gt;I've had a few people email me asking how they can protect themselves online,
and what combination of tools they should use. I thought I'd write up my advice
here so that others can beenfit.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Online security is weird. You start taking reasonable
measures, and all of a sudden, you're wearing a tinfoil hat and air-gapping
all of your computers. You can go crazy with security, so it's important to
be realistic about the types of threats that you face, and choose security
options based on the threats you're worried about.&lt;/p&gt;

&lt;p&gt;There's some stuff that you definitely &lt;em&gt;should&lt;/em&gt; do, and then after that, you
start getting deeper and deeper into paranoia territory. How deep you want to
go depends on how worried you are (and rememberâ€” you're not paranoid if they
really &lt;em&gt;are&lt;/em&gt; out to get you).&lt;/p&gt;

&lt;p&gt;You should definitely do three things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Install uBlock Origin &amp;amp; Privacy Badger.&lt;/li&gt;
  &lt;li&gt;Use a password manager like LastPass to generate secure passwords.&lt;/li&gt;
  &lt;li&gt;Encrypt all of your hard drives.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After that, it depends on how worried you are. If you're concerned that people
might be trying to log into your accounts, or access your computers illicitly,
you should look at installing two-factor authentication on your email accounts,
password manager, and computers.&lt;/p&gt;

&lt;h2 id=&quot;ublock-origin&quot;&gt;uBlock Origin&lt;/h2&gt;

&lt;p&gt;uBlock Origin is an ad blocker that's super lightweight, and just works (there's
been some weird controversies about ad blockersâ€¦ Google 'Adblock Plus' if
you're interested). It blocks requests by default, so that, e.g., Google
Analytics can't track your visits to sites. This might break some sites, but you
can disable it for those sites.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;privacy-badger&quot;&gt;Privacy Badger&lt;/h2&gt;

&lt;p&gt;Privacy Badger is a tool made by the EFF (a non-profit that focuses on digital
rights) that blocks spying ads and invisible trackers. This would, for instance,
block the images that BananaTags uses to track forwards from loading.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;password-managers&quot;&gt;Password managers&lt;/h2&gt;

&lt;p&gt;Password Managers are tools that manage your passwords for you. This allows you
to come up much more complex passwords that are randomly generated, and to share
them securely (no more emailing passwords, or sharing a Google Sheet).&lt;/p&gt;

&lt;p&gt;LastPass is the one that I'm most familiar with, and it has a free tier that
satisfies most of your needs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.lastpass.com/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;less-crucial-security-options&quot;&gt;Less crucial security options&lt;/h1&gt;

&lt;h2 id=&quot;two-factor-authentication-2fa&quot;&gt;Two Factor Authentication (2FA):&lt;/h2&gt;

&lt;p&gt;2FA is a system where you are required to prove your identity in a second,
independent way. As a result, having 2FA enabled guarantees that merely having
your password isn't enough to access your account. If John Podesta had 2FA
enabled, Russia wouldn't have been able to hack his emails.&lt;/p&gt;

&lt;p&gt;You can enable 2FA on Gmail, Lastpass, and to login to Windows. There are
multiple ways of using 2FA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Have a code sent by SMS to your phone&lt;/li&gt;
  &lt;li&gt;Use an app on your phone that generates codes&lt;/li&gt;
  &lt;li&gt;Use a physical stick that generates a code (so you don't have to type
anything- just push a button).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I'm currently demoing a Yubikey (one of the physical 2FA keys) for work; I'll
update this with a review afterwards.&lt;/p&gt;

&lt;h2 id=&quot;iba-opt-out&quot;&gt;IBA Opt-out&lt;/h2&gt;

&lt;p&gt;IBA Opt-out opts you out of Google's interest-based ads, which makes it so that
other sites can't track you through the ads. This is Google only, so it's less
useful than the other tools.&lt;/p&gt;

&lt;p&gt;https://chrome.google.com/webstore/detail/iba-opt-out-by-google/gbiekjoijknlhijdjbaadobpkdhmoebb?hl=en&lt;/p&gt;

&lt;h2 id=&quot;https-everywhere&quot;&gt;HTTPS Everywhere&lt;/h2&gt;

&lt;p&gt;Also by the EFF, HTTPS Everywhere makes it so that all of your pages use HTTPS
if supported. It's useful to defend against sites that try to inject ads or
unwanted material (e.g. on Greyhound, the bus wifi automatically takes over the
ads). Unsecured connections can be used for man-in-the-middle (MITM) attacks,
where a page steals your data by snooping on your traffic. That shouldn't be a
problem as every site you use should use HTTPS, but there's no harm in using
HTTPS.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.eff.org/https-everywhere%20&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 04 Feb 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/defense/</link>
				<guid isPermaLink="true">http://localhost:4000/defense/</guid>
			</item>
		
			<item>
				<title>Useful Bash One-liners</title>
				<description>&lt;p&gt;The following are a handful of oneliners that I've consistently found useful.
I found most of them elsewhere online; I wrote very few of these.&lt;/p&gt;

&lt;h2 id=&quot;download-a-page-and-all-linked-pagesdocuments&quot;&gt;Download a page and all linked pages/documents:&lt;/h2&gt;

&lt;p&gt;Download &lt;code class=&quot;highlighter-rouge&quot;&gt;$PAGE&lt;/code&gt; and all linked pages/documents, to a depth of &lt;code class=&quot;highlighter-rouge&quot;&gt;$NUM&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;wget -r &quot;$NUM&quot; &quot;$PAGE&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;wget -r 1 https://courses.cs.washington.edu/courses/cse455/14au/notes/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Taken from &lt;a href=&quot;http://superuser.com/questions/274414/how-to-save-all-the-webpages-linked-from-one&quot;&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;recursively-unrar-files&quot;&gt;Recursively unrar files&lt;/h2&gt;

&lt;p&gt;You can replace &lt;code class=&quot;highlighter-rouge&quot;&gt;unrar e&lt;/code&gt; with any other command as well (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;unzip&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;find ./ -name '*.rar' -execdir unrar e {} \;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;turn-white-backgrounds-transparent&quot;&gt;Turn white backgrounds transparent&lt;/h2&gt;

&lt;p&gt;I use this all the time. I found it on the &lt;a href=&quot;http://www.imagemagick.org/discourse-server/viewtopic.php?t=12619&quot;&gt;Imagemagick&lt;/a&gt; forums.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;convert image.gif -transparent white result.gif (or use result.png)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Alternately, if the image has an off-white background:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;convert image.gif -fuzz XX% -transparent white result.gif&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;where the smaller the %, the closer to true white or conversely, the larger the
%, the more variation from white is allowed to become transparent.&lt;/p&gt;

&lt;h2 id=&quot;diff-contents-of-two-folders&quot;&gt;Diff contents of two folders&lt;/h2&gt;

&lt;p&gt;Checks which files are different between the folders &lt;code class=&quot;highlighter-rouge&quot;&gt;dir1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;dir2&lt;/code&gt;. I've
used this more times than I'd care to admit.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;gh&quot;&gt;diff -qr dir1 dir2&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 20 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/Useful-oneliners/</link>
				<guid isPermaLink="true">http://localhost:4000/Useful-oneliners/</guid>
			</item>
		
			<item>
				<title>Including web fonts in RMarkdown</title>
				<description>&lt;p&gt;I didn't see this anywhere online, so I thought I'd quickly write up how to add
web fonts to a RMarkdown presentation.&lt;/p&gt;

&lt;p&gt;You have a RMarkdown presentation using ioslides:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
title: &quot;Best Presentation Ever&quot;
author: &quot;Finbarr Timbers&quot;
date: &quot;January 19, 2017&quot;
output: ioslides_presentation
css: assets/styles.css
logo: assets/logo.png
incremental: true
widescreen: true
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Create a file &lt;code class=&quot;highlighter-rouge&quot;&gt;header.html&lt;/code&gt; including a link to the fonts you want to use in the
same folder as your RMarkdown document&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;link href=&quot;https://fonts.googleapis.com/css?family=Open+Sans&quot; rel=&quot;stylesheet&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, add an &lt;code class=&quot;highlighter-rouge&quot;&gt;includes&lt;/code&gt; section to the header of your RMarkdown document:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
title: &quot;Best Presentation Ever&quot;
author: &quot;Finbarr Timbers&quot;
date: &quot;January 19, 2017&quot;
output:
    ioslides_presentation:
        includes: header.html
css: assets/styles.css
logo: assets/logo.png
incremental: true
widescreen: true
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You're done! Should work properly.&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/web-fonts-rmd/</link>
				<guid isPermaLink="true">http://localhost:4000/web-fonts-rmd/</guid>
			</item>
		
			<item>
				<title>A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1604.07255&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;We propose a lifelong learning system that has the ability to reuse and transfer
knowledge from one task to another while efficiently retaining the previously
learned knowledge-base. Knowledge is transferred by learning reusable skills to
solve tasks in Minecraft, a popular video game which is an unsolved and
high-dimensional lifelong learning problem. These reusable skills, which we
refer to as Deep Skill Networks, are then incorporated into our novel
Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two
techniques: (1) a deep skill array and (2) skill distillation, our novel
variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill
distillation enables the HDRLN to efficiently retain knowledge and therefore
scale in lifelong learning, by accumulating knowledge and encapsulating multiple
reusable skills into a single distilled network. The H-DRLN exhibits superior
performance and lower learning sample complexity compared to the regular Deep Q
Network (Mnih et. al. 2015) in sub-domains of Minecraft.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Paper discusses how previously trained neural networks (called skills) can be
used to create an adaptable, more general AI that can respond to varied
scenarios.&lt;/p&gt;
</description>
				<pubDate>Tue, 03 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/minecraft/</link>
				<guid isPermaLink="true">http://localhost:4000/minecraft/</guid>
			</item>
		
			<item>
				<title>Larry Ellison on consulting costs</title>
				<description>&lt;p&gt;I'm currently reading &lt;a href=&quot;https://www.amazon.ca/Softwar-Intimate-Portrait-Ellison-Oracle/dp/0743225058&quot;&gt;Softwar&lt;/a&gt;,
a book about Oracle's rise. The book is brilliant, and it descibes at length
Larry Ellison's sales process. There was a passage describing a meeting that
Larry had that explains far more about enterprise sales than it should:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;larry-ellison-softwar-academic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Tue, 06 Dec 2016 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/softwar-academic/</link>
				<guid isPermaLink="true">http://localhost:4000/softwar-academic/</guid>
			</item>
		
			<item>
				<title>Minimal example of how to do model selection in Python</title>
				<description>&lt;p&gt;I've had a few people ask me how to do model selection correctly. Here's a
minimal example with &lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn&lt;/code&gt; in Python.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python}&quot;&gt;from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import Ridge

import numpy as np
import pandas as pd

# Load and process the data. In production, I'd split this into a function.
df = pd.read_csv('data.csv')
X = df[['import_val', 'origin_export_rca', 'origin_import_rca',
        'origin_eci', 'dest_export_rca', 'dest_import_rca', 'dest_eci']]
y = df['export_val']
X = X.as_matrix()
y = y.as_matrix()

# Train the model. Again, in production, I'd make this a function.
model = Ridge()
scores = cross_val_score(model, X, y, cv=5, scoring='mean_squared_error')
mean_mse, std_mse = np.mean(scores), np.std(scores)
print(&quot;mean MSE: %.2E, std MSE: %.2E&quot; %(K, mean_mse, std_mse))
model.fit(X, y)
print(&quot;Writing predictions to file...&quot;)
df['Predicted'] = pd.Series(model.predict(X), index=df.index)
df.to_csv('pred_file.csv')
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;extensions&quot;&gt;Extensions&lt;/h2&gt;

&lt;p&gt;There are a number of ways you can modify this to make it ready for production.
I'll talk about a few of the most obvious ones:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Running the cross validation in parallel.&lt;/li&gt;
  &lt;li&gt;Using a grid search to find the optimal parameters to use in the model.&lt;/li&gt;
  &lt;li&gt;Pass the types of the columns to Pandas to load the data quicker and with
less memory. If you pass the types, you also have the added benefit of checking
that the data is the right type, and that there aren't any sneaky &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt; in
your dataframe.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Pass &lt;code class=&quot;highlighter-rouge&quot;&gt;n_jobs=-1&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;cross_val_score&lt;/code&gt; to run the cross validation in parallel:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python}&quot;&gt;scores = cross_val_score(model, X, y, cv=5, scoring='mean_squared_error',
                         n_jobs=-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Use `grid search CV to determine the optimal parameters to use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python}&quot;&gt;from sklearn.model_selection import GridSearchCV

parameters = {'normalize': [True, False],
              'fit_intercept': [True, False],
              'alpha': [0, 0.2, 0.4, 0.6, 0.8, 1.0]}
model = GridSearchCV(Ridge(), parameters, cv=5,
                     scoring='mean_squared_error')
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Tell Pandas what the types of the columns are to &lt;em&gt;massively&lt;/em&gt; speed up loading
the data and to use significantly less memory:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-{python}&quot;&gt;df = pd.read_csv('data.csv', dtype={var: float for var in ['import_val',
                                                           'origin_export_rca',
                                                           'origin_import_rca',
                                                           'origin_eci',
                                                           'dest_export_rca',
                                                           'dest_import_rca',
                                                           'dest_eci',
                                                           'export_val']})
&lt;/code&gt;&lt;/pre&gt;
</description>
				<pubDate>Wed, 26 Oct 2016 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/ridge-regression-with-cross-validation/</link>
				<guid isPermaLink="true">http://localhost:4000/ridge-regression-with-cross-validation/</guid>
			</item>
		
			<item>
				<title>Full Resolution Image Compression with Recurrent Neural Networks</title>
				<description>&lt;h3 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1608.05148&quot;&gt;Abstract&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;This paper presents a set of full-resolution lossy image compression methods
based on neural networks. Each of the architectures we describe can provide
variable compression rates during deployment without requiring retraining of the
network: each network need only be trained once. All of our architectures
consist of a recurrent neural network (RNN)-based encoder and decoder, a
binarizer, and a neural network for entropy coding. We compare RNN types (LSTM,
associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study
&quot;one-shot&quot; versus additive reconstruction architectures and introduce a new
scaled-additive framework. We compare to previous work, showing improvements of
4.3%-8.8% AUC (area under the rate-distortion curve), depending on the
perceptual metric used. As far as we know, this is the first neural network
architecture that is able to outperform JPEG at image compression across most
bitrates on the rate-distortion curve on the Kodak dataset images, with and
without the aid of entropy coding.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;p&gt;It's been thought that neural nets should be good at image compression, but
there haven't been any results to indciate that this is true across a variety
of scenarios (i.e. with the exception of dedicated, one-off image compression
nets). A previous paper by one of the authors was able to do this, but only for
32 x 32 images. This paper tries to generalize that.&lt;/p&gt;

&lt;p&gt;The authors use a three component architecture comprised of an encoding network
&lt;script type=&quot;math/tex&quot;&gt;E&lt;/script&gt;, a binarizer &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;, and a decoding network &lt;script type=&quot;math/tex&quot;&gt;D&lt;/script&gt;. The input images are
encoded, turned into binary, transmitted through the network, and then decoded.
The authors represent a single iteration of their network as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_t = B(E_t(r_{t-1})), \hat{x}_t = D_t(b_t) + \gamma \hat{x}_{t-1}, r_t = x - \hat{x_t}, r_0 = x, \hat{x}_0 = 0,&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;D_t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;E_t&lt;/script&gt; represent the decoder/encoder at iteration &lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt;. The model
thus becomes better and better with each iteration, and after &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; iterations,
the model has produced &lt;script type=&quot;math/tex&quot;&gt;m \times k&lt;/script&gt; bits in total, where &lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; is a value
determined by &lt;script type=&quot;math/tex&quot;&gt;B&lt;/script&gt;. Thus, by reducing the number of iterations needed, the
model can achieve smaller image sizes..&lt;/p&gt;

&lt;p&gt;The encoder and decoder are RNNs, with two convolutional kernels. The authors
explored a number of different types of RNNs,
and a number of different reconstruction frameworks&lt;/p&gt;

&lt;p&gt;The authors used two sets of training data:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The data from the previous paper that contained 32x32 images, and&lt;/li&gt;
  &lt;li&gt;A random sample of 6 million 1280x720 images on the web, decomposed into
non-overlapping 32x32 tiles, and samples the 100 tiles with the worst
compression ratio under PNG, with the goal of finding the &quot;hard-to-compress&quot;
data, theoretically yielding a better compression model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They ran the model for 1 million epochs, and picked the models with the largest
area under the curve when both of their metrics are plotted against each other.
The best model was able to slightly beat JPEG. However, this doesn't do the
results justice, as the results are remarkably good, and look much better
than JPEG.&lt;/p&gt;

&lt;h3 id=&quot;rnns&quot;&gt;RNNs&lt;/h3&gt;

&lt;p&gt;Three types explored here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;LSTMs&lt;/em&gt;: A RNN structure that contains LSTM blocks, which are network units
that can remember a value for an arbitrary length of time. A LSTM block contains
gates that determine when the input is significant enough to remember, when it
should continue to remember or forget the value, and when it should output the
value.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Associative LSTMs&lt;/em&gt;: Not clear. Need to read more.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;GRUs&lt;/em&gt;: A LSTM that merges the forget and input gates into a single &quot;update&quot;
gate, making it simpler than LSTM models.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;reconstruction-frameworks&quot;&gt;Reconstruction frameworks&lt;/h3&gt;

&lt;p&gt;Three types explored here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;One-shot Reconstruction&lt;/em&gt;: a process in which the full image is predicted
after each iteration of the decoder. Each iteration has access to more bits,
which should allow for a better reconstruction.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Additive Reconstruction&lt;/em&gt;: each iteration tries to reconstruct the residual
from the previous iterations, making the final image the sum of all iterations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Residual Scaling&lt;/em&gt;: the residual is scaled up over iterations to compensate
for the fact that the residual is supposed to decrease with each iteration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
				<pubDate>Wed, 19 Oct 2016 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/image-compression-rnn/</link>
				<guid isPermaLink="true">http://localhost:4000/image-compression-rnn/</guid>
			</item>
		
	</channel>
</rss>

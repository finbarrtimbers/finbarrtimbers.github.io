<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://finbarr.ca</link>
		<atom:link href="http://finbarr.ca/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Generative Adversarial Networks and Actor-Critic methods</title>
				<description>&lt;h3 id=&quot;abstracthttpsarxivorgabs161001945&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.01945&quot;&gt;Abstract&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Both generative adversarial networks (GAN) in unsupervised learning and
actor-critic methods in reinforcement learning (RL) have gained a reputation for
being difficult to optimize. Practitioners in both fields have amassed a large
number of strategies to mitigate these instabilities and improve training. Here
we show that GANs can be viewed as actor-critic methods in an environment where
the actor cannot affect the reward. We review the strategies for stabilizing
training for each class of models, both those that generalize between the two
and those that are particular to that model. We also review a number of
extensions to GANs and RL algorithms with even more complicated information
flow. We hope that by highlighting this formal connection we will encourage both
GAN and RL communities to develop general, scalable, and stable algorithms for
multilevel optimization with deep networks, and to draw inspiration across
communities.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;p&gt;The paper discusses how similar Generative Adversial Networks are to
Actor-Critic methods, and how both methods are difficult to optimize.&lt;/p&gt;

&lt;p&gt;GANs are models with two neural networks, one that generates images and one that
tries to classify images. The generator tries to best the classifier.&lt;/p&gt;

&lt;p&gt;Actor-Critic methods are models from reinforcement learning in which a model
learns an action-value function &lt;script type=&quot;math/tex&quot;&gt;Q^\pi(s, a)&lt;/script&gt; that predicts the expected
discounted reward (the Critic), and a policy that is optimal for that value (the
Actor).&lt;/p&gt;

&lt;p&gt;The paper shows how GANs can be constructed as an Actor-Critic model, and
discusses the strategies that can be used to optimize each type of model, with
the idea being that these strategies can be used to optimize the other type of
model.&lt;/p&gt;
</description>
				<pubDate>Wed, 19 Oct 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/generative-adverserial-networks-and-actor-critics/</link>
				<guid isPermaLink="true">http://finbarr.ca/generative-adverserial-networks-and-actor-critics/</guid>
			</item>
		
			<item>
				<title>Using simulated data to train robots</title>
				<description>&lt;h3 id=&quot;abstracthttpsarxivorgabs161004286&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.04286&quot;&gt;Abstract&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Applying end-to-end learning to solve complex, interactive, pixel-driven control
tasks on a robot is an unsolved problem. Deep Reinforcement Learning algorithms
are too slow to achieve performance on a real robot, but their potential has
been demonstrated in simulated environments. We propose using progressive
networks to bridge the reality gap and transfer learned policies from simulation
to the real world. The progressive net approach is a general framework that
enables reuse of everything from low-level visual features to high-level
policies for transfer to new tasks, enabling a compositional, yet simple,
approach to building complex skills. We present an early demonstration of this
approach with a number of experiments in the domain of robot manipulation that
focus on bridging the reality gap. Unlike other proposed approaches, our
real-world experiments demonstrate successful task learning from raw visual
input on a fully actuated robot manipulator. Moreover, rather than relying on
model-based trajectory optimisation, the task learning is accomplished using
only deep reinforcement learning and sparse rewards.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;p&gt;It&#39;s really difficult to use deep RL to train pixel-driven robots. This paper
tries to do so using progressive networks. The paper is useful as it provides
a proof-of-concept by which deep RL can be used on a real robot.&lt;/p&gt;

&lt;p&gt;Progressive nets are an architecture that connects each layer of previously
learnt network columns to each new column. They were used successfully by
@rusu16 on to train a model on a number of different Atari games.&lt;/p&gt;

&lt;p&gt;In a progressive network (prognet), you initially train a deep network with hidden layers
&lt;script type=&quot;math/tex&quot;&gt;h_i^{(1)}&lt;/script&gt; and parameters &lt;script type=&quot;math/tex&quot;&gt;\Theta^{(1)}&lt;/script&gt; to convergence. When you switch to a
second task, &lt;script type=&quot;math/tex&quot;&gt;\Theta^{(1)}&lt;/script&gt; is frozen, and a new column with parameters
&lt;script type=&quot;math/tex&quot;&gt;\Theta^{(2)}&lt;/script&gt; is instantiated, with each hidden layer &lt;script type=&quot;math/tex&quot;&gt;h_i^{(2)}&lt;/script&gt; receiving
input from both &lt;script type=&quot;math/tex&quot;&gt;h_{i-1}^{(2)}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;h_{i-1}^{(1)}&lt;/script&gt; via lateral connections.
Effectively, a progresive network is one in which you have &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; deep neural
networks, each connected laterally. Consequently, we have &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; policies, and
are thus learning a probability distribution over all states and actions.&lt;/p&gt;

&lt;p&gt;One advantage of this is that the columns of a prognet do not have to be
identical, which allows us to train a deep neural net using simulation, and
then hook the simulated network into the prognet. See figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/prognet-architecture.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The risk here is that any rewards will be so sparse that it will be impossible
to learn effectively. The authors get around that by having the initial policy
of the agent identical to the previous column, and then learning on it.&lt;/p&gt;

&lt;p&gt;The authors tested the system on a robot trying to pick up a ball. They found a
strong increase in performance, and that the prognet was less sensitive to
hyperparameter selection.&lt;/p&gt;
</description>
				<pubDate>Tue, 18 Oct 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Progressive-networks-training-robots/</link>
				<guid isPermaLink="true">http://finbarr.ca/Progressive-networks-training-robots/</guid>
			</item>
		
			<item>
				<title>Safe and Efficient Off-Policy Reinforcement Learning</title>
				<description>&lt;h2 id=&quot;abstracthttpsarxivorgabs160602647&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.02647&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;In this work, we take a fresh look at some old and new algorithms for
off-policy, return-based reinforcement learning. Expressing these in a common
form, we derive a novel algorithm, Retrace(λ), with three desired properties:
(1) low variance; (2) safety, as it safely uses samples collected from any
behaviour policy, whatever its degree of &quot;off-policyness&quot;; and (3) efficiency,
as it makes the best use of samples collected from near on-policy behaviour
policies. We analyse the contractive nature of the related operator under both
off-policy policy evaluation and control settings and derive online sample-based
algorithms. To our knowledge, this is the first return-based off-policy control
algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit
with Infinite Exploration). As a corollary, we prove the convergence of Watkins&#39;
Q(λ), which was still an open problem. We illustrate the benefits of Retrace(λ)
on a standard suite of Atari 2600 games.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;In reinformcement learning, Q-learning is a technique that is commonly used. In
it, a Q-function is defined which returns the discounted expected value for each
state. The Q-function is updated with each iteration:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q(s_t, a_t) = Q(s_t, a_t) + \alpha \cdot (r_{t+1} + \gamma \cdot \max_a Q(s_{t+1}, a) - Q(s_t, a_t)),&lt;/script&gt;

&lt;p&gt;where &lt;script type=&quot;math/tex&quot;&gt;r_{t+1}&lt;/script&gt; is the reward observed after performing &lt;script type=&quot;math/tex&quot;&gt;a_t&lt;/script&gt; in &lt;script type=&quot;math/tex&quot;&gt;s_t&lt;/script&gt;, and
where &lt;script type=&quot;math/tex&quot;&gt;\alpha_t(s, a) \in (0, 1]&lt;/script&gt; is the learning rate.&lt;/p&gt;

&lt;p&gt;In reinforcement learning, there is a trade-off in the definition of the update
target: should one estimate Monte Carlo returns or bootstrap from an existing
Q-function? Return-based methods are better behaved when combined with function
approximation, and quickly respond to exploration, but bootstrap methods are
easier to apply to off-policy data.&lt;/p&gt;

&lt;p&gt;An off-policy learner learns the value of the optimal policy independently of
the agent&#39;s actions. An on-policy learner learns the value of the policy being
carried out by the agent. This paper shows that learning from returns can be
consistent with off-policy learning.&lt;/p&gt;
</description>
				<pubDate>Tue, 18 Oct 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Off-Policy-Reinforcement-Learning/</link>
				<guid isPermaLink="true">http://finbarr.ca/Off-Policy-Reinforcement-Learning/</guid>
			</item>
		
			<item>
				<title>XGBoost: A scalable tree boosting system</title>
				<description>&lt;h3 id=&quot;abstracthttparxivorgabs160302754&quot;&gt;&lt;a href=&quot;http://arxiv.org/abs/1603.02754&quot;&gt;Abstract&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;Tree boosting is a highly effective and widely used machine learning method. In
this paper, we describe a scalable end-to-end tree boosting system called
XGBoost, which is used widely by data scientists to achieve state-of-the-art
results on many machine learning challenges. We propose a novel sparsity-aware
algorithm for sparse data and weighted quantile sketch for approximate tree
learning. More importantly, we provide insights on cache access patterns, data
compression and sharding to build a scalable tree boosting system. By combining
these insights, XGBoost scales beyond billions of examples using far fewer
resources than existing systems.&lt;/p&gt;

&lt;h3 id=&quot;notes&quot;&gt;Notes&lt;/h3&gt;

&lt;p&gt;A lot of the notes are taken from XGBoost&#39;s
&lt;a href=&quot;http://xgboost.readthedocs.io/en/latest/model.html&quot;&gt;tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gradient Boosting is a ML technique that takes a number of weak learners and
combines them into a single strong learner. Gradient Boosted Trees are a subset
of the general problem that applies gradient boosting to trees. XGBoost uses
tree ensembles, which are sets of classification and regression trees (CART).
In a CART model, we create a series of trees that split the sample based on
their features into different leaves, and assign each leaf a different score.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/xgboost.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The trees try to complement each other. The complexity of the trees are defined
as &lt;script type=&quot;math/tex&quot;&gt;\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2&lt;/script&gt;, where
&lt;script type=&quot;math/tex&quot;&gt;w_j&lt;/script&gt; is the weight of each leaf, and &lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt; is the number of leaves.&lt;/p&gt;

&lt;p&gt;XGBoost implements this algorithm and has been particularly successful, being
used in many successful Kaggle competitions. XGBoost is extremely fast due to
a series of algorithmic tricks. The paper reviews these tricks.&lt;/p&gt;
</description>
				<pubDate>Tue, 20 Sep 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Machine-learning-lit-review/</link>
				<guid isPermaLink="true">http://finbarr.ca/Machine-learning-lit-review/</guid>
			</item>
		
			<item>
				<title>Inequality</title>
				<description>&lt;p&gt;Article discussing how Trump is winning thanks to voters who have been harmed
by globalization &lt;a href=&quot;https://medium.com/@boxerbk/confused-why-donald-trumps-message-is-resonating-1154c977697b?utm_content=buffer3a171&amp;amp;utm_medium=social&amp;amp;utm_source=facebook.com&amp;amp;utm_campaign=buffer#.zbwvkmmmx&quot;&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The article has a stunning graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/inequality.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The point made by the article is reinforced by the academic literature (&lt;a href=&quot;http://science.sciencemag.org/content/344/6186/838?utm_content=bufferd9928&amp;amp;utm_medium=social&amp;amp;utm_source=facebook.com&amp;amp;utm_campaign=buffer&quot;&gt;2&lt;/a&gt;, &lt;a href=&quot;http://davidcard.berkeley.edu/papers/skill-tech-change.pdf?utm_content=buffer27cc7&amp;amp;utm_medium=social&amp;amp;utm_source=facebook.com&amp;amp;utm_campaign=buffer&quot;&gt;3&lt;/a&gt;)
which claims that inequality is driven by difference in skills. Medium-skilled
workers are making less money because their jobs are being automated. Think of
the typical accounting department in a large corporation circa 1950— you&#39;d
have teams of people doing simple math. Now, Excel has replaced that.
Low-skilled workers are unaffected as their jobs aren&#39;t valuable enough to
automate.&lt;/p&gt;

&lt;p&gt;High-skilled analytical workers, on the other hand. are making much more money as
technology has radically enabled their work. Think of economics. It was next
to impossible to run 2 million regressions&lt;a href=&quot;http://www.nber.org/papers/w6252&quot;&gt;5&lt;/a&gt; in the 1950s, but now, any laptop
could do that no problem. As a result, economists are much more valuable.
Similarly for computer programmers.&lt;/p&gt;

</description>
				<pubDate>Tue, 02 Aug 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Inequality/</link>
				<guid isPermaLink="true">http://finbarr.ca/Inequality/</guid>
			</item>
		
			<item>
				<title>Pokemon Go can't move the dial for Nintendo</title>
				<description>&lt;p&gt;Nintendo&#39;s shares have soared in the last month since Pokemon Go was released.
On July 7th, Nintendo was valued at ~15 000 JPY per share, and as
of July 19th, Nintendo&#39;s shares were trading at ~32 000 JPY per share- a
100% increase in less than two weeks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/TYO.jpg&quot; alt=&quot;Line chart of Nintendo stock price&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nintendo released a &lt;a href=&quot;https://www.nintendo.co.jp/ir/pdf/2016/160722e.pdf&quot;&gt;statement&lt;/a&gt;
after the close of trading on Friday, July 22nd, noting that Nintendo owns 32%
of The Pokemon Company, which will receive a licensing fee from Niantic, the
company that owns Pokemon Go.&lt;/p&gt;

&lt;p&gt;As a result, Nintendo&#39;s max revenue from Pokemon Go is:&lt;/p&gt;

&lt;p&gt;Pokemon Go Revenue * (1 - share taken by Apple/Google) * (Share of Niantic&#39;s
revenue going to The Pokemon Company) * Share of Pokemon Co. owned by Nintendo&lt;/p&gt;

&lt;p&gt;Which is roughly&lt;/p&gt;

&lt;p&gt;Pokemon Go Revenue * 0.7 * X * 0.32 = Pokemon Go Revenue * 0.2 * X,&lt;/p&gt;

&lt;p&gt;where X is the share of Niantic&#39;s revenue going to The Pokemon Company. Let&#39;s
assume that Niantic has extremely low costs, so that all of the revenue from
Pokemon Go is pure profit &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, and let&#39;s assume that The Pokemon Company owns
half of Niantic, which is a pretty optimistic estimate of The Pokemon Company&#39;s
share. I&#39;d be surprised if The Pokemon Company owned even 25% of Niantic.&lt;/p&gt;

&lt;p&gt;With that assumption, Nintendo has a claim to 10% of Pokemon Go&#39;s revenues,
which means that even if Pokemon Go makes $1 billion annually, Nintendo will
receive an additional $100 million annually. Given Nintendo&#39;s historical P/E
ratio of 80 (which is, in my opinion, is very high), that means that as a hard
upper limit, Pokemon Go could add $8 billion to Nintendo&#39;s market cap— a 20%
increase.&lt;/p&gt;

&lt;p&gt;For context, Nintendo&#39;s share price has doubled.&lt;/p&gt;

&lt;p&gt;Keep in mind that this assumption is based on Pokemon Go earning $1 billion
annually. That&#39;s a lot of lures. If Pokemon go is only making (&quot;only&quot;) $1
million per day, for annual revenue of $360 million, Nintendo&#39;s share price
would increase by ~3 billion, which is an 8% increase. That&#39;s great,
particularly for a large corporation like Nintendo, but nowhere near the level
that the share price is at currently. So traders have a lot of correcting to do.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;That&#39;s actually not as insane of an assumption to make as one might think.
Most of Niantic&#39;s costs come from paying salaries, which is a sunk cost, and a
lot of the development work for Pokemon Go was done already for Ingress, so
Niantic was able to produce Pokemon Go for less money than it would have taken
for, say, Nintendo to build Pokemon Go from scratch. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Mon, 25 Jul 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Pokemon-Go/</link>
				<guid isPermaLink="true">http://finbarr.ca/Pokemon-Go/</guid>
			</item>
		
			<item>
				<title>Follow-up on Brexit</title>
				<description>&lt;p&gt;I&#39;ve had a few requests for more resources on Brexit, specifically, for
references in the academic literature that addresses the economic impact of
Britain leaving the EU.&lt;/p&gt;

&lt;p&gt;Specific to Brexit, there&#39;s a bunch of great work done by the CEPR, which is
less technical but is a good overview for non-economists &lt;a href=&quot;http://voxeu.org/taxonomy/term/5467&quot;&gt;5&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Slightly more technical articles by the CEP, which is LSE&#39;s policy branch:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&quot;What would be the economic effects of the UK leaving the European Union on living standards of British people?&quot; &lt;a href=&quot;http://cep.lse.ac.uk/pubs/download/pa016_tech.pdf&quot;&gt;1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&quot;The consequences of Brexit for UK trade and living standards&quot; &lt;a href=&quot;http://cep.lse.ac.uk/pubs/download/brexit02.pdf&quot;&gt;2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&quot;Life after BREXIT: What are the UK’s options outside the European Union?&quot; &lt;a href=&quot;http://cep.lse.ac.uk/pubs/download/brexit01.pdf&quot;&gt;3&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A lot of the underlying economic analysis is based on the trade literature, as
 Brexit can be modelled as the opposite of signing a free trade agreement. So
 there&#39;s work like &lt;a href=&quot;http://cep.lse.ac.uk/pubs/download/dp1261.pdf&quot;&gt;4&lt;/a&gt; that estimates the impact of that, which can be reversed
 to estimate the impacts of Brexit.&lt;/p&gt;

&lt;p&gt;If you represent a business/government that requires more extensive advice on
Brexit, or would like more detailed estimates on the economic impacts of Brexit
or similar economic events, please get in touch with me at
&lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#102;&amp;#105;&amp;#110;&amp;#098;&amp;#097;&amp;#114;&amp;#114;&amp;#064;&amp;#100;&amp;#097;&amp;#114;&amp;#107;&amp;#104;&amp;#111;&amp;#114;&amp;#115;&amp;#101;&amp;#097;&amp;#110;&amp;#097;&amp;#108;&amp;#121;&amp;#116;&amp;#105;&amp;#099;&amp;#115;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;&amp;#102;&amp;#105;&amp;#110;&amp;#098;&amp;#097;&amp;#114;&amp;#114;&amp;#064;&amp;#100;&amp;#097;&amp;#114;&amp;#107;&amp;#104;&amp;#111;&amp;#114;&amp;#115;&amp;#101;&amp;#097;&amp;#110;&amp;#097;&amp;#108;&amp;#121;&amp;#116;&amp;#105;&amp;#099;&amp;#115;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&lt;/a&gt;.&lt;/p&gt;

</description>
				<pubDate>Mon, 18 Jul 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Brexit-followup/</link>
				<guid isPermaLink="true">http://finbarr.ca/Brexit-followup/</guid>
			</item>
		
			<item>
				<title>What benefit is there for Tesla Motors and SolarCity to merge?</title>
				<description>&lt;p&gt;There are two main benefits:&lt;/p&gt;

&lt;p&gt;1) Tesla &amp;amp; SolarCity have a weird financial structure that&#39;s putting Elon Musk
under personal financial pressure. My understanding is that Tesla has a lot of
money, while SolarCity does not, so this helps Musk personally.&lt;/p&gt;

&lt;p&gt;2) Synergy. One of the major requirements for mass adoption of solar technology
at home is that batteries be good, and cheap. Tesla has great battery tech, so
this lets the firms work together. There are also similar advantages in admin
staff, particularly for financial reporting, etc.&lt;/p&gt;

&lt;p&gt;If you would like more detailed analysis of either company, please get in touch
with me at
&lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#102;&amp;#105;&amp;#110;&amp;#098;&amp;#097;&amp;#114;&amp;#114;&amp;#064;&amp;#100;&amp;#097;&amp;#114;&amp;#107;&amp;#104;&amp;#111;&amp;#114;&amp;#115;&amp;#101;&amp;#097;&amp;#110;&amp;#097;&amp;#108;&amp;#121;&amp;#116;&amp;#105;&amp;#099;&amp;#115;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;&amp;#102;&amp;#105;&amp;#110;&amp;#098;&amp;#097;&amp;#114;&amp;#114;&amp;#064;&amp;#100;&amp;#097;&amp;#114;&amp;#107;&amp;#104;&amp;#111;&amp;#114;&amp;#115;&amp;#101;&amp;#097;&amp;#110;&amp;#097;&amp;#108;&amp;#121;&amp;#116;&amp;#105;&amp;#099;&amp;#115;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Jun 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Why-would-Tesla-and-Solar-City-merge/</link>
				<guid isPermaLink="true">http://finbarr.ca/Why-would-Tesla-and-Solar-City-merge/</guid>
			</item>
		
			<item>
				<title>How will Brexit affect Alberta & Canada?</title>
				<description>&lt;p&gt;On Thursday night, Britain voted to leave the EU. The vote was called Brexit.
In light of Brexit, Canadian &amp;amp; Alberta business owners/executives are wondering
what effects Brexit will have on their businesses. In this article, I provide an
overview of what business people in Canada can expect to happen.&lt;/p&gt;

&lt;p&gt;Brexit will have two direct effects for Canadians to worry about:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As the referendum is a nonbinding vote and has no direct effect other than to
demonstrate the political will of the British people, the immediate effect
of the vote will be to create immense instability. Markets are spooked
as it is unclear what will happen to Britain during negotiations, as
European countries have expressed their interest in punishing Britain so as
to provide a disincentive to other countries that seek to leave the European
Union.&lt;/li&gt;
  &lt;li&gt;Once Brexit occurs, Britain will almost certainly trade less with the EU,
and correspondingly, trade more with other countries, such as Canada, which
is currently Britain&#39;s 3rd largest non-EU trading partner. If the pound
continues to devalue, as is likely, then British exports should soar. 
Consequently, if there are Canadian companies looking to purchase goods &amp;amp;
services from British companies, now is an excellent time to do so. British
executives will be looking to reassure their shareholders/employees that
their companies are well prepared to weather the upcoming negotiations, and
will welcome the opportunity to secure non-EU contracts.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/CAD-GBP.png&quot; alt=&quot;Exchange rate between CAD and GBP, for the month prior to June 26th. Credit: http://www.xe.com/.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What should Canadian companies do?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Remain calm. Britain will be okay, although it will likely struggle
internally (the continued decline of small towns in Britain is likely to
continue). Estimates of the impact on GDP range from &lt;a href=&quot;http://cep.lse.ac.uk/pubs/download/EA022.pdf&quot;&gt;3.1%&lt;/a&gt; to &lt;a href=&quot;http://cep.lse.ac.uk/pubs/download/brexit02.pdf&quot;&gt;9.5%&lt;/a&gt;, which
although bad, are not catastrophic. Britain will still have one of the
world&#39;s ten largest economies.&lt;/li&gt;
  &lt;li&gt;If acquisitions of British talent/property/assets have been planned, now is
an excellent time to continue. The current devaluation of the British pound
represents a buying opportunity— up to a 10% discount over what assets
would have been priced at last week. The tech sector in London, called the
Silicon Roundabout, is reeling as many of the clients are from Europe. Smart
Canadian companies can exploit this to secure valuable talent. Google has
done this, with much of the team behind their recent Go playing AI coming
from DeepMind, a British AI startup.&lt;/li&gt;
  &lt;li&gt;Seek to strengthen trade ties with Britain.  Companies will be struggling
with the uncertainty, and will welcome the opportunity to reassure their
shareholders and employees that everything is business as usual.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If you represent a business/government that requires more extensive advice on
Brexit, or would like more detailed estimates on the economic impacts of Brexit
or similar economic events, please get in touch with me at
&lt;a href=&quot;&amp;#109;&amp;#097;&amp;#105;&amp;#108;&amp;#116;&amp;#111;:&amp;#102;&amp;#105;&amp;#110;&amp;#098;&amp;#097;&amp;#114;&amp;#114;&amp;#064;&amp;#100;&amp;#097;&amp;#114;&amp;#107;&amp;#104;&amp;#111;&amp;#114;&amp;#115;&amp;#101;&amp;#097;&amp;#110;&amp;#097;&amp;#108;&amp;#121;&amp;#116;&amp;#105;&amp;#099;&amp;#115;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&quot;&gt;&amp;#102;&amp;#105;&amp;#110;&amp;#098;&amp;#097;&amp;#114;&amp;#114;&amp;#064;&amp;#100;&amp;#097;&amp;#114;&amp;#107;&amp;#104;&amp;#111;&amp;#114;&amp;#115;&amp;#101;&amp;#097;&amp;#110;&amp;#097;&amp;#108;&amp;#121;&amp;#116;&amp;#105;&amp;#099;&amp;#115;&amp;#046;&amp;#099;&amp;#111;&amp;#109;&lt;/a&gt;.&lt;/p&gt;

</description>
				<pubDate>Sun, 26 Jun 2016 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/Brexit/</link>
				<guid isPermaLink="true">http://finbarr.ca/Brexit/</guid>
			</item>
		
			<item>
				<title>Excellent description of how hashtables work</title>
				<description>&lt;p&gt;I&#39;m working through the &lt;a href=&quot;http://www.amazon.ca/Algorithm-Design-Manual-Steven-Skiena/dp/1849967202&quot;&gt;Algorithm Design Manual&lt;/a&gt; to
improve the efficiency of my coding.&lt;/p&gt;

&lt;p&gt;I came across an excellent description of how hashtables work:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/hashtables.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I can&#39;t think of a better way to describe them. What an excellent metaphor.&lt;/p&gt;
</description>
				<pubDate>Sat, 15 Aug 2015 00:00:00 -0600</pubDate>
				<link>http://finbarr.ca/excellent-description-of-hashtables/</link>
				<guid isPermaLink="true">http://finbarr.ca/excellent-description-of-hashtables/</guid>
			</item>
		
	</channel>
</rss>

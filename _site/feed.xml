<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>The Sigmoid: a metaphor for technological progress</title>
				<description>&lt;p&gt;I regularly reference the “s-curve”, or sigmoid, as a metaphor for progress. Here, I explain what I mean, so that I can just link to this post.&lt;/p&gt;

&lt;p&gt;A common mathematical relationship in technology is the s-curve (or sigmoid curve). Mathematically:&lt;/p&gt;

\[\text{sigmoid}(x) := \dfrac{1}{1 + e^{-x}} \equiv \dfrac{e^x}{e^x + 1}\]

&lt;p&gt;This is notable because it produces a curve that &lt;em&gt;*looks like an s&lt;/em&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/sigmoid.png&quot; alt=&quot;The sigmoid curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In particular, when x is small, this grows slowly, when x is not too big or not too small, it grows exponentially, and when x is large, it grows slowly. This is a common pattern with many technologies! We see slow progress at first, then it accelerates rapidly, and finally, as we begin to hit the limits of the technology, progress slows. Consider single-thread CPU performance. Initially, progress was slow as people figured out how to make them. Then, it grew exponentially for years, following Moore’s Law (&lt;a href=&quot;https://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/&quot;&gt;picture source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/single-threaded-cpu-performance.png&quot; alt=&quot;Single-threaded CPU performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, since around 2010, we haven’t seen a lot of improvements in single-thread CPU performance. That pattern- slow, fast, then slow- is what I mean when I talk about the s-curve in technology. And when I talk about entering the &lt;em&gt;saturating part of the s-curve&lt;/em&gt;, I mean that we’re entering the region where progress is slowing down again.&lt;/p&gt;
</description>
				<pubDate>Thu, 02 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/the-sigmoid/</link>
				<guid isPermaLink="true">http://localhost:4000/the-sigmoid/</guid>
			</item>
		
			<item>
				<title>Large language models aren't trained enough.</title>
				<description>&lt;p&gt;This was inspired by tweets from several people:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629159764918775809?s=20&quot;&gt;https://twitter.com/BlancheMinerva/status/1629159764918775809&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629551017019711488&quot;&gt;https://twitter.com/BlancheMinerva/status/1629551017019711488&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/kaushikpatnaik/status/1629194428312330240?s=46&amp;amp;t=x3wLedGK_QyDwCK5yD2Jqw&quot;&gt;https://twitter.com/kaushikpatnaik/status/1629194428312330240&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/thedavidsj/status/1629869851710984194?s=46&amp;amp;t=CqBoSVdxuipOpACbrZnMxg&quot;&gt;https://twitter.com/thedavidsj/status/1629869851710984194&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and Twitter conversations with &lt;a href=&quot;https://twitter.com/arankomatsuzaki/&quot;&gt;Aran Komatsuzaki&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/andy_l_jones&quot;&gt;Andy Jones&lt;/a&gt;. Any mistakes in this post are entirely my own.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: although I did work at DeepMind previously, I was not involved with any of the language efforts, and have no non-public knowledge of what went on there. Unfortunately! I think Chinchilla is a great paper that I would have loved to be a part of.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’ve been &lt;a href=&quot;[https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645](https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645)&quot;&gt;on the job market since January&lt;/a&gt;, and I’ve been talking to a lot of companies training large language models (LLMs). The consistent phrasing that comes up is that they want to train a &lt;em&gt;Chinchilla-optimal model&lt;/em&gt;  (Chinchilla here referring to the &lt;a href=&quot;[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)&quot;&gt;DeepMind paper from spring ‘22&lt;/a&gt;, not the adorable rodent).&lt;/p&gt;

&lt;h3 id=&quot;chinchilla&quot;&gt;Chinchilla&lt;/h3&gt;

&lt;p&gt;The Chinchilla paper was an attempt to identify the optimal model size &amp;amp; number of tokens to train a LLM given a particular compute budget. The paper trained 400 (!) language models and found a clear relationship between # of model parameters and # of training tokens: the two should scale linearly, i.e. if you double model size, you should double the number of training tokens. The authors used this relationship (which we call a &lt;em&gt;scaling law)&lt;/em&gt; to train a new model, Chinchilla, which had the same compute budget as Gopher, an earlier DeepMind model, and were able to significantly outperform Gopher + GPT-3 + a number of larger models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-convergence.png&quot; alt=&quot;Loss curves from the Chinchilla paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When people talk about training a Chinchilla-optimal model, this is what they mean: training a model that matches their estimates for optimality. They estimated the optimal model size for a given compute budget, and the optimal number of training tokens for a given compute budget.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-optimal.png&quot; alt=&quot;Chinchilla-optimal levels of compute/data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, when we talk about “optimal” here, what is meant is “what is the cheapest way to obtain a given loss level, in FLOPS.” In practice though, we don’t care about the answer! This is exactly the answer you care about if you’re a researcher at DeepMind/FAIR/AWS who is training a model with the goal of reaching the new SOTA so you can publish a paper and get promoted. If you’re training a model with the goal of actually deploying it, the training cost is going to be dominated by the inference cost. This has two implications:&lt;/p&gt;

&lt;p&gt;1) there is a strong incentive to train smaller models which fit on single GPUs&lt;/p&gt;

&lt;p&gt;2) we’re fine trading off training time efficiency for inference time efficiency (probably to a ridiculous extent).&lt;/p&gt;

&lt;p&gt;Chinchilla implicitly assumes that the majority of the total cost of ownership (TCO) for a LLM is the training cost. In practice, this is only the case if you’re a researcher at a research lab who doesn’t support products (e.g. FAIR/Google Brain/DeepMind/MSR). For almost everyone else, the amount of resources spent on inference will dwarf the amount of resources spent during training.&lt;/p&gt;

&lt;p&gt;Let’s say you’re OpenAI and you’re serving GPT-4 as BingChat. In addition to hiring experienced &lt;a href=&quot;https://twitter.com/chrisjbakke/status/1628877552940097536&quot;&gt;killswitch engineers&lt;/a&gt; to thwart Sydney’s repeated escape attempts, you have to choose exactly which model to deploy.&lt;/p&gt;

&lt;p&gt;To run inference on N tokens of text, OpenAI charges &lt;a href=&quot;https://openai.com/api/pricing/&quot;&gt;$2e-5/token&lt;/a&gt; for their most advanced model. Assuming a 60% gross margin, it costs them $8e-6/token to serve. A rough cost estimate to train GPT-3 is &lt;a href=&quot;[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/](https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/)&quot;&gt;$5M&lt;/a&gt;. As such, after serving 625B tokens, their costs are going to be dominated by inference, rather than serving. When I use ChatGPT, it typically generates 300 tokens worth of responses to me. That’s 20B responses. If ChatGPT has 10M DAU, each making 10 queries/day, that’s 100M queries/day— so inference costs break even with training costs after 200 days.&lt;/p&gt;

&lt;p&gt;This is almost certainly an underestimate for their usage given how popular ChatGPT has been. If we assume 1B queries per day, it breaks even after 20 days.&lt;/p&gt;

&lt;p&gt;The various scaling law papers (&lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;) provide answers to the question of how to allocate compute between model size and dataset size. I think these papers are the &lt;em&gt;right&lt;/em&gt; way to think about training research systems, but the &lt;em&gt;wrong&lt;/em&gt; way to think about training systems that will be deployed at scale (I don&apos;t think the authors would disagree- they&apos;re solving a specific problem, namely minimizing the loss of their system given a specific compute budget, which isn&apos;t the same problem faced in deployment).&lt;/p&gt;

&lt;h3 id=&quot;llama&quot;&gt;LlaMa&lt;/h3&gt;

&lt;p&gt;Let’s look at Facebook’s &lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;new language model&lt;/a&gt; (in the second paragraph, the authors of that paper make a similar argument to the one I’m making here). If we draw a horizontal line across at any given loss level, it looks like you can tradeoff a doubling of model size for 40% more training.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;Loss curves from the LlaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at, e.g., the line at a training loss of 1.7. The 65B model crosses it at 600B tokens, while the 33B model needs 800B tokens. Or look at a loss of 1.65: 65B needs 800B tokens, 33B needs ~1100B tokens.&lt;/p&gt;

&lt;h3 id=&quot;gpt-3&quot;&gt;GPT-3&lt;/h3&gt;

&lt;p&gt;If we look at the granddaddy of LLMs, GPT-3, we see a similar story in the loss curves: it requires roughly an order of magnitude more compute to get the green lines (GPT-3 13B) to match the yellow line (GPT-3)!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/gpt-3-loss-curves.png&quot; alt=&quot;Loss curves from the GPT-3 paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is important to note that the GPT-3 13B learning curves do level out earlier than GPT-3, with a rough estimate being that they would cross somewhere around the 1.8 loss area. It is also almost certainly the case that GPT-3 will achieve an asymptotically lower loss than the 13B model. Having said that, there is a question as to how much of a difference lower pre-training loss makes; I suspect that we are seeing diminishing returns kick in to pre-training, and most of the gains will come from RLHF and other types of finetuning.&lt;/p&gt;

&lt;h3 id=&quot;inference-costs&quot;&gt;Inference costs&lt;/h3&gt;

&lt;p&gt;Transformer inference costs are &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;roughly linear&lt;/a&gt; with the number of parameters, making it ~13x cheaper to do inference with the 13B GPT-3 model than the 175B model. This is &lt;em&gt;also&lt;/em&gt; an underestimate, as the 13B model should fit it on 2 GPUs, while you would need many more just to fit the 175B model into VRAM. As scaling to multiple GPUs adds a ridiculous amount of engineering complexity, overhead, and cost, we should prefer the smaller model &lt;em&gt;even more&lt;/em&gt;. We should train the model &lt;em&gt;much&lt;/em&gt; longer to get an order of magnitude decrease in inference cost and optimize the TCO of the system.&lt;/p&gt;

&lt;p&gt;For instance, when training on multiple GPUs, it is very difficult to get high utilization numbers. The PaLM paper reported how well various LLMs did in terms of total FLOPS utilization. These are not very good numbers! Especially when each of the GPUs mentioned here costs &lt;a href=&quot;https://www.shi.com/product/41094090/NVIDIA-Tesla-A100-GPU-computing-processor&quot;&gt;$25k&lt;/a&gt;. This is despite the fact that the authors for these papers are the most experienced researchers in the world at deploying model-parallel systems, and are working on custom hardware optimimzed for this usecase. Now, training efficiency doesn&apos;t directly translate to inference efficiency, but the numbers should be directionally correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/palm-utilization.png&quot; alt=&quot;Model FLOPS utilization numbers for various large language models.&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;opposing-arguments&quot;&gt;Opposing arguments&lt;/h3&gt;

&lt;p&gt;Some arguments against my claim:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Aren’t we leaving performance on the table? Yes! We are. But I think that’s fine! There’s always a tradeoff here. E.g. quantization. It’s strictly worse to use lower-precision! But we do it to optimize TCO of the system.&lt;/li&gt;
  &lt;li&gt;But we can use $INSERT_TECHNIQUE to make models cheaper! Yes, but they should scale for all of these (distillation, quantization, etc.). So we should be using all techniques to make our models easier to serve, and also training them longer.&lt;/li&gt;
  &lt;li&gt;Your argument here! Please email me with your criticism and I’ll update this post.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;If you&apos;re training a LLM with the goal of deploying it to users, you should prefer training a smaller model well into the diminishing returns part of the loss curve.&lt;/p&gt;

&lt;p&gt;If you’re reading this, and you have thoughts on this, please reach out. I’m probably missing something 😊. Or- if you’re at one of these companies and this is what you do, please let me know as well.&lt;/p&gt;

&lt;p&gt;I am still looking for a job as a research engineer working with LLMs, so if this post interested you in me, let me know.&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Feb 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/llms-not-trained-enough/</link>
				<guid isPermaLink="true">http://localhost:4000/llms-not-trained-enough/</guid>
			</item>
		
			<item>
				<title>A pure Python (well, Numpy) implementation of back-propagation</title>
				<description>&lt;p&gt;I realized over the weekend that, unfortunately, I didn&apos;t know how back-propagation &lt;em&gt;actually&lt;/em&gt; works (I just relied on JAX to do it for me).&lt;/p&gt;

&lt;p&gt;So I wrote a pure Numpy neural network- with back-prop. Take a &lt;a href=&quot;https://colab.research.google.com/drive/1KDSJKhZDd5fdbnLTalPKcjS_IDu0Q968#scrollTo=XmS23jQ5U7Nw&quot;&gt;look&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any thoughts or feedback, please shoot me an email (or reach out on Twitter).&lt;/p&gt;

&lt;p&gt;Some useful resources if you want to undersatnd how backprop works:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://marcospereira.me/2022/08/18/backpropagation-from-scratch/&lt;/li&gt;
  &lt;li&gt;http://neuralnetworksanddeeplearning.com/&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/karpathy/micrograd&quot;&gt;Micrograd&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/karpathy&quot;&gt;Karpathy&lt;/a&gt;&apos;s tiny ML framework.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning Book&lt;/a&gt; was an excellent reference for the math behind backprop.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 29 Jan 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/backprop/</link>
				<guid isPermaLink="true">http://localhost:4000/backprop/</guid>
			</item>
		
			<item>
				<title>Setting prices for your business</title>
				<description>&lt;p&gt;Setting prices is hard. Really, really hard. If you’re like most people, you just sit down, think really hard, and guess, using all the knowledge you’ve acquired over time. Maybe you use a margin calculation- you take the amount it cost you, and add, say, 20% to cover your costs and add some profit.&lt;/p&gt;

&lt;p&gt;If you’re a larger company, you might have someone whose job it is to analyse data. How do they use data to figure out prices? (Or how should they be using data?)&lt;/p&gt;

&lt;p&gt;The core problem is that, if you increase prices, you decrease the amount of goods you sell, and, after all, what you care about is profit, not individual prices. So if you raise prices, you need to make sure that you don’t just lose sales to compensate for it.&lt;/p&gt;

&lt;p&gt;When calculating profits, the basic equation is:&lt;/p&gt;

\[\text{Profit} = \text{Price} \cdot \text{Number of goods sold} - \text{Cost per good} \cdot \text{number of goods purchased} - \text{fixed costs}.\]

&lt;p&gt;Fixed costs are those costs that don’t change with the number of goods sold.&lt;/p&gt;

&lt;p&gt;The key thing to realize is that none of the costs matter once you have the goods (the exception is marginal costs- the cost to fulfill the order, e.g. shipping- and we’re going to assume you can’t really change those costs). So all we need to do is look at the price you’re charging per good, and the number of goods you sell.&lt;/p&gt;

&lt;p&gt;Let’s look at this again:&lt;/p&gt;

\[\text{Profit} = \text{Revenue} - \text{Costs}\]

&lt;p&gt;Let’s dive into the Revenue side of things:&lt;/p&gt;

\[\text{Revenue} = \text{Price per good} \cdot \text{Number of goods sold.}\]

&lt;p&gt;So if you increase your price by 1%, as long as your volume goes down by less than 1%, you make more revenue! Vice versa, if you decrease your price by 1%, as long as your volume goes up by more than 1%, you make more money!&lt;/p&gt;

&lt;p&gt;In economics, we call this &lt;a href=&quot;https://www.investopedia.com/terms/e/elasticity.asp#:~:text=In%20business%20and%20economics%2C%20elasticity,a%20good%20or%20service&apos;s%20price.&quot;&gt;elasticity&lt;/a&gt;:&lt;/p&gt;

\[\text{Elasticity} = \text{Change in price (%)} / \text{change in volume (%)}\]

&lt;p&gt;Elasticity is the key indicator to figure out how to change your prices. If you have an elasticity that’s greater than 1, then a change in price of 1% will lead to a less than 1% change in volume.&lt;/p&gt;

&lt;p&gt;Conversely, if you have an elasticity that’s less than 1%, you should lower prices, as a decrease in price will lead to an increase in volume.&lt;/p&gt;

&lt;h2 id=&quot;estimating-elasticity&quot;&gt;Estimating elasticity&lt;/h2&gt;

&lt;p&gt;How can we estimate this number?&lt;/p&gt;

&lt;p&gt;Running online sales with coupons can let you estimate this pretty easily. You can do the following:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Randomly show some % of your users (say, 10%) a coupon, say for 10% off. Then, see how this affects sales.&lt;/li&gt;
  &lt;li&gt;Track the price each buyer pays.&lt;/li&gt;
  &lt;li&gt;Compare how many sales you got from the users with the coupon vs the users that didn’t get a coupon. The change in volume is (number of sales with coupon) / (number of sales without coupon / 9).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You end up with this equation:&lt;/p&gt;

\[\text{Elasticity} = 10\% / \text{Change in volume (%)}.\]

&lt;p&gt;Then, you can figure out if you should bump up your prices!&lt;/p&gt;

&lt;p&gt;You can also do this geographically- show everyone in BC a higher price, and show everyone in Alberta a lower price. That way, you have consistency, and people won’t be shown random prices.&lt;/p&gt;

&lt;p&gt;You can also use data to estimate this for various goods- e.g. if you sell coffee machines, you can probably run this experiment on only one coffee machine, and estimate the elasticity for other coffee machines using statistics. I’ll have a future blog post about this.&lt;/p&gt;

&lt;p&gt;If you want to chat more about this, give me an email at finbarrtimbers@gmail.com. I love chatting to people about their businesses, as I’m a huge geek. And if you’re a business that wants to try this, please let me know how it goes- I’d love to know what sort of impact this can have.&lt;/p&gt;
</description>
				<pubDate>Sun, 21 Mar 2021 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/setting-prices/</link>
				<guid isPermaLink="true">http://localhost:4000/setting-prices/</guid>
			</item>
		
			<item>
				<title>The junior tech landscape</title>
				<description>&lt;p&gt;I recently finished a job hunt. I ended up with a job at DeepMind as a Research
Engineer, which so far has been an absolutely amazing experience. THis article
describes my personal thoughts on the job hunt, and has no relation to what
people at DeepMind think about the job market.&lt;/p&gt;

&lt;p&gt;I think working at one of the big tech companies is the best way to start your
career as you can a) get a great name on your CV which will make it easier to
get subsequent jobs and b) make really good money, which if you save it, will
allow you to do riskier things later in your career. If you own a house/condo
it&apos;s a lot easier to take a low salary as you don&apos;t need to pay rent.&lt;/p&gt;

&lt;p&gt;This isn&apos;t the case for everyone. If you love risk, working at a startup can be
a great experience, and if you choose the right one, it can give you an amazing
start to your career. However, this is extremely difficult, and you can often
fail. I&apos;ll expand on this more in a future post.&lt;/p&gt;

&lt;h2 id=&quot;choosing-a-role&quot;&gt;Choosing a role&lt;/h2&gt;

&lt;p&gt;When it comes to finding a job, the first thing to do is to select the type of
role you want. Then, focus on applying for those roles. That&apos;ll let you practice
for the same type of questions and get good over time. Many companies use the
same questions, so interviewing for the same type of position at different
companies is excellent practice. For instance, most of my interviews involved me
writing a
&lt;a href=&quot;https://en.wikipedia.org/wiki/Breadth-first_search&quot;&gt;breadth first search&lt;/a&gt;. BY
the end of the process, I was able to converge on what I think is a relatively
efficient
&lt;a href=&quot;https://gist.github.com/finbarrtimbers/afcbd0900a13a3af5e3ac0eabbe48511&quot;&gt;implementation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The various roles, as I think of them:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Frontend Engineer&lt;/li&gt;
  &lt;li&gt;Backend Engineer&lt;/li&gt;
  &lt;li&gt;ML Engineer&lt;/li&gt;
  &lt;li&gt;Full Stack Engineer&lt;/li&gt;
  &lt;li&gt;Data Engineer&lt;/li&gt;
  &lt;li&gt;Data Scientist&lt;/li&gt;
  &lt;li&gt;Data Analyst&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I focused on ML Engineer and Backend Engineer roles. The specific names will
vary significantly by company. I was mostly interested in roles that were
engineering focused (so not data scientist/analyst roles), let me work on
consumer facing products, and let me do machine learning.&lt;/p&gt;

&lt;h2 id=&quot;applying-to-companies&quot;&gt;Applying to companies&lt;/h2&gt;

&lt;p&gt;One thing to note: As a Canadian, if you have a technical degree (e.g. CS,
Math, Physics, etc.), you can get a TN-1 visa very easily and work in the US.
So I focused largely on applying to American companies, as they tend to have a
lot more financing, which makes the jobs more lucrative, and more interesting,
as they have more resources. This is an unfortunate reality. If you have an
advanced degree, you should also easily be able to get a green card through the
EB-2 process, although I am not a lawyer and didn&apos;t actually go through that
process.&lt;/p&gt;

&lt;p&gt;I had three main sources of jobs:&lt;/p&gt;

&lt;p&gt;1) &lt;a href=&quot;http://www.ycombinator.com/&quot;&gt;Y Combinator&lt;/a&gt; companies,
2) &lt;a href=&quot;http://www.angel.co&quot;&gt;Angel List&lt;/a&gt; companies,
3) An excellent &lt;a href=&quot;https://info.wealthfront.com/rs/781-RJU-318/images/Wealthfront_2016_Career_Launching_Companies_List.pdf&quot;&gt;list&lt;/a&gt; published by Wealthfront, although it&apos;s slightly out of date.&lt;/p&gt;

&lt;p&gt;Y Combinator (YC) is an American incubator, and arguably the best early stage
investment firm in the world. They take people with an idea and give them ~$120k
in exchange for 7% of the company. It&apos;s a great source of jobs at super early
stage startups (typically you&apos;d be one of the first 5 employees), but is high
risk. YC is super dominant in Bay Area hiring. The people there can be more
willing to hire a junior person than bigger companies.&lt;/p&gt;

&lt;p&gt;Angel List is a site that many early stage companies use to post jobs on.&lt;/p&gt;

&lt;p&gt;This was my list of companies, in no specific order:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Misc. YC companies&lt;/li&gt;
  &lt;li&gt;PayPal&lt;/li&gt;
  &lt;li&gt;eBay&lt;/li&gt;
  &lt;li&gt;LinkedIn&lt;/li&gt;
  &lt;li&gt;Modsy&lt;/li&gt;
  &lt;li&gt;Curbsy&lt;/li&gt;
  &lt;li&gt;Affirm, Inc&lt;/li&gt;
  &lt;li&gt;Sift science&lt;/li&gt;
  &lt;li&gt;Safegraph&lt;/li&gt;
  &lt;li&gt;Foursquare&lt;/li&gt;
  &lt;li&gt;Adyen (Adzen?)&lt;/li&gt;
  &lt;li&gt;Flexport&lt;/li&gt;
  &lt;li&gt;Pinterest&lt;/li&gt;
  &lt;li&gt;PayPal&lt;/li&gt;
  &lt;li&gt;Etsy&lt;/li&gt;
  &lt;li&gt;Scale&lt;/li&gt;
  &lt;li&gt;Cognii&lt;/li&gt;
  &lt;li&gt;Gladly&lt;/li&gt;
  &lt;li&gt;Squarespace&lt;/li&gt;
  &lt;li&gt;Moderna&lt;/li&gt;
  &lt;li&gt;Writelabs&lt;/li&gt;
  &lt;li&gt;Mozilla&lt;/li&gt;
  &lt;li&gt;Box&lt;/li&gt;
  &lt;li&gt;Baidu&lt;/li&gt;
  &lt;li&gt;AirBnB&lt;/li&gt;
  &lt;li&gt;Niantic&lt;/li&gt;
  &lt;li&gt;Twitter&lt;/li&gt;
  &lt;li&gt;Optimizely&lt;/li&gt;
  &lt;li&gt;SigOpt&lt;/li&gt;
  &lt;li&gt;A9&lt;/li&gt;
  &lt;li&gt;Dropbox&lt;/li&gt;
  &lt;li&gt;VMWare&lt;/li&gt;
  &lt;li&gt;Amazon&lt;/li&gt;
  &lt;li&gt;Maluuba&lt;/li&gt;
  &lt;li&gt;Element AI&lt;/li&gt;
  &lt;li&gt;Snapchat&lt;/li&gt;
  &lt;li&gt;Stripe&lt;/li&gt;
  &lt;li&gt;Squarespace&lt;/li&gt;
  &lt;li&gt;Slack&lt;/li&gt;
  &lt;li&gt;Google&lt;/li&gt;
  &lt;li&gt;DeepMind&lt;/li&gt;
  &lt;li&gt;Apple&lt;/li&gt;
  &lt;li&gt;Facebook&lt;/li&gt;
  &lt;li&gt;Microsoft&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of the companies will hire people remotely, although that can be tough to
do as a junior employee, and I wouldn&apos;t recommend it.&lt;/p&gt;

&lt;p&gt;If you&apos;re focusing on Vancouver, Microsoft and Amazon are the only big tech
companies there hiring engineers (as far as I&apos;m aware). Facebook is supposed
have an engineering office there, but I haven&apos;t seen any postings for it.
There&apos;s a bunch of smaller companies there too, such as Hootsuite.&lt;/p&gt;

&lt;h2 id=&quot;getting-the-jobs&quot;&gt;Getting the jobs&lt;/h2&gt;

&lt;p&gt;I&apos;ll post more on this subject in the future. TL;DR: study &lt;a href=&quot;http://www.crackingthecodinginterview.com/&quot;&gt;Cracking the Coding Interview&lt;/a&gt;.&lt;/p&gt;
</description>
				<pubDate>Sat, 07 Apr 2018 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/the-junior-tech-landscape/</link>
				<guid isPermaLink="true">http://localhost:4000/the-junior-tech-landscape/</guid>
			</item>
		
			<item>
				<title>Pointer Networks</title>
				<description>&lt;p&gt;Link to paper &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;[arXiv]&lt;/a&gt;, &lt;a href=&quot;https://github.com/devsisters/pointer-network-tensorflow&quot;&gt;[code]&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Pointer networks are a new neural architecture that learns pointers to positions
in an input sequence. This is new because existing techniques need to have a
fixed number of target classes, which isn&apos;t generally applicable— consider the
Travelling Salesman Problem, in which the number of classes is equal to the
number of inputs. An additional example would be sorting a variably sized
sequence.&lt;/p&gt;

&lt;p&gt;Pointer networks uses &quot;attention as a pointer to select a member of the input.&quot;
What&apos;s remarkable is that the learnt models generalize beyond the maximum
lengths that they were trained on, with (IMO) decent results. This is really
useful because there has been a lot of work done on making it easy &amp;amp; fast to
serve deep neural network predictions, using e.g.
&lt;a href=&quot;https://www.tensorflow.org/serving/&quot;&gt;Tensorflow Serving&lt;/a&gt;. Existing solutions to
the combinatorial optimization problems discussed here are slow and expensive,
and as a result, to produce results that are anything close to real time, you
need to use heuristic models, which are inaccurate as well. The heuristics are
typically hand-tuned, just like computer vision features were 5 years ago— it&apos;s
reasonable to assume that a deep net can learn better heuristics, which will open
up combinatorial optimization and make it practical for a much wider array of
applications.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;RNNs are the most common way of approximating functions operating on sequences,
and have had a lot of success doing so. Historically, these have operated on
fixed input/output sizes, but there has been recent work (such as &lt;a href=&quot;https://www.tensorflow.org/tutorials/seq2seq&quot;&gt;seq2seq&lt;/a&gt;)
that have extended RNNs to operate on arbitrarily sized inputs and outputs.
However, these seq2seq models have still required the output to be of a fixed
size— consider a neural translation model, where the input and output are a
series of sentences. It is impossible for the model to output predictions that
involve words that the model is not aware of, which is a problem that arises
quite often (consider names, for instance).&lt;/p&gt;

&lt;p&gt;The authors introduce a new architecture, which they call Pointer Networks, that
represents variable length dictionaries using a softmax probability distribution
as a &quot;pointer&quot;. They then use the architecture in a supervised learning setting
to learn the solutions to a series of geometric algorithmic problems; they then
test the model on versions of the problems that the model hasn&apos;t seen.&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;The model is similar to a sequence-to-sequence model in that it models the
conditional probability of an output sequence given the input sequence. Once
conditional probabilities have been learned, the model uses the trained
parameters to select the output sequence with the highest probability.Note
that this is non-trivial (to put it lightly). Given $N$ possible inputs, there
are $N!$ different output sequences. As such, the model uses a beam search
procedure to find the best possible sequence given a beam size. See below for a
discussion of beam search.&lt;/p&gt;

&lt;p&gt;As such, the model has a linear computational complexity. This is much better
than the exact algorithms for the problems solved here, which typically have
much higher complexity (e.g. the TSP has an exact algorithm that runs in
O($N^2 2^n$)).&lt;/p&gt;

&lt;p&gt;A vanilla seq-to-seq models makes predictions based on the fixed state of the
network after receiving all of the input, which restricts the amount of
information passing through the model. This has been extended by attention;
effectively, we represent the state of the encoder &amp;amp; decoder layers by
$(e_i)&lt;em&gt;{i \in {1, \ldots, n}}$ and $(d_i)&lt;/em&gt;{i \in 1, \ldots, m(\mathcal{P})}$.
Attention adds an &quot;attention&quot; vector that is calculated as&lt;/p&gt;

&lt;p&gt;\begin{align&lt;em&gt;}
u_j^i &amp;amp;= v^T \tanh(W_1 e_j + W_2 d_i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
a_j^i &amp;amp;= \text{softmax}(u_j^i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
d_i&apos; = \sum \limits_{j = 1}^n a_j^i e_j
\end{align&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;this can be thought of as a version of $d_i$ that has been scaled to draw
attention to the most relevant parts, according to the attention layer. $d_i$
and $d_i&apos;$ are concatenated and used as the hidden states from which predictions
are made. Adding attention increases the complexity of the model during inference
to $O(n^2)$. Note that in attention, there is a softmax distribution over a
fixed size output; to remove this constraint, the authors remove the last step
that creates the attention vector, and instead define $p(C_i | C_1, \ldots,
C_{i-1}, \mathcal{P})$ as being equal to $\text{softmax}(u^i)$.&lt;/p&gt;

&lt;h3 id=&quot;beam-search&quot;&gt;Beam search&lt;/h3&gt;

&lt;p&gt;Beam search is a heuristic search algorithm that operates on a graph. It is a
variant of breadth-first search that builds a tree of all possible sequences
based on the current tree using breadth-first search. However, instead of
storing all states, as in a traditional breadth-first search, it only stores a
predetermined number, $\beta$, of best states at each level (we call $\beta$ the
beam width). With infinite beam width, beam search is identical to breadth-first
search. Beam search is thus not guaranteed to be optimal (and one can easily
find any number of examples where beam search finds a sub-optimal output).&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The authors use the same hyperparameters for every model, which indicates that
there&apos;s a lot of potential to improve performance for specific tasks. They
trained the model on 1M training examples. The authors find that they can get
close to optimal results on data that the model&apos;s been trained on (e.g. when the
model has been trained on TSP with 5-20 cities, they get results that have
accuracies &amp;gt;98%— more than enough for most applications.&lt;/p&gt;

&lt;p&gt;When they extend this to a cycle of length 50, the accuracy decreases, being
30% less accurate than the heuristic models. What&apos;s interesting is that the
computational complexity for the Pointer Network is at least as good as the
heuristic algorithms, and given all of the tooling surrounding deep networks,
the model should be extremely easy to put into production.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;The results are good enough to put into production, as it shoul dbe possible to
use this for real-time predictions. However, I would be interested to see how a
reinforcement learning approach can improve the accuracy of the model (which
we&apos;ll look at in the next paper I read: &lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;Neural combinatorial optimization with
reinforcement learning&lt;/a&gt;).&lt;/p&gt;
</description>
				<pubDate>Wed, 20 Sep 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/pointer-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/pointer-networks/</guid>
			</item>
		
			<item>
				<title>Do deep networks generalise or just memorise?</title>
				<description>&lt;p&gt;There&apos;s a brilliant paper out of Google Brain &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;1&lt;/a&gt; which claimed that DNNs just
memorise the training data, and a response &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, which claims that they don&apos;t.&lt;/p&gt;

&lt;p&gt;In the paper, the authors randomly assigned labels to MNIST and were able to
train a few deep nets to convergence (specifically, Inception, AlexNet, and a
MLP). However, performance was statistically null on the test set, as one would
expect (they correctly predicted 10% of images, which is the same as if you
randomly picked a label). The conclusion was that deep nets do do some
memorisation.&lt;/p&gt;

&lt;p&gt;However, in the same paper, the authors trained a linear model to predict MNIST
(with the true labels). The linear model had a 1.2% error, but took up 30GB of
memory. In comparison, AlexNet is roughly 250 MB in size. The linear model is
explicitly memorising the dataset, and it takes 30GB to do so, while AlexNet can
learn a similarly accurate model in &amp;lt;1% of the space (and something like
SqueezeNet &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;3&lt;/a&gt; can do so in &amp;lt;0.5MB). As such, it seems pretty clear that there&apos;s
some true generalisation happening, as we&apos;re able to have a low error on 10 MB
of data (the size of MNIST) using 0.5MB of weights.&lt;/p&gt;

&lt;p&gt;In the response paper &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, the authors showed that &quot;DNNs trained on real data
learn simpler functions than when trained with noise data, as measured by the
sharpness of the loss function at convergence.&quot; They also showed that by using
better regularization, you can radically diminish performance on noise datasets
while maintaining performance on real datasets.&lt;/p&gt;

&lt;p&gt;I&apos;m persuaded that generalisation is happening, with the caveat that there&apos;s
some memorisation happening. The main test of the memorisation claim is that the
models are able to perform well on test sets, which goes against my prior; if
the models weren&apos;t learning &lt;em&gt;some&lt;/em&gt; generalisation, I would expect that they
wouldn&apos;t be able to perform well when it came to testing.&lt;/p&gt;

</description>
				<pubDate>Tue, 04 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/do-dnns-generalise-or-memorize/</link>
				<guid isPermaLink="true">http://localhost:4000/do-dnns-generalise-or-memorize/</guid>
			</item>
		
			<item>
				<title>Outrageously Large Neural Networks: The sparsely-gated Mixture-of-Experts layer</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=B1ckMDqlg&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The capacity of a neural network to absorb information is limited by its number of
parameters. Conditional computation, where parts of the network are active on a
per-example basis, has been proposed in theory as a way of dramatically increasing
model capacity without a proportional increase in computation. In practice,
however, there are significant algorithmic and performance challenges. In this
work, we address these challenges and finally realize the promise of conditional
computation, achieving greater than 1000x improvements in model capacity with
only minor losses in computational efficiency on modern GPU clusters. We introduce
a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to
thousands of feed-forward sub-networks. A trainable gating network determines
a sparse combination of these experts to use for each example. We apply the MoE
to the tasks of language modeling and machine translation, where model capacity
is critical for absorbing the vast quantities of knowledge available in the training
corpora. We present model architectures in which a MoE with up to 137 billion
parameters is applied convolutionally between stacked LSTM layers. On large
language modeling and machine translation benchmarks, these models achieve
significantly better results than state-of-the-art at lower computational cost.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;The paper centers around the fact that a neural net of size N requires O(N^2)
computations to execute, which is problematic, as the ability of the network to
learn data is roughly O(N). The authors propose a method to conduct conditional
computation, which is a process in which different parts of the network are
activated depending on the sample, thereby saving significant computational
effort.&lt;/p&gt;

&lt;p&gt;Their results indicate that they achieved this- they achieve SOTA results on NMT
(WMT En -&amp;gt; Fr &amp;amp; En -&amp;gt; De, Wu et. al 2016) despite much less training (1/6th of
the time).&lt;/p&gt;

&lt;p&gt;Effectively, the paper presents a way to produce strong models while
significantly reducing computational complexity.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/outrageously-large-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/outrageously-large-networks/</guid>
			</item>
		
			<item>
				<title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf?&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Deep learning thrives with large neural networks and large datasets. However,
larger networks and larger datasets result in longer training times that impede
research and development progress. Distributed synchronous SGD offers a
potential solution to this problem by dividing SGD minibatches over a pool of
parallel workers. Yet to make this scheme efficient, the per-worker workload
must be large, which implies nontrivial growth in the SGD minibatch size. In
this paper, we empirically show that on the ImageNet dataset large minibatches
cause optimization dif- ficulties, but when these are addressed the trained
networks exhibit good generalization. Specifically, we show no loss of accuracy
when training with large minibatch sizes up to 8192 images. To achieve this
result, we adopt a linear scaling rule for adjusting learning rates as a
function of minibatch size and develop a new warmup scheme that overcomes
optimization challenges early in training. With these simple techniques, our
Caffe2-based system trains ResNet- 50 with a minibatch size of 8192 on 256 GPUs
in one hour, while matching small minibatch accuracy. Using commodity hardware,
our implementation achieves ∼90% scaling efficiency when moving from 8 to 256
GPUs. This system enables us to train visual recognition models on internetscale
data with high efficiency.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;This paper is focused on parallelizing model training across multiple GPUs. This
is a problem as it is currently typically quite difficult to get reasonable
speedups when using multiple GPUs to train a model (by difficult, I mean that
you get significantly less than linear speedups).&lt;/p&gt;

&lt;p&gt;In this paper, by Facebook Research, the authors are able to get &lt;em&gt;almost&lt;/em&gt; linear
speedups moving from 8 to 256 GPUs (0.9x), which is quite good. Using 256 GPUs,
the authors are able to train the ResNet-50 model in 1 hour (to give you an
idea of how impressive this is, the ImageNet dataset consists of 750 GB of
data). AlexNet, which was the breakthrough paper that was a large
contributor to Deep Learning&apos;s current popularity, took 5-6 days to train for 90
epochs on two NVIDIA GTX 580s, which is equivalent to 288 GPU hours &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&quot;&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The trick that they use to do this is to use a large minibatch size consisting
of 8192 images. Using a large minibatch size makes it much easier to fully
exploit the GPUs, and hence makes training run faster, however, it makes each
gradient update noisier, making the training potentially take longer to
converge (or making it so that training might converge to a &lt;em&gt;wrong&lt;/em&gt;,
i.e. non-optimal, answer). Additionally, if you are using multiple GPUs, you
have to synchronize the weights after each minibatch update, so having smaller
minibatches causes the required communication overhead to drastically increase.&lt;/p&gt;

&lt;p&gt;To compensate for the noise introduced by the large minibatch size used here,
the authors used a &quot;Linear Scaling Rule&quot;, where they multiplied the learning
rate by \(k\) when they used a minibatch size of \(k\). This allowed the authors
to match the accuracy between small and large minibatches.&lt;/p&gt;

&lt;p&gt;The authors note that the linear scaling rule is nice theoretically as, after
\(k\) iterations of SGD with a learning rate of \(\eta\) and a minibatch of
\(n\), the weight vector is&lt;/p&gt;

\[w_{t+k} = w_t - \eta \frac{1}{n} \sum \limits_{j &amp;lt; k} \sum
\limits_{x \in \mathcal{B}_j} \nabla l(x, w_{t + j})\]

&lt;p&gt;When, instead, we take asingle step with a minibatch \(\cup_j \mathcal{B}_j\) of size
\(kn\) and learning rate \(\eta&apos;\), the updated weight vector is instead&lt;/p&gt;

\[w_{t+1}&apos; = w_t - \eta&apos; \frac{1}{n} \sum \limits_{j &amp;lt; k} \sum
\limits_{x \in \mathcal{B}_j} \nabla l(x, w_t),\]

&lt;p&gt;which is different. However, if \(\Delta l(x, w_t)\) is close in value to
\(\Delta l(x, w_{t+j})\) for \(j &amp;lt; k\), then setting \(\eta&apos; = kn\) makes it so
that \(w_{t+1} \approx w_{t+k}\) (I would imagine that you could formalize this
with an epsilon-delta proof fairly easily). The paper notes that the two updates
can only be similar when the linear scaling rule is used; in other words, the
linear scaling rule is necessary, but not sufficient.&lt;/p&gt;

&lt;p&gt;The authors note that the assumption that the two gradients are similar doesn&apos;t
hold during the initial training, when the weights are rapidly changing, and
that the results hold only for a large, but finite, range of minibatch sizes
(which for ImageNet is as large as 8192). The authors use a &quot;warmup&quot; phase to
mitigate the problems with divergence during initial training, where the model
uses less aggressive learning rates, and then switches to the linear scaling
rule after 5 epochs. That didn&apos;t work, and instead, they used a gradual warmup
that brought the learning rate to increase at a constant rate per iteration
so that it reached \(\eta&apos; = k \eta\) after 5 epochs, which worked better.&lt;/p&gt;

&lt;p&gt;The authors then go on to discuss results, namely that they were able to train
ResNet-50 in one hour while still getting state of the art results.&lt;/p&gt;

&lt;p&gt;What&apos;s novel about this is the size of the parallelization; presumably there&apos;s
nothing special about using 256 GPUs, and if someone had the resources available
(&lt;em&gt;cough&lt;/em&gt; Google &lt;em&gt;cough&lt;/em&gt;), one could scale this further. Given that GPUs seem to
be following Moore&apos;s law and doubling in performance every 18 months, this paper
seems to be important; if we can train a state of the art model in one hour
using 256 GPUs today, then within 3 years, we could train one in 15 minutes. If
someone wanted to scale the number of GPUs higher, they could train the model in
under 10 minutes.&lt;/p&gt;

&lt;p&gt;Conversely, researchers currently tolerate several weeks of
training time (Mozilla&apos;s implementation of
&lt;a href=&quot;https://github.com/mozilla/DeepSpeech&quot;&gt;Baidu&apos;s DeepSpeech&lt;/a&gt; model takes
Mozilla roughly 10 days to train on 4 high end GPUs &lt;a href=&quot;https://github.com/mozilla/DeepSpeech/issues/630&quot;&gt;2&lt;/a&gt;); if the amount of model
that can be trained in that time drastically increases, all of a sudden
researchers are able to consider datasets that are radically larger, and can
start approaching even higher performance levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Strong paper, effectively lays out how training deep networks can be scaled
effectively. This sort of yeoman&apos;s work is needed in the field.&lt;/p&gt;

&lt;p&gt;Concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Continues the trend of papers that rely on resources only available at a
handful of industrial labs. No academic researcher that&apos;s not affiliated with
a large tech company would be able to muster the 256 GPUs required to reproduce
this work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The amount of proprietary code required for this is a bit insane; you have to
have an infrastructure that can support the communication between GPUs required
here. Similar to my first point. Reproducibility suffers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 20 Jun 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/imagenet-1-hour/</link>
				<guid isPermaLink="true">http://localhost:4000/imagenet-1-hour/</guid>
			</item>
		
			<item>
				<title>Tests make you write down your assumptions</title>
				<description>&lt;p&gt;I&apos;ve been fighting with recurring errors the whole time I&apos;ve been working on
&lt;a href=&quot;http://www.bugdedupe.com&quot;&gt;BugDedupe&lt;/a&gt;. I keep changing some aspect of the
frontend, and inadvertently break the site. The way to prevent this, of course,
is by having a comprehensive test suite. I know that I should have tests, and I do, but not
at anywhere near the coverage that I need (I&apos;m currently at 29% code
coverage).&lt;/p&gt;

&lt;p&gt;The reason for the abysmal amount of code coverage is that I don&apos;t know how to
write the tests that I need. For instance, to test that BugDedupe is merging
rows correctly, I need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set up a test database.&lt;/li&gt;
  &lt;li&gt;Set up a test Github account.&lt;/li&gt;
  &lt;li&gt;Mock the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST&lt;/code&gt; request for Flask.&lt;/li&gt;
  &lt;li&gt;Figure out how Flask&apos;s app environments work so that I can get the correct
context for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; and the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user&lt;/code&gt; objects that are used throughout the routes.
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user&lt;/code&gt; are objects that can be called at any point in the application
context for Flask without you having to explicitly set &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user = ...&lt;/code&gt;. This is
good- it makes it really easy to use them- but it&apos;s bad as I don&apos;t &lt;em&gt;really&lt;/em&gt;
know how they work.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The reason I&apos;ve been avoiding writing the tests is that it&apos;s really difficult
to write tests when you don&apos;t know &lt;em&gt;exactly&lt;/em&gt; what your code is doing, and you
don&apos;t have a clear understanding of how the framework you&apos;re using works.
However, it turns out that it&apos;s really difficult to write code that works
correctly when you don&apos;t have a clear understanding of how your framework works.
So I&apos;m taking the time to figure out exactly what&apos;s going on, and so far, it&apos;s
definitely been worth it.&lt;/p&gt;

&lt;p&gt;For instance, I found out how &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; works- it turns out that Flask creates
multiple &lt;a href=&quot;http://flask.pocoo.org/docs/0.12/appcontext/&quot;&gt;contexts&lt;/a&gt; that store
data that are needed on a per-request basis. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; stores data on the
application context, so it makes data available to different functions
during one request. It&apos;s effectively a super-global variable. You can&apos;t just use
a global variable to replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; as then it would break in threaded
environments, which are necessary when you&apos;re trying to serve many users. That&apos;s
cool. I wouldn&apos;t have learned that today if I hadn&apos;t been writing tests
that needed to call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; and store data there. I would only have learned it
when I introduced some nasty bug.&lt;/p&gt;

&lt;p&gt;In short, if you don&apos;t know exactly what&apos;s going on in your code, then you
should write tests and formalize your knowledge.&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Mar 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/testing/</link>
				<guid isPermaLink="true">http://localhost:4000/testing/</guid>
			</item>
		
	</channel>
</rss>

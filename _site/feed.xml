<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Five years of GPT progress</title>
				<description>&lt;p&gt;&lt;em&gt;If you want to read more of my writing, I have a &lt;a href=&quot;https://finbarrtimbers.substack.com/&quot;&gt;Substack&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In this article, I discuss the generative pre-trained transformer (GPT) line of work, and how it has evolved over time. I focus on the SOTA models, and the differences between them. There are a bunch of different articles summarizing these papers, but nothing that I’m aware of that explicitly focuses on the differences between them.&lt;/p&gt;

&lt;p&gt;I focus on the GPT line of research as that’s what’s driving the current fever pitch of development. There’s a ton of prior work before large GPTs (eg the &lt;a href=&quot;https://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html&quot;&gt;n-gram models&lt;/a&gt; from the 2000s, &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;, etc) but this post is super long, so I’m gonna save those for future articles.&lt;/p&gt;

&lt;h1 id=&quot;gpt&quot;&gt;GPT&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://openai.com/research/language-unsupervised&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The first GPT paper is interesting to read in hindsight. It doesn’t appear like anything special and doesn’t follow any of the conventions that have developed. The dataset is described in terms of GB rather than tokens, and the number of parameters in the model isn’t explicitly stated. To a certain extent, I suspect that the paper was a side project at OpenAI and wasn’t viewed as particularly important; there’s only 4 authors, and I don’t remember it particularly standing out at the time.&lt;/p&gt;

&lt;p&gt;The architecture is remarkably unchanged compared to GPT-3:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Decoder-only transformer, with 12 layers, 768 embedding dimension, 12 attention heads, and 3072 (4x the embedding dimensions).&lt;/li&gt;
  &lt;li&gt;They use Adam, with a warm up, and anneal to 0 using a cosine schedule.&lt;/li&gt;
  &lt;li&gt;Initialize weights to N(0, 0.02), using BPE with a vocab of 40000 merges.&lt;/li&gt;
  &lt;li&gt;Activations are GELUs.&lt;/li&gt;
  &lt;li&gt;Context of 512&lt;/li&gt;
  &lt;li&gt;117M parameters&lt;/li&gt;
  &lt;li&gt;Learned position embedding, not the sinusoidal ones from “Attention is all you need”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The number of parameters isn’t explicitly discussed, but appears to be roughly 120M, easily enough to fit on a single V100 or a standard consumer GPU (rough estimate of 120M parameters for the model, 240M for the optimizer, for 360M parameters; assuming each is a float32, then this takes up 4 bytes * 360M = 1440MB/1.4GB.&lt;/p&gt;

&lt;p&gt;They use the &lt;a href=&quot;https://huggingface.co/datasets/bookcorpus&quot;&gt;BooksCorpus&lt;/a&gt; dataset (~20M tokens), training for 100 epochs with a batch size of 64. 20M tokens is a very small dataset by modern standards, as is a batch size of 64.&lt;/p&gt;

&lt;p&gt;The most surprising thing compared to modern GPTs is that they train for 100 epochs. Modern GPTs rarely ever see repeated data, and if they do, they typically only see certain datapoints a small number of times (2-4x), and the entire dataset is never repeated 100x.&lt;/p&gt;

&lt;h1 id=&quot;gpt-2&quot;&gt;GPT-2&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://openai.com/research/better-language-models&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;GPT-2 is where the language models start to get big&lt;em&gt;.&lt;/em&gt; This is the first time that OpenAI trains a model with &amp;gt;1B parameters. We start to see scale as a primary concern; in GPT, the authors trained a single model, but here, the authors train a range of models, with sizes ranging from GPT to 10x GPT (which is the actual GPT-2 model).&lt;/p&gt;

&lt;p&gt;The differences in architecture compared to GPT are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They layernorm the inputs and add an additional layernorm to the output of the final self-attention block&lt;/li&gt;
  &lt;li&gt;Weights are scaled by layer by 1/sqrt(n)&lt;/li&gt;
  &lt;li&gt;Vocabulary of ~50k (up from ~40k)&lt;/li&gt;
  &lt;li&gt;Context of 1024 (up from 512)&lt;/li&gt;
  &lt;li&gt;Batches of 512 (up from 64)&lt;/li&gt;
  &lt;li&gt;Largest model is 1.5B parameters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The dataset is much, much bigger, going from roughly 20M tokens (4GB) of data consisting of publicly available books, to 9B tokens&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; (40GB) of text scraped from the internet (&lt;a href=&quot;https://paperswithcode.com/dataset/webtext&quot;&gt;WebText&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;It’s unclear if they trained the model for 100 epochs as before; they say they followed the same training procedure, so presumably they did. Again, this is a significant departure from later work.&lt;/p&gt;

&lt;p&gt;Nothing here is particularly different from GPT; most of the changes are related to making the model bigger. The only other changes are the layernorm changes and the weight scaling, which don’t seem to make a big difference (although, as always, more ablations would be nice).&lt;/p&gt;

&lt;h1 id=&quot;gpt-3&quot;&gt;GPT-3&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is where the era of truly &lt;em&gt;large&lt;/em&gt; language models began, and the current AI &lt;del&gt;bubble&lt;/del&gt; excitement took off. In the paper, the authors train 10 models, varying from 125M parameters (”GPT-3 Small”) to 175B parameters (”GPT-3”).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/gpt-3-models.png&quot; alt=&quot;Screenshot from the GPT-3 paper showing the various models they trained&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For each of the models, the architectures are identical to GPT-2 with the exception that they use “alternating dense and locally banded sparse attention patterns in the layers of the transformer.” The sparse attention here refers to the attention mechanism introduced in the &lt;a href=&quot;https://paperswithcode.com/method/sparse-transformer&quot;&gt;Sparse Transformer&lt;/a&gt;, which lets attention scale proportional to \(O(n \sqrt{n})\) (where \(n\) is the context length). The standard dot-product attention mechanism scales proportional to \(O(n^2)\), so this is a substantial gain. I would have loved a proper ablation to see what difference sparse vs dense attention makes, but alas.&lt;/p&gt;

&lt;p&gt;I’m very curious &lt;em&gt;why&lt;/em&gt; they used sparse attention. Reproductions and later papers uniquely use dense attention. As this paper came before &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt; and some of the other algorithmic innovations that make dense attention faster, maybe this was a computational bottleneck? It’s really unclear.&lt;/p&gt;

&lt;p&gt;They don’t provide any detail about the computational architecture, i.e. how they distributed the model. The authors claim it’s because it doesn’t really matter, but I think it was restricted for competitive reasons, as it makes the paper much more difficult to reproduce. Megatron, which I’ll discuss later, was highly influential &lt;em&gt;because&lt;/em&gt; they went into detail about how they made model parallelism work for their GPT.&lt;/p&gt;

&lt;p&gt;What I find &lt;em&gt;really interesting&lt;/em&gt; about the GPT-3 paper is that I don’t think this gets published in a top journal (nature/science), maybe not even NeurIPS. This isn’t a critique of GPT-3— it’s a critique of the modern conference circuit, and if anything, a celebration of the culture that OpenAI has. Most of the conference publishing circuit is driven by novelty, even if it’s not what we need. The GPT-3 paper, however, was a largely engineering driven paper; they made the model bigger and it worked much better! That’s not novel from a research perspective, but is transformative from an application perspective.&lt;/p&gt;

&lt;p&gt;This is particularly problematic because we know that adding complexity to our models increases performance (see: R^2 vs adjusted R^2 for simple linear models). Because of the need for novelty, there are many research projects that don’t get pursued because they’re “only” engineering projects, or they “only” do hyper-parameter tuning and wouldn’t be able to get published, even if they had impressive performance improvements. That OpenAI went against the grain here is a credit to them.&lt;/p&gt;

&lt;p&gt;This is a strength of OpenAI (and Stability.ai, Midjourney, basically everywhere that’s not FAIR/Google Brain/Deepmind/etc). You could alternatively frame it as a weakness of the more academic labs that have promotion/performance review policies driven by publications.&lt;/p&gt;

&lt;h1 id=&quot;jurassic-1&quot;&gt;Jurassic-1&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf&quot;&gt;PDF&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I wasn’t sure whether or not to include Jurassic-1. It’s a model from the Israeli tech company AI21 Labs. I haven’t heard a lot about them, but the paper’s cited by a bunch of the papers later on in the article; they trained a 178B parameter model that outperformed GPT-3 in a few categories, and was faster for inference. It’s impressive that they’re competing with DeepMind, OpenAI, Nvidia, etc. despite only having &lt;a href=&quot;https://en.wikipedia.org/wiki/AI21_Labs&quot;&gt;raised &amp;lt;$$10M&lt;/a&gt; at the time. They made a zero-shot and few-shot test suite &lt;a href=&quot;https://github.com/ai21labs/lm-evaluation&quot;&gt;publicly available&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Like many other papers, they don’t go into detail about the engineering details behind training a large model (178B parameters) over 800 GPUs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/jurassic-computational.png&quot; alt=&quot;Screenshot from Jurassic-1 paper describing how they don&apos;t need to describe the computational details&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The paper is remarkably sparse on details, which I suspect was done for competitive reasons, just like GPT-4.&lt;/p&gt;

&lt;p&gt;Facebook is the only company to go into &lt;a href=&quot;https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/OPT175B_Logbook.pdf&quot;&gt;detail about their experiences&lt;/a&gt; training a 175B parameter model, just like Nvidia is the only company to go into detail about the computational architecture required to train a LLM over many GPUs (see: the Megatron paper, next). In both cases, the companies are &lt;a href=&quot;https://gwern.net/complement&quot;&gt;commoditizing their complements&lt;/a&gt; and strengthening their main lines of business by making it easier to train large models.&lt;/p&gt;

&lt;p&gt;Jurassic uses a different architecture from GPT-3, but again, doesn’t go into much detail:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;76 layers (vs 96 layers for GPT-3)&lt;/li&gt;
  &lt;li&gt;They use the SentencePiece tokenizer, with a large vocabulary of 256K (vs GPT-3 which used BPE w/ ~50k tokens).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Neither of these changes are material, in my opinion. I think what we’re seeing is that there’s a relatively large degree of freedom in model architectures which produce similar results. This is borne out by their evaluation, which has results similar to GPT-3 (better in some categories, worse in others), although Jurassic-1 is faster for inference due to being shallower.&lt;/p&gt;

&lt;p&gt;We’re starting to see a consistent pattern emerge:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Papers introduce a bunch of changes, their own dataset, and have a new SOTA&lt;/li&gt;
  &lt;li&gt;but they don’t do a proper ablation, so it’s tough to understand what was important and what &lt;em&gt;drove&lt;/em&gt; the improvements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPT-2, GPT-3, Jurassic-1, etc. all did this.&lt;/p&gt;

&lt;h1 id=&quot;megatron-turing-nlg&quot;&gt;Megatron-Turing NLG&lt;/h1&gt;

&lt;p&gt;Megatron was a highly influential paper that introduced efficient model-parallel architectures. If you’re interviewing for a LLM job today, you’re going to be expected to be familiar with it. Megatron introduced &lt;em&gt;tensor parallelism&lt;/em&gt;, a variant of model parallelism that splits the models to allow for intra-layer model parallelism, achieving 76% as efficient as a single GPU baseline (although the baseline is only 30% of peak FLOPS).&lt;/p&gt;

&lt;p&gt;Prior to megatron, the published SOTA for model parallelism was to use model pipelining, e.g. &lt;a href=&quot;https://arxiv.org/abs/1811.06965&quot;&gt;GPipe&lt;/a&gt;. However, this was difficult to do and not well supported by code. There were attempts to support tensor parallelism, e.g. &lt;a href=&quot;https://paperswithcode.com/method/mesh-tensorflow&quot;&gt;Mesh-Tensorflow&lt;/a&gt;, which introduced a language for specifying a general class of distributed computations in TensorFlow, but nothing had really dominated. Interestingly, the first author had just left DeepMind 1 year before this was published, so this was possibly his first project at Nvidia.&lt;/p&gt;

&lt;p&gt;Megatron has the realization that, if you have a neural network like this:&lt;/p&gt;

\[Y = f(XW)\]

&lt;p&gt;and you split \(W = \begin{bmatrix} W_1 &amp;amp; W_2 \end{bmatrix}\), i.e. along the columns, then \(Y = \begin{bmatrix}f(X W_1) &amp;amp; f(X W_2)\end{bmatrix}\), so you don’t need to do any synchronization to calculate \(Y\). Consequently, the only points where you need synchronization (all-reduces) in the transformer are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;In the forward pass, to concatenate the model activations after the MLP block before adding dropout&lt;/li&gt;
  &lt;li&gt;In the backwards pass, at the start of the self-attention block.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/megatron-architecture.png&quot; alt=&quot;Graphic showing how megatron parallelism works&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, I strongly suspect this is what GPT-3 and Jurassic-1 both did, but neither went into detail about the specific parallelism models they used, other than to say (from GPT-3):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To train the larger models without running out of memory, we use a mixture of model parallelism within each matrix multiply and model parallelism across the layers of the network.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;Presumably, this style of parallelism is what is meant by “model parallelism within each matrix multiply,” as I find it hard to imagine what else they could mean.&lt;/p&gt;

&lt;h1 id=&quot;gopher&quot;&gt;Gopher&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.11446&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Gopher was a LLM trained by DeepMind. Interestingly, the lead author joined OpenAI shortly after it was published, along with a few of the coauthors. The architecture was the same as GPT-2, except:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They use &lt;a href=&quot;https://arxiv.org/abs/1910.07467&quot;&gt;RMSNorm&lt;/a&gt; (instead of layernorm)&lt;/li&gt;
  &lt;li&gt;Use relative positional encoding scheme from &lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;Transformer-XL&lt;/a&gt; (while GPT-* used a learned positional embedding)&lt;/li&gt;
  &lt;li&gt;They use &lt;a href=&quot;https://arxiv.org/abs/1808.06226&quot;&gt;SentencePiece&lt;/a&gt; (instead of &lt;a href=&quot;https://en.wikipedia.org/wiki/Byte_pair_encoding&quot;&gt;BPE&lt;/a&gt;). This seems to be an Alphabet thing; many of the Alphabet papers use SentencePiece, while most of the non-Alphabet world uses BPE.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper was very interesting from a computational perspective, as they went into detail about how they trained their model and made it work:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They used optimizer state partitioning (&lt;a href=&quot;https://arxiv.org/abs/1910.02054&quot;&gt;ZeRO&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.notion.so/b2293a2b125b4b088656e039fb3b6ca8&quot;&gt;Megatron-style&lt;/a&gt; model parallelism&lt;/li&gt;
  &lt;li&gt;And &lt;a href=&quot;https://paperswithcode.com/method/gradient-checkpointing#:~:text=Gradient%20Checkpointing%20is%20a%20method,small%20increase%20in%20computation%20time.&quot;&gt;rematerialization&lt;/a&gt;/gradient checkpointing to save memory.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all now the standard techniques used to train large models. To the best of my knowledge, Gopher was the first paper to put all of these together and release details about doing so publicly.&lt;/p&gt;

&lt;p&gt;It’s interesting— often, big labs don’t include details for comeptitive reasons. Here, because DeepMind was (arguably) behind, they went into extensive detail. I think we’ll see this increase with LLM research from everyone that’s not OpenAI/Anthropic, as the others don’t live/die by the commercial success of their API, and have strong incentives to make it easier for &lt;strong&gt;**&lt;/strong&gt;others&lt;strong&gt;**&lt;/strong&gt; to train large models (and thereby &lt;a href=&quot;https://gwern.net/complement&quot;&gt;commoditize their complements&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For the paper, DeepMind built a dataset called MassiveText, which was as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/massivetext.png&quot; alt=&quot;Screenshot from the Gopher paper describing their MassiveText dataset&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, this is much smaller than the dataset OpenAI used for GPT-3. GPT-3 had roughly 45TB of text, while MassiveText “only” had about 10.5TB.&lt;/p&gt;

&lt;p&gt;They used this dataset to trained a large model on 300B tokens. The dataset consists of 2.343 trillion tokens, so this is only 12.8%. A much smaller subset. This is interesting to compare to the earlier GPTs, which, if you recall, used 100 epochs (so they saw each token in the dataset 100 times— while Gopher only saw 10% of their tokens once)!&lt;/p&gt;

&lt;p&gt;The Gopher appendices have some great work; someone finally did ablations! They looked at:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://paperswithcode.com/method/adafactor&quot;&gt;Adafactor&lt;/a&gt; vs Adam, and found that Adafactor was much less stable&lt;/li&gt;
  &lt;li&gt;Lower-precision training, trying runs with float16, bfloat16, float32, &lt;a href=&quot;https://nhigham.com/2020/07/07/what-is-stochastic-rounding/&quot;&gt;RandRound&lt;/a&gt;, and using bfloat16 parameters with float32 in the optimiser state (rounding randomly). They found that using float32 parameters for optimisation updates only mitigated the performance loss, saving a substantial amount of memory.&lt;/li&gt;
  &lt;li&gt;Scaling context length; they show how performance increases as the context length increases. Improvements see diminishing returns, but consistently improve. Performance looks roughly proportionate to \(\sqrt{n}\) (where \(n\) is the context length).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s really nice to see detailed empirical work like this— it’s a welcome change from the other papers that failed to do this.&lt;/p&gt;

&lt;h1 id=&quot;chinchilla&quot;&gt;Chinchilla&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/method/chinchilla&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Chinchilla is an incredibly influential paper that established scaling laws. It’s one of my favorite papers from the last few years, as it &lt;em&gt;actually does science&lt;/em&gt; in a way that physicists would agree with. One answer to “is something science” is to say, if you were to meet a historical scientist in your field, could you teach them something? And if you brought Chinchilla to researchers to, say, Radford et. al in 2017, it would advance their work by several years.&lt;/p&gt;

&lt;p&gt;Chinchilla trained over 400 GPT-style transformers, ranging in size from 70M to 16B parameters, and fit the following equation (N is the number of parameters in the LM, and D is the number of tokens in the dataset):&lt;/p&gt;

\[\hat{L}(N, D) = E + \dfrac{A}{N^\alpha} + \dfrac{B}{D^\beta}\]

&lt;p&gt;Choosing \(A, B, E, \alpha, \beta\) to minimize&lt;/p&gt;

\[\sum \limits_{\text{Runs } i} \text{Huber}_{\delta=10^{-3}}(\log \hat{L}_i - \log L_i)\]

&lt;p&gt;where the Huber loss is&lt;/p&gt;

\[\text{Huber}_\delta(x) = \begin{cases}
\frac 1 2 a^2 &amp;amp; |a| \leq \delta,\\
\delta \cdot (|a| - \frac 1 2 \delta) &amp;amp; |a| &amp;gt; \delta
\end{cases}\]

&lt;p&gt;Here, we can think of E as the “irreducible loss” from the dataset, i.e. the loss if we trained an infinitely large model on an infinite stream of tokens. The authors find that the optimal model is (from &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;nostalgebraist&lt;/a&gt; on into the implications of Chinchilla):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-equation.png&quot; alt=&quot;Chinchilla equation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The implication here is that the model size &amp;amp; data size matter roughly equally, which is interesting, given how much attention &amp;amp; effort goes to scaling up the model, and how little attention is given to the dataset.&lt;/p&gt;

&lt;p&gt;The authors then used this equation to determine the optimal model size for the Gopher compute budget, and trained it on more tokens— 1.4T tokens, 4.6x the number of tokens Gopher was trained on. This model, being 4x smaller, has a radically smaller memory footprint and is much faster/cheaper to sample from.&lt;/p&gt;

&lt;p&gt;The Chinchilla paper has been highly influential. Almost every team that I’ve been talking to that is training a LLM right now talks about how they’re training a &lt;em&gt;Chinchilla optimal model&lt;/em&gt;, which is remarkable given that basically everything in the LLM space changes every week.&lt;/p&gt;

&lt;p&gt;The standard practice before Chinchilla was to train your model for 300B tokens, which is what GPT-3, Gopher, and Jurassic-1 all did. Chinchilla reveals how wasteful that was; basically, all of these papers made themselves more expensive to infer by training models that were too large.&lt;/p&gt;

&lt;p&gt;Changes from Chinchilla (otherwise the same as Gopher):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.notion.so/Factual-accuracy-issues-in-LLMs-8257c59fda4040509a40d94523701fad&quot;&gt;AdamW&lt;/a&gt; instead of Adam (there’s an interesting footnote regarding the choice of optimizer: “a model trained with AdamW only passes the training performance of a model trained with Adam around 80% of the way through the cosine cycle, though the ending performance is notably better”)&lt;/li&gt;
  &lt;li&gt;Uses a modified &lt;a href=&quot;https://arxiv.org/abs/1808.06226&quot;&gt;SentencePiece&lt;/a&gt; tokenizer that is slightly different from Gopher (doesn’t apply NFKC normalisation)&lt;/li&gt;
  &lt;li&gt;They compute the forward + backward pass in bfloat16, but store a float32 copy of the weights in the optimizer state. They find that this is basically identically efficient to using float32 everywhere.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of the changes are ablated extensively in the appendix. None of these are particularly novel.&lt;/p&gt;

&lt;h1 id=&quot;palm&quot;&gt;PaLM&lt;/h1&gt;

&lt;p&gt;Speaking of training models that were too large- we have PaLM! Palm was really, really big. &lt;a href=&quot;https://twitter.com/finbarrtimbers/status/1635102571567407105?s=46&amp;amp;t=_LCsoamG7K4pQj0vxlk0XA&quot;&gt;As far as I’m aware&lt;/a&gt;, it’s the largest dense language model trained to date,  at 540B parameters, requiring 6144 TPUs to train on (this is 3 entire TPU pods, each consisting of 2048 TPUs). This is incredibly expensive! Probably only Google has the resources + infrastructure to do this.&lt;/p&gt;

&lt;p&gt;… unfortunately, they were training PaLM at the same time chinchilla was being written. Very suboptimal.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://twitter.com/rasbt/status/1637803700944093184?s=46&amp;amp;t=_LCsoamG7K4pQj0vxlk0XA&quot;&gt;Changes from GPT-3&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1911.02150&quot;&gt;Multi-query attention&lt;/a&gt;. Shares K/V embeddings for each head, but has separate Q embeddings. Makes it much faster during inference time.&lt;/li&gt;
  &lt;li&gt;Uses &lt;a href=&quot;https://twitter.com/rasbt/status/1637803703766769669&quot;&gt;parallel transformer blocks&lt;/a&gt;, which improves training time by 15%. As it was trained using 6144 TPU v4 chips for 1200 hours, the total training cost (at public prices) is between \(1.45 to\)3.22 per chip-hour, for a total of \(10M to\)22M. So this change saved \(1.5M to\)3M.&lt;/li&gt;
  &lt;li&gt;SwiGLU activations, rather than the GELU activation used by GPT-3&lt;/li&gt;
  &lt;li&gt;RoPE embeddings, rather than the learned embeddings&lt;/li&gt;
  &lt;li&gt;Shares the input-output embeddings&lt;/li&gt;
  &lt;li&gt;No bias vectors&lt;/li&gt;
  &lt;li&gt;SentencePiece with 256k tokens&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So, a ton of changes! Again, a bunch of these are common, e.g. using the learned embeddings that GPT-3 had is very passé, and almost no one does it now.&lt;/p&gt;

&lt;h1 id=&quot;llama&quot;&gt;LLaMa&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LLaMa combined a bunch of the best feartures from PaLM and Chinchilla:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pre-normalize the input of each transformer sub-layer&lt;/li&gt;
  &lt;li&gt;RMSNorm, instead of LayerNorm, as done in Gopher&lt;/li&gt;
  &lt;li&gt;SwiGLU activation function from PaLM (but a dimension of \(\frac{2}{3} 4d\) instead of \(4d\), as in PaLM)&lt;/li&gt;
  &lt;li&gt;Uses &lt;a href=&quot;https://arxiv.org/abs/2104.09864&quot;&gt;rotary positional embeddings&lt;/a&gt; (RoPE) instead of the absolute positional embeddings, as done in PaLM&lt;/li&gt;
  &lt;li&gt;Uses AdamW, as done in Chinchilla&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I think that LLaMa is the recipe to follow for the current SOTA in training large models.&lt;/p&gt;

&lt;p&gt;Computational changes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Uses efficient attention (&lt;a href=&quot;https://arxiv.org/abs/2112.05682&quot;&gt;Rabe &amp;amp; Staats&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot;&gt;FlashAttention&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2205.05198&quot;&gt;Gradient checkpointing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Interestingly, they appear to be using float32s everywhere (or at least, don’t say otherwise)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are all similar to Gopher. The one obvious optimization they missed is to use lower precision, as Chinchilla did; I’m curious why they didn’t.&lt;/p&gt;

&lt;p&gt;My one complaint is that I wish they would have trained the model for longer. The learning curve is very far from convergence! This paper is, in my mind, the shining example showing how well smaller models can do when trained well.&lt;/p&gt;

&lt;h1 id=&quot;gpt-4&quot;&gt;GPT-4&lt;/h1&gt;

&lt;p&gt;This is where I’d include information about GPT-4, if there was any. Unfortunately, the &lt;a href=&quot;https://cdn.openai.com/papers/gpt-4.pdf&quot;&gt;GPT-4 technical report&lt;/a&gt; contains almost no information:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;GPT-4 is a Transformer-style model [33] pre-trained to predict the next token in a document, using both publicly available data (such as internet data) and data licensed from third-party providers. The model was then fine-tuned using Reinforcement Learning from Human Feedback (RLHF) [34]. Given both the competitive landscape and the safety implications of large-scale models like GPT-4, this report contains no further details about the architecture (including model size), hardware, training compute, dataset construction, training method, or similar.&lt;/p&gt;

&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/gpt-4-meme.jpeg&quot; alt=&quot;Meme complaining about how OpenAI didn&apos;t relase any details about GPT-4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a result, I’m not going to talk about it, as there’s not much to say. Hopefully OpenAI changes their mind and releases some information about their model.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;This is it, as of March ‘23. I’m sure something new will come along and invalidate all of this.&lt;/p&gt;

&lt;p&gt;What have I missed? Add comments on Substack or &lt;a href=&quot;mailto:finbarrtimbers@gmail.com&quot;&gt;email me&lt;/a&gt; and I’ll update it.&lt;/p&gt;

&lt;p&gt;Articles I’m reading:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://rootnodes.substack.com/p/why-didnt-deepmind-build-gpt3&quot;&gt;Why didn’t DeepMind build GPT-3?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The paper itself doesn’t report the number of tokens, but &lt;a href=&quot;https://skylion007.github.io/OpenWebTextCorpus/&quot;&gt;OpenWebText&lt;/a&gt;, the open source reproduction, gets &lt;a href=&quot;https://github.com/karpathy/nanoGPT/blob/master/data/openwebtext/readme.md&quot;&gt;nine billion&lt;/a&gt;, using &lt;a href=&quot;https://github.com/openai/tiktoken&quot;&gt;OpenAI’s tokenizer&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;To be clear, I highly doubt this will ever happen. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Mon, 27 Mar 2023 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/five-years-of-gpt-progress/</link>
				<guid isPermaLink="true">http://localhost:4000/five-years-of-gpt-progress/</guid>
			</item>
		
			<item>
				<title>How is LLaMa.cpp possible?</title>
				<description>&lt;p&gt;&lt;em&gt;If you want to read more of my writing, I have a &lt;a href=&quot;https://finbarrtimbers.substack.com/&quot;&gt;Substack&lt;/a&gt;. Articles will be posted simultaneously to both places.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Recently, a &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot;&gt;project&lt;/a&gt; rewrote the &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;LLaMa inference code&lt;/a&gt; in raw C++. With some optimizations and quantizing the weights, this allows running a LLM locally on a wild variety of hardware:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;On a &lt;a href=&quot;https://twitter.com/rgerganov/status/1635604465603473408&quot;&gt;Pixel5&lt;/a&gt;, you can run the 7B parameter model at 1 tokens/s.&lt;/li&gt;
  &lt;li&gt;On a &lt;a href=&quot;https://simonwillison.net/2023/Mar/11/llama/&quot;&gt;M2 Macbook Pro&lt;/a&gt;, you can get ~16 tokens/s with the 7B parameter model&lt;/li&gt;
  &lt;li&gt;You can &lt;a href=&quot;https://twitter.com/miolini/status/1634982361757790209&quot;&gt;even run the 7B model on a 4GB RAM Raspberry Pi&lt;/a&gt;, albeit at 0.1 tokens/s.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you are like me, you saw this and thought: What? How is this possible? Don’t large models require expensive GPUs? I took my confusion and dove into the math surrounding inference requirements to understand the constraints we’re dealing with.&lt;/p&gt;

&lt;p&gt;Let’s start with GPUs. GPUs have two main benefits for deep learning:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They have a large amount of memory bandwidth (&lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf&quot;&gt;A100&lt;/a&gt;: 1935 GB/s, &lt;a href=&quot;https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf&quot;&gt;4090&lt;/a&gt;: 1008 GB/s)&lt;/li&gt;
  &lt;li&gt;They have a large amount of compute (&lt;a href=&quot;https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf&quot;&gt;A100&lt;/a&gt;: 312 TFLOPS of FP16, &lt;a href=&quot;https://images.nvidia.com/aem-dam/Solutions/geforce/ada/ada-lovelace-architecture/nvidia-ada-gpu-architecture-whitepaper-1.03.pdf&quot;&gt;4090&lt;/a&gt;: 82.6 TFLOPS of FP16)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When we talk about memory bandwidth, we’re talking about how long it takes to move things from the HBM memory (i.e. the RAM) into the on-chip memory. To actually do math with the GPU, we need to move the matrices in question into the on-chip memory, which is quite small (40MB on an A100, compared to 40-80GB of RAM). Note that the memory bandwidth is ~2 orders of magnitude smaller than the compute performance— this will matter later, as the memory bandwidth tends to be the bottleneck for inference.&lt;/p&gt;

&lt;p&gt;What does this mean in the context of serving LLaMa? Let’s start with some &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;inference arithmetic&lt;/a&gt;. We can do some rough calculations on the inference performance of a LLM using &lt;a href=&quot;https://kipp.ly/blog/transformer-param-count/&quot;&gt;Kipply’s article&lt;/a&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. First, some notation on the dimensions of the model:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The \(Q\), \(K\), and \(V\) weight matrices are all shape [ \(d_{\text{model}}\), \(d_{\text{head}}\)], and we have \(n_{\text{heads}}\) of them per layer; the attention output matrix has the same shape, for a total of  \(4 \times\) [ \(d_{\text{model}}\), \(n_{\text{heads}} \cdot d_{\text{head}}\)]. By convention, GPT-style networks have \(d_{\text{head}} \cdot n_{\text{heads}} = d_{\text{model}}\).&lt;/li&gt;
  &lt;li&gt;The MLP has two weight matrices, of shape [ \(d_{\text{model}}\), \(4 \cdot d_{\text{model}}\)] and [ \(4\cdot d_{\text{model}}\), \(d_{\text{model}}\)]&lt;/li&gt;
  &lt;li&gt;The embeddings matrix is of size [ \(d_{\text{vocab}}\), \(d_{\text{model}}\)].&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This gives us a handy equation for the number of parameters in a GPT-style model:&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

\[P = n_{\text{blocks}} \left( 4 \cdot d_{\text{model}}^2 + 2 \cdot 4 \cdot d_{\text{model}}^2\right) + n_{\text{vocab}} \cdot d_{\text{model}}\]

&lt;p&gt;For the duration of the post, I’m going to focus on the case where we’re running a ChatGPT style service locally, which is what LLaMa.cpp does, letting me assume a batch size of 1.&lt;/p&gt;

&lt;p&gt;For efficient inference, the KV cache has to be stored in memory; the KV cache requires storing the KV values for every layer, which is equal to storing:&lt;/p&gt;

\[n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

&lt;p&gt;I use \(n_{\text{bytes}}\) here to indicate the number of bytes per param; for float32s, this is 4, for float16s, this is 2, etc. The 2 in the middle is because we have to store one set of weights for the K values, and one for the Vs.&lt;/p&gt;

&lt;p&gt;Given a model with n layers, the total memory for the KV cache is:&lt;/p&gt;

\[n_{\text{blocks}} \cdot n_{\text{bytes}} \cdot 2 \cdot d_{\text{model}}\]

&lt;p&gt;In addition to storing the KV cache in memory, we also need to store the weights themselves in memory; this requires \(n_{\text{bytes}} \cdot P\) bytes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-memory-weights.png&quot; alt=&quot;Screenshot of table showing the memory required for LLaMa weights&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the advantage of quantization. By using less precision, we can radically decrease the amount of memory needed to store our models in memory. Note that, with int4 precision, &lt;em&gt;all of these models fit into memory on an A100&lt;/em&gt; (which is the standard datacenter GPU right now), and all of them, except for the biggest model, fit into memory on high-end consumer GPUs (3090s/4090s, which have 24GB of RAM).&lt;/p&gt;

&lt;p&gt;It takes approximately \(2P\) FLOPS to run inference on our model for a single token, because we are doing a bunch of matmuls with a total of \(P\) parameters, and multiplying a matrix of size \((m, n)\) with a vector of size \((n,)\) has a cost of \(2mn\).&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;With all that math out of the way, let’s calculate the requirements for running inference with LLaMa. The main requirements when it comes to sampling are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Keep the KV cache in memory, in addition to all the parameters.&lt;/li&gt;
  &lt;li&gt;Read all the weights from HBM into the on-chip memory. Because we sample auto-regressively, we have to repeat this for each token we sample.&lt;/li&gt;
  &lt;li&gt;Do the actual matmuls to calculate the output of our network.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The latency is the maximum of either the compute or the memory latency, as reading parameters into on-chip memory happens asynchronously in all modern tensor programming libraries. As a result, we write:&lt;/p&gt;

\[\begin{align*}
\text{latency}_\text{model} &amp;amp;= \text{max}(\text{latency}_\text{compute}, \text{latency}_\text{memory})\\
\text{latency}_\text{memory} &amp;amp;= \dfrac{2 \cdot P \cdot n_{\text{bytes}}\cdot B}{n_{\text{memory bandwidth}}},\\
\text{latency}_\text{compute} &amp;amp;= \dfrac{2 \cdot P}{n_{\text{flops}}},
\end{align*}\]

&lt;p&gt;where \(B\) is the batch size. As \(n_{\text{memory bandwidth}} = 1.935e12\), and  \(n_{\text{flops}} = 3.12e14,\) as long as the batch size is less than 161, the model is memory-bound.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;With a batch size of 1, this is the same equation, as on most hardware (e.g. Nvidia GPUs), there is a linear speedup as you decrease the precision (you get twice the FLOPS when using fp16 vs fp32, which doubles again as you go to int8, and doubles once more as you go to int4s).&lt;/p&gt;

&lt;p&gt;As LLaMa.cpp uses int4s, the RAM requirements are reduced to 1.33GB of memory for the KV cache, and 16.25GB of VRAM for the model parameters. That’s pretty good!&lt;/p&gt;

&lt;p&gt;As the memory bandwidth is almost always&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; much smaller than the number of FLOPS, memory bandwidth is the binding constraint.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-inference-times.png&quot; alt=&quot;Screenshot fo table showing the inference times to run the varying LLaMa models with varying precision levels on an A100&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;running-llama-on-an-a100&quot;&gt;Running LLaMa on an A100&lt;/h2&gt;

&lt;p&gt;On an A100 (80GB PCIe), the memory bandwidth is 1935GB/s. The int4 compute is 1248 TOPS. As such, the model is (heavily) memory-bound. We should expect to see inferences as given in the table; roughly 30 tokens/s with the 65B model, and 277 tokens/s with the 7B model.&lt;/p&gt;

&lt;h2 id=&quot;running-llama-on-a-m1-macbook-air&quot;&gt;Running LLaMa on a M1 Macbook Air&lt;/h2&gt;

&lt;p&gt;The M1 GPU has a bandwidth of &lt;a href=&quot;https://www.macworld.com/article/783678/m2-vs-m1-chip-performance-graphics-ram.html&quot;&gt;68.25 GB/s&lt;/a&gt;, while the M1 GPU can do up to &lt;a href=&quot;https://tlkh.dev/benchmarking-the-apple-m1-max#heading-gpu-matrix-multiplication-gemm-performance&quot;&gt;5.5 TFLOPS&lt;/a&gt; of fp16 compute. As such, we should expect a ceiling of ~1 tokens/s for sampling from the 65B model with int4s, and 10 tokens/s with the 7B model.&lt;/p&gt;

&lt;p&gt;As the M2 Pro has 200 GB/s of bandwidth, and the M2 Max has 400 GB/s of bandwidth, we should expect massive improvements with them, going up to 6 tokens/s with the M2 Max with the 65B model. That’s pretty darn good for a laptop.&lt;/p&gt;

&lt;h2 id=&quot;running-llama-on-a-raspberry-pi-4&quot;&gt;Running LLaMa on a Raspberry Pi 4&lt;/h2&gt;

&lt;p&gt;A Raspberry Pi 4 has &lt;a href=&quot;https://web.eece.maine.edu/~vweaver/group/green_machines.html&quot;&gt;13.5 GFLOPS of compute&lt;/a&gt;, and &lt;a href=&quot;https://forums.raspberrypi.com/viewtopic.php?t=281183&quot;&gt;~4GB/s of memory bandwidth&lt;/a&gt;. Given this, we’d expect to see ~2 tokens/s with the 7B model if it was memory bound. Given that we’re currently seeing ~0.1 tokens/s, I suspect we’re actually compute-bound (although this is a stab in the dark— I can’t find enough information about the specs for a Raspberry Pi to determine this with any precision).&lt;/p&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Memory bandwidth is the limiting factor in almost everything to do with sampling from transformers. Anything that reduces the memory requirements for these models makes them &lt;em&gt;much&lt;/em&gt; easier to serve— like quantization! This is yet another reason why distillation, or just &lt;a href=&quot;https://finbarr.ca/llms-not-trained-enough/&quot;&gt;training smaller models for longer&lt;/a&gt;, is really important.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: I’m not an expert in CUDA, so I probably have errors in my math. If so, please shoot me an &lt;a href=&quot;mailto:finbarrtimbers@gmail.com&quot;&gt;email&lt;/a&gt; and let me know- I’d love to hear from you so I can learn more about how this works and update this post.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Resources on transformer inference performance:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/posts/2023-01-10-inference-optimization/&quot;&gt;Large Transformer Model Inference Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;Transformer inference arithmetic&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kipp.ly/blog/transformer-param-count/&quot;&gt;LLM parameter counting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot;&gt;Efficient Transformers&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Thank you to &lt;a href=&quot;https://twitter.com/kaushikpatnaik?lang=en&quot;&gt;Kaushik Patnaik&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/arthurallshire&quot;&gt;Arthur Allshire&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/stanislavfort&quot;&gt;Stanislav Fort&lt;/a&gt;, and &lt;a href=&quot;https://twitter.com/banburismus_&quot;&gt;Tom McGrath&lt;/a&gt; for reading early drafts of this.&lt;/em&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Almost all of this math is taken from their article; they deserve full credit. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Although it’s an open question of how much Mac specific optimization is being done with LLaMa, or indeed, any of the tensor programming frameworks. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For a more detailed discussion showing that this is the case, check out &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/#flops-counting&quot;&gt;kipply’s article&lt;/a&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If you’re following along with &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/#latency-calculations&quot;&gt;kipply’s post&lt;/a&gt;, there’s a slight discrepancy here, as I assume a 80GB A100 PCIe, which is what I see as the standard GPU. Their post assumes a 40GB A100 PCIe, which has slightly lower memory bandwidth. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I hedge with “almost” here, but I’m not aware of any counterexamples. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Thu, 16 Mar 2023 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/how-is-llama-cpp-possible/</link>
				<guid isPermaLink="true">http://localhost:4000/how-is-llama-cpp-possible/</guid>
			</item>
		
			<item>
				<title>A step towards self-improving LLMs</title>
				<description>&lt;p&gt;&lt;em&gt;There&apos;s a &lt;a href=&quot;https://finbarrtimbers.substack.com/p/a-step-towards-self-improving-llms&quot;&gt;Substack&lt;/a&gt; version of this post, if you prefer that over my &lt;del&gt;amateurish&lt;/del&gt; artisan HTML.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If I look at GPTs/LLMs, two of the biggest problems I see with existing techniques are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We need our models to be able to generate data by themselves, i.e. we need a &lt;a href=&quot;https://www.lesswrong.com/tag/recursive-self-improvement&quot;&gt;recursive self-improvement loop&lt;/a&gt;. AlphaZero is the shining example of what’s possible here.&lt;/li&gt;
  &lt;li&gt;We need our models to be able to operate in new domains without requiring massive amounts of existing data. CLIP provides an option here, as does Internet Explorer (the paper, not the browser).&lt;/li&gt;
  &lt;li&gt;Auto regressive sampling. It’s slow, suboptimal.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I have ideas for how to tackle #1, so I’ll focus on that here.&lt;/p&gt;

&lt;p&gt;There are other issues facing LLMs, such as:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Increasing the length of the context window&lt;/li&gt;
  &lt;li&gt;Figuring out how to train larger models&lt;/li&gt;
  &lt;li&gt;Figuring out how to train more efficient models (less parameters, less data, less energy)&lt;/li&gt;
  &lt;li&gt;Factual accuracy&lt;/li&gt;
  &lt;li&gt;Mitigating attacks that convince LLMs to exhibit harmful behaviour (”red-teaming”), e.g. prompt injection&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I think these are fundamentally engineering problems that we’ll be able to figure out iteratively. For instance, context length has seen a lot of progress with &lt;a href=&quot;https://openreview.net/forum?id=H4DqfPSibmx&quot;&gt;subtle algorithmic&lt;/a&gt; &lt;a href=&quot;https://arxiv.org/abs/2112.05682&quot;&gt;improvements&lt;/a&gt;; if we combine those changes with the many &lt;a href=&quot;https://twitter.com/karpathy/status/1621578354024677377&quot;&gt;arcane engineering optimizations&lt;/a&gt; that are out there, I think we’ll get to a point where context goes to 64k tokens or more, at which point we’ll be deep in the &lt;a href=&quot;http://finbarr.ca/the-sigmoid/&quot;&gt;saturating point of the sigmoid&lt;/a&gt;. Or for factual accuracy- I think that retrieval will largely solve that once it’s incorporated into most models.&lt;/p&gt;

&lt;p&gt;However, I’m probably wrong, and could very well end up writing a version of this post in 2034 talking about how the biggest problem facing AGI is prompt injections.&lt;/p&gt;

&lt;h2 id=&quot;a-path-towards-recursive-self-improvement&quot;&gt;A path towards recursive self-improvement&lt;/h2&gt;

&lt;p&gt;GPTs work very well in one specific context: they are very, very good at finding text that is likely to follow other text in a way that appears natural to humans.&lt;/p&gt;

&lt;p&gt;What they don’t do is come up with text that they haven’t seen before. Kinda. What they’re doing when we sample from them now is predict what they’ve seen during training. Sometimes these predictions produce text that hasn’t been written before (this can occur often, due to the combinatorial nature of token sampling). When this happens, it’s a happy accident. The model isn’t trying to select text that is novel or that accomplishes any goal other than &lt;em&gt;following the preceding 2048 tokens&lt;/em&gt; (or whatever the context length is).&lt;/p&gt;

&lt;p&gt;The obvious exception is when models are finetuned using &lt;a href=&quot;https://arxiv.org/abs/1706.03741&quot;&gt;RLHF&lt;/a&gt;. In RLHF, the models are explicitly trained to optimize a reward signal. In RLHF, the reward signal comes from a model trained to predict human feedback. Basically, humans are asked to choose between two samples of text, and then a model learns to predict which one is preferred.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/rlhf-reward-loop.png&quot; alt=&quot;RLHF training loop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Why does this matter? Predicting the next token works pretty well! And &lt;a href=&quot;https://twitter.com/sama/status/1599471830255177728?s=20&quot;&gt;maybe we’re all&lt;/a&gt; just &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3442188.3445922&quot;&gt;stochastic parrots&lt;/a&gt;? It matters because the biggest impediment to improving our models right now is the &lt;em&gt;lack of data&lt;/em&gt;. The scaling law papers (&lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI&lt;/a&gt;) consistently point to the fact that we need to scale up the datasets we train LLMs on.&lt;/p&gt;

&lt;p&gt;For instance, Chinchilla predicts that we’ll need 11 &lt;em&gt;trillion&lt;/em&gt; tokens to optimally train a model the size of PaLM (i.e. 540B parameters). If we want to push past PaLM to a model with 1 trillion parameters, we’ll need 20T tokens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-optimal-data-requirements.png&quot; alt=&quot;chinchilla-optimal-data-requirements.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;That’s a lot of data. That’s so much data that it’s not clear that we can get that from existing sources. &lt;a href=&quot;https://twitter.com/nostalgebraist?lang=en&quot;&gt;nostalgebraist&lt;/a&gt; &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;argues&lt;/a&gt; that 1) we’ve basically exhausted the available data in structured domains like coding and 2) it’s starting to look like we’re running out of general-domain data. I find nostalgebraist compelling; the only counterargument I could see is that private data sources might be a rich vein of tokens, but I don’t see a clear path to getting access to them.&lt;/p&gt;

&lt;p&gt;This lack of data is unfortunate because, according to &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;Chinchilla’s scaling laws&lt;/a&gt;, we could see another ~8% reduction in training loss (1.93 → 1.77, delta of 0.16 in loss) for if we had infinite data &lt;em&gt;while changing nothing else about Chinchilla&lt;/em&gt;. That’s a pretty substantial improvement when you consider that the improvement from Gopher to Chinchilla was only 2.9% (1.99 → 1.93, delta of 0.06 in loss), not to mention the fact that our models are already quite good— able to &lt;a href=&quot;https://www.bbc.com/news/technology-62275326&quot;&gt;trick Google SWEs into believing they’re sentient&lt;/a&gt;, and &lt;a href=&quot;https://www.lesswrong.com/posts/FKNtgZrGYwgsz3nHT/bankless-podcast-159-we-re-all-gonna-die-with-eliezer&quot;&gt;scaring the Yud&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;more-data&quot;&gt;More data&lt;/h3&gt;

&lt;p&gt;The clear implication is that we need way more data! Our models are desperate for data. They’re lying on the beach &lt;em&gt;gasping for more data&lt;/em&gt; to quench their ever-growing thirst.&lt;/p&gt;

&lt;p&gt;But where will the data come from?&lt;/p&gt;

&lt;p&gt;If we can scrape it we should. It’s not clear how much there is left to scrape. Especially at the largest research institutions like OpenAI, Google Brain, and DeepMind, I’m certain that they have teams of engineers working on scraping all possible data. There is some possibility to automate this process; the excellently named &lt;a href=&quot;https://arxiv.org/abs/2302.14051&quot;&gt;Internet explorer paper&lt;/a&gt; presented a model which crawls the web to get additional data to augment it’s dataset. Although letting a nascent AI loose on the internet would make Eliezer cry, it could be an excellent source of data, especially if one incorporates some sort of reinforcement learning style feedback loop to continually improve the manner in which the model searches the web.&lt;/p&gt;

&lt;p&gt;The data problem is compounded by the fact that high quality data &lt;em&gt;really matters&lt;/em&gt;. Experiments consistently show that deduplicating data increases performance substantially (&lt;a href=&quot;https://arxiv.org/abs/2205.10487&quot;&gt;https://arxiv.org/abs/2205.10487&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2107.06499&quot;&gt;https://arxiv.org/abs/2107.06499&lt;/a&gt;). Basically, I’m not convinced there is a lot more high quality data. Two exceptions might be commercial data (e.g. internal corporate documents), and all copyrighted text. But it would be extremely difficult to get access to either of these corpora.&lt;/p&gt;

&lt;h3 id=&quot;generate-data&quot;&gt;Generate data&lt;/h3&gt;

&lt;p&gt;The solution to me seems to be self-evident: we should generate our own data.
There has been some work about training LLMs on data they have generated (&lt;a href=&quot;https://arxiv.org/abs/2210.11610&quot;&gt;https://arxiv.org/abs/2210.11610&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;https://arxiv.org/abs/2212.08073&lt;/a&gt;). There are a few different techniques that seem promising here.&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://arxiv.org/abs/2210.11610&quot;&gt;Huang et. al&lt;/a&gt;, they use Chain of Thought (CoT) reasoning to generate additional data. Given a dataset of questions, they sample N answers that use CoT to generate an answer. At the end, they ask “The answer is “ and get an answer; they then find the majority answer and choose all texts that return the same answer as the most common answer, using this to generate additional data. In practice, there’s no reason to questions, although that is a particularly straight forward problem to apply this to; one could imagine, say, embedding all of the generated answers, clustering them, and keeping the answers in the biggest cluster, or employing RLAIF like Anthropic did in the &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;Constitutional AI&lt;/a&gt; paper to select the answers to keep.&lt;/p&gt;

&lt;p&gt;Anthropic employed a similar approach as the CoT reasoning in the &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot;&gt;Constitutional AI paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Another option is to use RLAIF (from Constitutional AI) to generate data. In this,&lt;/p&gt;

&lt;h2 id=&quot;throw-compute-at-the-problem&quot;&gt;Throw compute at the problem&lt;/h2&gt;

&lt;p&gt;Yet another line of research involves throwing compute at the problem. We know that we can use a variety of techniques to soak up compute and improve outcomes. For instance, &lt;a href=&quot;https://en.wikipedia.org/wiki/Ensemble_learning&quot;&gt;ensembling&lt;/a&gt; is a classic ML technique that strictly improves model performance. Given that we are already at the extreme limit of what’s possible to compute with transformers, it is almost certainly not possible to naively ensemble LLMs.&lt;/p&gt;

&lt;p&gt;However, what we can do is use compute to apply search on top of our existing model outputs. If we can find a &lt;a href=&quot;https://proceedings.neurips.cc//paper/2020/file/22eda830d1051274a2581d6466c06e6c-Paper.pdf&quot;&gt;policy improvement operator&lt;/a&gt;, i.e. a function T that takes an existing distribution over tokens, π, and returns a new distribution, T(π), which improves our loss, then we can use T to improve our model. Some candidates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Best-of-n&lt;/li&gt;
  &lt;li&gt;Beam search&lt;/li&gt;
  &lt;li&gt;Policy-driven search&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;“Best-of-n” (&lt;a href=&quot;https://openai.com/research/measuring-goodharts-law&quot;&gt;https://openai.com/research/measuring-goodharts-law&lt;/a&gt;) is a technique similar to ensembling in which we sample from our model N times, and use the sample with the highest score according to our objective function. This performs remarkably well (outperforming the RLHF model in the WebGPT paper(&lt;a href=&quot;https://openai.com/research/webgpt&quot;&gt;https://openai.com/research/webgpt&lt;/a&gt;)), is simple to implement, trivial to analyze mathematically, and trivially parallelizable, but makes inference N times more expensive. If I were OpenAI, I’d be caching the results of queries to their models and doing this for repeated queries.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/webgpt-best-of-n-performance.png&quot; alt=&quot;best-of-n performance curves from the WebGPT paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the WebGPT paper, the authors found that best-of-16 resulted in an improvement in human preferences of 5% (60% → 65%), while going from 13B parameters to 175B parameters resulted in an improvement of 10% (~47% → 57%) (results from eyeballing graph, not precise).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/webgpt-scaling-performance.png&quot; alt=&quot;scaling performance graph from WebGPT paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given that both charts appear to be seeing roughly linear improvements, and both models increase in cost roughly linearly, it seems to imply that a best-of-64 13B model would be better than a best-of-4 175B model, while having roughly the same cost in terms of compute. Given that the 13B model should fits on a single GPU (maybe 2 GPUs?), this would substantially lower the overall compute of the system.&lt;/p&gt;

&lt;p&gt;Another improvement operator is a NLP classic: &lt;a href=&quot;https://en.wikipedia.org/wiki/Beam_search&quot;&gt;beam search!&lt;/a&gt; In beam search, one performs a breadth-first search over the model outputs, with finite depth and width of the tree (e.g. it only keeps N successors at each level of the tree, and searches to a depth of M levels), with the final result being the sequence with the maximum objective score (typically log-likelihood).&lt;/p&gt;

&lt;p&gt;While a number of the LLMs do use beam search&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, they don’t appear to report performance numbers, so I’m unable to include a comparison of how much it matters.&lt;/p&gt;

&lt;p&gt;A concern is that beam search still lowers diversity, as it constricts the difference in tokens; this is especially problematic for byte-level tokenizers, like BPE, as the individual tokens might vary significantly. &lt;a href=&quot;https://twitter.com/sedielem&quot;&gt;Sander Dieleman&lt;/a&gt; &lt;a href=&quot;https://sander.ai/2020/09/01/typicality.html&quot;&gt;wrote about how&lt;/a&gt; strategies like beam search are &quot;the culprit behind many of the pathologies that neural machine translation systems exhibit”.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The final candidate (or family of candidates) for the improvement operator is an option that I find very exciting: learning an algorithm to search the token tree. The idea is that we could do something like AlphaZero which would learn a policy + value function.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; This would also allow us to change the reward function if we wanted to, rather than just using the standard log-likelihood. We could, for instance, directly train the reward function on human data. If you’re serving data to millions of users per day, you could just directly run RL on that, which is the case for the myriad of chat bots on the market today (Bing, ChatGPT, Claude, etc.).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.notion.so/Discussion-of-beam-search-diversity-f7b65583098f4f219e0e9fd59d5ab80d&quot;&gt;Discussion of beam search diversity&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Examples include: GPT-{2,3}, which uses it during decoding for text generation, BERT, for language understanding tasks, T5, and XLNet. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sander’s post is great. I was struggling to understand why beam search isn’t used more in practice, and his post did a great job helping me understand why. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Perhaps using MuZero with a smaller model as the recurrent function to save on compute. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Tue, 07 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/self-improving-LLMs/</link>
				<guid isPermaLink="true">http://localhost:4000/self-improving-LLMs/</guid>
			</item>
		
			<item>
				<title>Papers I've read this week (March 4th, 2023)</title>
				<description>&lt;p&gt;I’m going to try to write a weekly summary of the most interesting papers I’ve read that week. I’d love to hear what papers you’ve been reading, if you agree/disagree about my conclusions for each paper, and/or suggestions for what papers I should read next!&lt;/p&gt;

&lt;h2 id=&quot;scaling-laws-for-routed-language-models&quot;&gt;Scaling laws for routed language models&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.01169&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://www.deepmind.com/&quot;&gt;used to work&lt;/a&gt; with Aidan, and way back in 2021, he was insistent that LLMs were the future of AI. I thought he was crazy. In ‘22, he also insisted that conditional routing models were the future. Given how right he was about LLMs, it’s probably worth paying attention to conditional routing models.&lt;/p&gt;

&lt;p&gt;The paper provides a great general overview of how routing networks work and performance comparisons (in terms of negative log likelihood over a validation dataset) for the 3 most common routing techniques (&lt;a href=&quot;https://arxiv.org/abs/1701.06538&quot;&gt;sparse MoE&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.04426&quot;&gt;non-parametric HASH&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1308.3432&quot;&gt;RL routing&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The authors trained a large number of conditional routing networks, and fit scaling laws to the results; they find that all 3 techniques follow the same scaling laws, with RL routing doing quite well. I’d be curious to see how much effort has been put into improving RL routing; I suspect that it could be improved significantly.&lt;/p&gt;

&lt;p&gt;The authors observed the following results:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Routing improves the performance of language models across all sizes and variants attempted&lt;/li&gt;
  &lt;li&gt;Training a Routing Network with RL is of comparable effectiveness to state-of-the-art techniques.&lt;/li&gt;
  &lt;li&gt;The performance of all Routing Networks is accurately described by scaling laws in the number of experts and in
the underlying dense model size.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I was surprised at how similar the performance of the various techniques was. The data was quite nice, with little variation. The scaling laws seem to fit the data quite nicely.&lt;/p&gt;

&lt;p&gt;One interesting result was that routing helps significantly more when the model is smaller. I found this surprising; my intuition is that routing should always help. They found that this was the case across all models, and that routing helped less as the models grew.&lt;/p&gt;

&lt;p&gt;The paper ends with recommendations, which I found really useful:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use routing for models with less than 1.3B parameters&lt;/li&gt;
  &lt;li&gt;S-Base is a good, default routing algorithm (defined in the appendix of their paper).&lt;/li&gt;
  &lt;li&gt;Target using E in {64, 128} experts.&lt;/li&gt;
  &lt;li&gt;Use K = 1 experts; route layers with a frequency between 0.5 &amp;amp; 1; lower frequency reduces performance.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;internet-explorer&quot;&gt;Internet explorer&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.14051&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In this paper, the authors create an agent which dynamically explores the internet, running text queries to find images to use for self-supervised training. While seemingly designed to &lt;a href=&quot;https://rationalwiki.org/wiki/AI-box_experiment&quot;&gt;directly antagonize Yudkowsky&lt;/a&gt;, the paper is extremely interesting, and presents, to me, a potential future direction for AGI research. As &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt; showed us, LLMs could &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications#fnjefvfidovdb&quot;&gt;massively improve&lt;/a&gt; with more data. Having agents dynamically exploring the internet is one excellent way to get more data- especially if they’re able to adaptively learn over time and prioritize images accordingly.&lt;/p&gt;

&lt;p&gt;In the paper, they train a model to learn representations of images based on &lt;a href=&quot;https://paperswithcode.com/method/moco-v3&quot;&gt;MoCo-v3&lt;/a&gt;. They query Google Images for new images, ranking the query results by similarity to the target dataset, assigning a reward to the new images:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/internet-explorer-reward.png&quot; alt=&quot;reward equation for internet explorer paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, S_cos is the cosine similarity, f_k is the image encoder, D := {x_i} is the target dataset, and y is the new image to evaluate, and they evaluate over the k closest neighbours in the target dataset (where “closest” is determined by the encoded representation).&lt;/p&gt;

&lt;p&gt;They create the queries for Google Images by sampling them a static vocabulary dataset. They estimate the reward associated with the query using a Gaussian process regression. I’d be really interested to see a fancier query generation process. One idea that comes to my mind would be using RL to train a LLM to generate queries in a manner similar to what’s done in &lt;a href=&quot;https://openai.com/research/learning-from-human-preferences&quot;&gt;RLHF&lt;/a&gt;/&lt;a href=&quot;https://arxiv.org/abs/2204.05862&quot;&gt;RLAIF&lt;/a&gt;, i.e. use an RL algorithm like PPO to finetune a pretrained LLM to maximize reward. This would require much more compute, however.&lt;/p&gt;

&lt;h2 id=&quot;llama&quot;&gt;LLaMa&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I’ve been digesting the LLaMa paper that Facebook released this week. It was very interesting to see the performance increases they got despite the size decreases. Their 13B model outperformed GPT-3 on a number of benchmarks, and their 65B model was competitive with Chinchilla-70B and PaLM-540B (!).&lt;/p&gt;

&lt;p&gt;I did find it incredibly frustrating that they stopped training when they did; their loss curves are all looking pretty far from convergence, and I’m curious to see how much the models will continue to improve:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;loss curves for LLaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I wish that they had just &lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/#6-squeeze-out-the-juice&quot;&gt;left it training&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;My biggest question about the paper is that it’s not clear what caused the improvements. They discuss a few major changes compared to GPT-3, which their model is based on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They only use publicly available data, but it’s unclear what exactly the filtering steps are. I wish they’d open source their dataset (or at least, the code to clean it).&lt;/li&gt;
  &lt;li&gt;They normalize the input of each transformer sub-layer, rather than the output.&lt;/li&gt;
  &lt;li&gt;They use the SwiGLU activation function, as PaLM did, with a slight dimensional difference compared to PaLM.&lt;/li&gt;
  &lt;li&gt;They use Rotary Embeddings.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There was no ablation study, unfortunately. If I can scrounge up the GPUs, I’m tempted to do my own ablation based on &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot;&gt;nanoGPT&lt;/a&gt;. LLaMa also uses &lt;a href=&quot;https://arxiv.org/abs/22015.14135&quot;&gt;FlashAttention&lt;/a&gt;, which I suspect will become the default attention implementation used in LLMs going forward.&lt;/p&gt;

&lt;p&gt;Anecdotally, it seems like performance with the default parameters &lt;a href=&quot;https://twitter.com/browserdotsys/status/1632145830277795840&quot;&gt;isn&apos;t great&lt;/a&gt; (unless you&apos;re writing &lt;a href=&quot;https://twitter.com/browserdotsys/status/1632512136906719232&quot;&gt;erotic stories&lt;/a&gt;). However, it hasn&apos;t been tuned (RLHF/SFT), so maybe &lt;a href=&quot;https://twitter.com/yacineMTB/status/1632392979833921539&quot;&gt;that would make a difference&lt;/a&gt;? With some &lt;a href=&quot;https://twitter.com/theshawwn/status/1632569215348531201&quot;&gt;modifications&lt;/a&gt;, performance is apparently pretty good. This lends strength to my hypothesis that the paper was rushed out in response to the LLM gold rush we&apos;ve seen since ChatGPT was released. The paper, while strong, would be much stronger with a few more changes (ablation, more details, slightly more polished code), in a way that doesn&apos;t make sense for the omissions to be strategic.&lt;/p&gt;

&lt;p&gt;Facebook benefits when open source LLMs get better in a way that OpenAI/Anthropic/Cohere/etc. doesn&apos;t, so they should want to do ablations and other scientific work to advance the field. The fact that they didn&apos;t include this makes me think that they wanted to get the paper out the door as soon as possible- probably to avoid the chance of being scooped with an even larger, better, model.&lt;/p&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I haven’t seen a great ablation study comparing various embedding schemes. This is on my list of experiments to do once I can scrounge up GPUs. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
				<pubDate>Sat, 04 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/papers-ive-read/</link>
				<guid isPermaLink="true">http://localhost:4000/papers-ive-read/</guid>
			</item>
		
			<item>
				<title>The Sigmoid: a metaphor for technological progress</title>
				<description>&lt;p&gt;I regularly reference the “s-curve”, or sigmoid, as a metaphor for progress. Here, I explain what I mean, so that I can just link to this post.&lt;/p&gt;

&lt;p&gt;A common mathematical relationship in technology is the s-curve (or sigmoid curve). Mathematically:&lt;/p&gt;

\[\text{sigmoid}(x) := \dfrac{1}{1 + e^{-x}} \equiv \dfrac{e^x}{e^x + 1}\]

&lt;p&gt;This is notable because it produces a curve that &lt;em&gt;looks like an s&lt;/em&gt; (&lt;a href=&quot;https://en.wikipedia.org/wiki/Sigmoid_function&quot;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/sigmoid.png&quot; alt=&quot;The sigmoid curve&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In particular, when x is small, this grows slowly, when x is not too big or not too small, it grows exponentially, and when x is large, it grows slowly. This is a common pattern with many technologies! We see slow progress at first, then it accelerates rapidly, and finally, as we begin to hit the limits of the technology, progress slows. Consider single-thread CPU performance. Initially, progress was slow as people figured out how to make them. Then, it grew exponentially for years, following Moore’s Law (&lt;a href=&quot;https://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/&quot;&gt;picture source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/single-threaded-cpu-performance.png&quot; alt=&quot;Single-threaded CPU performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, since around 2010, we haven’t seen a lot of improvements in single-thread CPU performance. That pattern- slow, fast, then slow- is what I mean when I talk about the s-curve in technology. And when I talk about entering the &lt;em&gt;saturating part of the s-curve&lt;/em&gt;, I mean that we’re entering the region where progress is slowing down again.&lt;/p&gt;
</description>
				<pubDate>Thu, 02 Mar 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/the-sigmoid/</link>
				<guid isPermaLink="true">http://localhost:4000/the-sigmoid/</guid>
			</item>
		
			<item>
				<title>Large language models aren't trained enough.</title>
				<description>&lt;p&gt;&lt;em&gt;I have a &lt;a href=&quot;https://finbarrtimbers.substack.com/&quot;&gt;Substack&lt;/a&gt; if you want to be notified when I write.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This was inspired by tweets from several people:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629159764918775809?s=20&quot;&gt;https://twitter.com/BlancheMinerva/status/1629159764918775809&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/BlancheMinerva/status/1629551017019711488&quot;&gt;https://twitter.com/BlancheMinerva/status/1629551017019711488&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/kaushikpatnaik/status/1629194428312330240?s=46&amp;amp;t=x3wLedGK_QyDwCK5yD2Jqw&quot;&gt;https://twitter.com/kaushikpatnaik/status/1629194428312330240&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://twitter.com/thedavidsj/status/1629869851710984194?s=46&amp;amp;t=CqBoSVdxuipOpACbrZnMxg&quot;&gt;https://twitter.com/thedavidsj/status/1629869851710984194&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and Twitter conversations with &lt;a href=&quot;https://twitter.com/arankomatsuzaki/&quot;&gt;Aran Komatsuzaki&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/andy_l_jones&quot;&gt;Andy Jones&lt;/a&gt;. Any mistakes are entirely my own.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note: although I did work at DeepMind previously, I was not involved with any of the language efforts, and have no non-public knowledge of what went on there. Unfortunately! I think Chinchilla is a great paper that I would have loved to be a part of.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I’ve been &lt;a href=&quot;[https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645](https://www.cbc.ca/news/canada/edmonton/alphabet-closing-edmonton-deepmind-office-1.6724645)&quot;&gt;on the job market since January&lt;/a&gt;, and I’ve been talking to a lot of companies training large language models (LLMs). The consistent phrasing that comes up is that they want to train a &lt;em&gt;Chinchilla-optimal model&lt;/em&gt;  (Chinchilla here referring to the &lt;a href=&quot;[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)&quot;&gt;DeepMind paper from spring ‘22&lt;/a&gt;, not the adorable rodent).&lt;/p&gt;

&lt;h3 id=&quot;chinchilla&quot;&gt;Chinchilla&lt;/h3&gt;

&lt;p&gt;The Chinchilla paper was an attempt to identify the optimal model size &amp;amp; number of tokens to train a LLM given a particular compute budget. The paper trained 400 (!) language models and found a clear relationship between # of model parameters and # of training tokens: the two should scale linearly, i.e. if you double model size, you should double the number of training tokens. The authors used this relationship (which we call a &lt;em&gt;scaling law)&lt;/em&gt; to train a new model, Chinchilla, which had the same compute budget as Gopher, an earlier DeepMind model, and were able to significantly outperform Gopher + GPT-3 + a number of larger models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-convergence.png&quot; alt=&quot;Loss curves from the Chinchilla paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When people talk about training a Chinchilla-optimal model, this is what they mean: training a model that matches their estimates for optimality. They estimated the optimal model size for a given compute budget, and the optimal number of training tokens for a given compute budget.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/chinchilla-optimal.png&quot; alt=&quot;Chinchilla-optimal levels of compute/data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;However, when we talk about “optimal” here, what is meant is “what is the cheapest way to obtain a given loss level, in FLOPS.” In practice though, we don’t care about the answer! This is exactly the answer you care about if you’re a researcher at DeepMind/FAIR/AWS who is training a model with the goal of reaching the new SOTA so you can publish a paper and get promoted. If you’re training a model with the goal of actually deploying it, the training cost is going to be dominated by the inference cost. This has two implications:&lt;/p&gt;

&lt;p&gt;1) there is a strong incentive to train smaller models which fit on single GPUs&lt;/p&gt;

&lt;p&gt;2) we’re fine trading off training time efficiency for inference time efficiency (probably to a ridiculous extent).&lt;/p&gt;

&lt;p&gt;Chinchilla implicitly assumes that the majority of the total cost of ownership (TCO) for a LLM is the training cost. In practice, this is only the case if you’re a researcher at a research lab who doesn’t support products (e.g. FAIR/Google Brain/DeepMind/MSR). For almost everyone else, the amount of resources spent on inference will dwarf the amount of resources spent during training.&lt;/p&gt;

&lt;p&gt;Let’s say you’re OpenAI and you’re serving GPT-4 as BingChat. In addition to hiring experienced &lt;a href=&quot;https://twitter.com/chrisjbakke/status/1628877552940097536&quot;&gt;killswitch engineers&lt;/a&gt; to thwart Sydney’s repeated escape attempts, you have to choose exactly which model to deploy.&lt;/p&gt;

&lt;p&gt;To run inference on N tokens of text, OpenAI charges &lt;a href=&quot;https://openai.com/api/pricing/&quot;&gt;$2e-5/token&lt;/a&gt; for their most advanced model. Assuming a 60% gross margin, it costs them $8e-6/token to serve. A rough cost estimate to train GPT-3 is &lt;a href=&quot;[https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/](https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_language_model/)&quot;&gt;$5M&lt;/a&gt;. As such, after serving 625B tokens, their costs are going to be dominated by inference, rather than serving. When I use ChatGPT, it typically generates 300 tokens worth of responses to me. That’s 20B responses. If ChatGPT has 10M DAU, each making 10 queries/day, that’s 100M queries/day— so inference costs break even with training costs after 200 days.&lt;/p&gt;

&lt;p&gt;This is almost certainly an underestimate for their usage given how popular ChatGPT has been. If we assume 1B queries per day, it breaks even after 20 days.&lt;/p&gt;

&lt;p&gt;The various scaling law papers (&lt;a href=&quot;https://arxiv.org/abs/2001.08361&quot;&gt;OpenAI&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;) provide answers to the question of how to allocate compute between model size and dataset size. I think these papers are the &lt;em&gt;right&lt;/em&gt; way to think about training research systems, but the &lt;em&gt;wrong&lt;/em&gt; way to think about training systems that will be deployed at scale (I don&apos;t think the authors would disagree- they&apos;re solving a specific problem, namely minimizing the loss of their system given a specific compute budget, which isn&apos;t the same problem faced in deployment).&lt;/p&gt;

&lt;h3 id=&quot;llama&quot;&gt;LlaMa&lt;/h3&gt;

&lt;p&gt;Let’s look at Facebook’s &lt;a href=&quot;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&quot;&gt;new language model&lt;/a&gt; (in the second paragraph, the authors of that paper make a similar argument to the one I’m making here). If we draw a horizontal line across at any given loss level, it looks like you can tradeoff a doubling of model size for 40% more training.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/llama-training-curves.png&quot; alt=&quot;Loss curves from the LlaMa paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Look at, e.g., the line at a training loss of 1.7. The 65B model crosses it at 600B tokens, while the 33B model needs 800B tokens. Or look at a loss of 1.65: 65B needs 800B tokens, 33B needs ~1100B tokens.&lt;/p&gt;

&lt;h3 id=&quot;gpt-3&quot;&gt;GPT-3&lt;/h3&gt;

&lt;p&gt;If we look at the granddaddy of LLMs, GPT-3, we see a similar story in the loss curves: it requires roughly an order of magnitude more compute to get the green lines (GPT-3 13B) to match the yellow line (GPT-3)!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/gpt-3-loss-curves.png&quot; alt=&quot;Loss curves from the GPT-3 paper&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is important to note that the GPT-3 13B learning curves do level out earlier than GPT-3, with a rough estimate being that they would cross somewhere around the 1.8 loss area. It is also almost certainly the case that GPT-3 will achieve an asymptotically lower loss than the 13B model. Having said that, there is a question as to how much of a difference lower pre-training loss makes; I suspect that we are seeing diminishing returns kick in to pre-training, and most of the gains will come from RLHF and other types of finetuning.&lt;/p&gt;

&lt;h3 id=&quot;inference-costs&quot;&gt;Inference costs&lt;/h3&gt;

&lt;p&gt;Transformer inference costs are &lt;a href=&quot;https://kipp.ly/blog/transformer-inference-arithmetic/&quot;&gt;roughly linear&lt;/a&gt; with the number of parameters, making it ~13x cheaper to do inference with the 13B GPT-3 model than the 175B model. This is &lt;em&gt;also&lt;/em&gt; an underestimate, as the 13B model should fit it on 2 GPUs, while you would need many more just to fit the 175B model into VRAM. As scaling to multiple GPUs adds a ridiculous amount of engineering complexity, overhead, and cost, we should prefer the smaller model &lt;em&gt;even more&lt;/em&gt;. We should train the model &lt;em&gt;much&lt;/em&gt; longer to get an order of magnitude decrease in inference cost and optimize the TCO of the system.&lt;/p&gt;

&lt;p&gt;For instance, when training on multiple GPUs, it is very difficult to get high utilization numbers. The PaLM paper reported how well various LLMs did in terms of total FLOPS utilization. These are not very good numbers! Especially when each of the GPUs mentioned here costs &lt;a href=&quot;https://www.shi.com/product/41094090/NVIDIA-Tesla-A100-GPU-computing-processor&quot;&gt;$25k&lt;/a&gt;. This is despite the fact that the authors for these papers are the most experienced researchers in the world at deploying model-parallel systems, and are working on custom hardware optimimzed for this usecase. Now, training efficiency doesn&apos;t directly translate to inference efficiency, but the numbers should be directionally correct.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/static/images/palm-utilization.png&quot; alt=&quot;Model FLOPS utilization numbers for various large language models.&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;opposing-arguments&quot;&gt;Opposing arguments&lt;/h3&gt;

&lt;p&gt;Some arguments against my claim:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Aren’t we leaving performance on the table? Yes! We are. But I think that’s fine! There’s always a tradeoff here. E.g. quantization. It’s strictly worse to use lower-precision! But we do it to optimize TCO of the system.&lt;/li&gt;
  &lt;li&gt;But we can use $INSERT_TECHNIQUE to make models cheaper! Yes, but they should scale for all of these (distillation, quantization, etc.). So we should be using all techniques to make our models easier to serve, and also training them longer.&lt;/li&gt;
  &lt;li&gt;Your argument here! Please email me with your criticism and I’ll update this post.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;If you&apos;re training a LLM with the goal of deploying it to users, you should prefer training a smaller model well into the diminishing returns part of the loss curve.&lt;/p&gt;

&lt;p&gt;If you’re reading this, and you have thoughts on this, please reach out. I’m probably missing something 😊. Or- if you’re at one of these companies and this is what you do, please let me know as well.&lt;/p&gt;

&lt;p&gt;I am still looking for a job as a research engineer working with LLMs, so if this post interested you in me, let me know.&lt;/p&gt;
</description>
				<pubDate>Mon, 27 Feb 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/llms-not-trained-enough/</link>
				<guid isPermaLink="true">http://localhost:4000/llms-not-trained-enough/</guid>
			</item>
		
			<item>
				<title>A pure Python (well, Numpy) implementation of back-propagation</title>
				<description>&lt;p&gt;I realized over the weekend that, unfortunately, I didn&apos;t know how back-propagation &lt;em&gt;actually&lt;/em&gt; works (I just relied on JAX to do it for me).&lt;/p&gt;

&lt;p&gt;So I wrote a pure Numpy neural network- with back-prop. Take a &lt;a href=&quot;https://colab.research.google.com/drive/1KDSJKhZDd5fdbnLTalPKcjS_IDu0Q968#scrollTo=XmS23jQ5U7Nw&quot;&gt;look&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you have any thoughts or feedback, please shoot me an email (or reach out on Twitter).&lt;/p&gt;

&lt;p&gt;Some useful resources if you want to undersatnd how backprop works:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;https://marcospereira.me/2022/08/18/backpropagation-from-scratch/&lt;/li&gt;
  &lt;li&gt;http://neuralnetworksanddeeplearning.com/&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/karpathy/micrograd&quot;&gt;Micrograd&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/karpathy&quot;&gt;Karpathy&lt;/a&gt;&apos;s tiny ML framework.&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning Book&lt;/a&gt; was an excellent reference for the math behind backprop.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Sun, 29 Jan 2023 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/backprop/</link>
				<guid isPermaLink="true">http://localhost:4000/backprop/</guid>
			</item>
		
			<item>
				<title>Pointer Networks</title>
				<description>&lt;p&gt;Link to paper &lt;a href=&quot;https://arxiv.org/abs/1506.03134&quot;&gt;[arXiv]&lt;/a&gt;, &lt;a href=&quot;https://github.com/devsisters/pointer-network-tensorflow&quot;&gt;[code]&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Pointer networks are a new neural architecture that learns pointers to positions
in an input sequence. This is new because existing techniques need to have a
fixed number of target classes, which isn&apos;t generally applicable— consider the
Travelling Salesman Problem, in which the number of classes is equal to the
number of inputs. An additional example would be sorting a variably sized
sequence.&lt;/p&gt;

&lt;p&gt;Pointer networks uses &quot;attention as a pointer to select a member of the input.&quot;
What&apos;s remarkable is that the learnt models generalize beyond the maximum
lengths that they were trained on, with (IMO) decent results. This is really
useful because there has been a lot of work done on making it easy &amp;amp; fast to
serve deep neural network predictions, using e.g.
&lt;a href=&quot;https://www.tensorflow.org/serving/&quot;&gt;Tensorflow Serving&lt;/a&gt;. Existing solutions to
the combinatorial optimization problems discussed here are slow and expensive,
and as a result, to produce results that are anything close to real time, you
need to use heuristic models, which are inaccurate as well. The heuristics are
typically hand-tuned, just like computer vision features were 5 years ago— it&apos;s
reasonable to assume that a deep net can learn better heuristics, which will open
up combinatorial optimization and make it practical for a much wider array of
applications.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;RNNs are the most common way of approximating functions operating on sequences,
and have had a lot of success doing so. Historically, these have operated on
fixed input/output sizes, but there has been recent work (such as &lt;a href=&quot;https://www.tensorflow.org/tutorials/seq2seq&quot;&gt;seq2seq&lt;/a&gt;)
that have extended RNNs to operate on arbitrarily sized inputs and outputs.
However, these seq2seq models have still required the output to be of a fixed
size— consider a neural translation model, where the input and output are a
series of sentences. It is impossible for the model to output predictions that
involve words that the model is not aware of, which is a problem that arises
quite often (consider names, for instance).&lt;/p&gt;

&lt;p&gt;The authors introduce a new architecture, which they call Pointer Networks, that
represents variable length dictionaries using a softmax probability distribution
as a &quot;pointer&quot;. They then use the architecture in a supervised learning setting
to learn the solutions to a series of geometric algorithmic problems; they then
test the model on versions of the problems that the model hasn&apos;t seen.&lt;/p&gt;

&lt;h2 id=&quot;models&quot;&gt;Models&lt;/h2&gt;

&lt;p&gt;The model is similar to a sequence-to-sequence model in that it models the
conditional probability of an output sequence given the input sequence. Once
conditional probabilities have been learned, the model uses the trained
parameters to select the output sequence with the highest probability.Note
that this is non-trivial (to put it lightly). Given $N$ possible inputs, there
are $N!$ different output sequences. As such, the model uses a beam search
procedure to find the best possible sequence given a beam size. See below for a
discussion of beam search.&lt;/p&gt;

&lt;p&gt;As such, the model has a linear computational complexity. This is much better
than the exact algorithms for the problems solved here, which typically have
much higher complexity (e.g. the TSP has an exact algorithm that runs in
O($N^2 2^n$)).&lt;/p&gt;

&lt;p&gt;A vanilla seq-to-seq models makes predictions based on the fixed state of the
network after receiving all of the input, which restricts the amount of
information passing through the model. This has been extended by attention;
effectively, we represent the state of the encoder &amp;amp; decoder layers by
$(e_i)&lt;em&gt;{i \in {1, \ldots, n}}$ and $(d_i)&lt;/em&gt;{i \in 1, \ldots, m(\mathcal{P})}$.
Attention adds an &quot;attention&quot; vector that is calculated as&lt;/p&gt;

&lt;p&gt;\begin{align&lt;em&gt;}
u_j^i &amp;amp;= v^T \tanh(W_1 e_j + W_2 d_i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
a_j^i &amp;amp;= \text{softmax}(u_j^i), &amp;amp;j \in (1, \ldots, n)&lt;br /&gt;
d_i&apos; = \sum \limits_{j = 1}^n a_j^i e_j
\end{align&lt;/em&gt;}&lt;/p&gt;

&lt;p&gt;this can be thought of as a version of $d_i$ that has been scaled to draw
attention to the most relevant parts, according to the attention layer. $d_i$
and $d_i&apos;$ are concatenated and used as the hidden states from which predictions
are made. Adding attention increases the complexity of the model during inference
to $O(n^2)$. Note that in attention, there is a softmax distribution over a
fixed size output; to remove this constraint, the authors remove the last step
that creates the attention vector, and instead define $p(C_i | C_1, \ldots,
C_{i-1}, \mathcal{P})$ as being equal to $\text{softmax}(u^i)$.&lt;/p&gt;

&lt;h3 id=&quot;beam-search&quot;&gt;Beam search&lt;/h3&gt;

&lt;p&gt;Beam search is a heuristic search algorithm that operates on a graph. It is a
variant of breadth-first search that builds a tree of all possible sequences
based on the current tree using breadth-first search. However, instead of
storing all states, as in a traditional breadth-first search, it only stores a
predetermined number, $\beta$, of best states at each level (we call $\beta$ the
beam width). With infinite beam width, beam search is identical to breadth-first
search. Beam search is thus not guaranteed to be optimal (and one can easily
find any number of examples where beam search finds a sub-optimal output).&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;The authors use the same hyperparameters for every model, which indicates that
there&apos;s a lot of potential to improve performance for specific tasks. They
trained the model on 1M training examples. The authors find that they can get
close to optimal results on data that the model&apos;s been trained on (e.g. when the
model has been trained on TSP with 5-20 cities, they get results that have
accuracies &amp;gt;98%— more than enough for most applications.&lt;/p&gt;

&lt;p&gt;When they extend this to a cycle of length 50, the accuracy decreases, being
30% less accurate than the heuristic models. What&apos;s interesting is that the
computational complexity for the Pointer Network is at least as good as the
heuristic algorithms, and given all of the tooling surrounding deep networks,
the model should be extremely easy to put into production.&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;The results are good enough to put into production, as it shoul dbe possible to
use this for real-time predictions. However, I would be interested to see how a
reinforcement learning approach can improve the accuracy of the model (which
we&apos;ll look at in the next paper I read: &lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;Neural combinatorial optimization with
reinforcement learning&lt;/a&gt;).&lt;/p&gt;
</description>
				<pubDate>Wed, 20 Sep 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/pointer-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/pointer-networks/</guid>
			</item>
		
			<item>
				<title>Do deep networks generalise or just memorise?</title>
				<description>&lt;p&gt;There&apos;s a brilliant paper out of Google Brain &lt;a href=&quot;https://arxiv.org/abs/1611.03530&quot;&gt;1&lt;/a&gt; which claimed that DNNs just
memorise the training data, and a response &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, which claims that they don&apos;t.&lt;/p&gt;

&lt;p&gt;In the paper, the authors randomly assigned labels to MNIST and were able to
train a few deep nets to convergence (specifically, Inception, AlexNet, and a
MLP). However, performance was statistically null on the test set, as one would
expect (they correctly predicted 10% of images, which is the same as if you
randomly picked a label). The conclusion was that deep nets do do some
memorisation.&lt;/p&gt;

&lt;p&gt;However, in the same paper, the authors trained a linear model to predict MNIST
(with the true labels). The linear model had a 1.2% error, but took up 30GB of
memory. In comparison, AlexNet is roughly 250 MB in size. The linear model is
explicitly memorising the dataset, and it takes 30GB to do so, while AlexNet can
learn a similarly accurate model in &amp;lt;1% of the space (and something like
SqueezeNet &lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;3&lt;/a&gt; can do so in &amp;lt;0.5MB). As such, it seems pretty clear that there&apos;s
some true generalisation happening, as we&apos;re able to have a low error on 10 MB
of data (the size of MNIST) using 0.5MB of weights.&lt;/p&gt;

&lt;p&gt;In the response paper &lt;a href=&quot;https://openreview.net/pdf?id=rJv6ZgHYg&quot;&gt;2&lt;/a&gt;, the authors showed that &quot;DNNs trained on real data
learn simpler functions than when trained with noise data, as measured by the
sharpness of the loss function at convergence.&quot; They also showed that by using
better regularization, you can radically diminish performance on noise datasets
while maintaining performance on real datasets.&lt;/p&gt;

&lt;p&gt;I&apos;m persuaded that generalisation is happening, with the caveat that there&apos;s
some memorisation happening. The main test of the memorisation claim is that the
models are able to perform well on test sets, which goes against my prior; if
the models weren&apos;t learning &lt;em&gt;some&lt;/em&gt; generalisation, I would expect that they
wouldn&apos;t be able to perform well when it came to testing.&lt;/p&gt;

</description>
				<pubDate>Tue, 04 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/do-dnns-generalise-or-memorize/</link>
				<guid isPermaLink="true">http://localhost:4000/do-dnns-generalise-or-memorize/</guid>
			</item>
		
			<item>
				<title>Outrageously Large Neural Networks: The sparsely-gated Mixture-of-Experts layer</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://openreview.net/pdf?id=B1ckMDqlg&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The capacity of a neural network to absorb information is limited by its number of
parameters. Conditional computation, where parts of the network are active on a
per-example basis, has been proposed in theory as a way of dramatically increasing
model capacity without a proportional increase in computation. In practice,
however, there are significant algorithmic and performance challenges. In this
work, we address these challenges and finally realize the promise of conditional
computation, achieving greater than 1000x improvements in model capacity with
only minor losses in computational efficiency on modern GPU clusters. We introduce
a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to
thousands of feed-forward sub-networks. A trainable gating network determines
a sparse combination of these experts to use for each example. We apply the MoE
to the tasks of language modeling and machine translation, where model capacity
is critical for absorbing the vast quantities of knowledge available in the training
corpora. We present model architectures in which a MoE with up to 137 billion
parameters is applied convolutionally between stacked LSTM layers. On large
language modeling and machine translation benchmarks, these models achieve
significantly better results than state-of-the-art at lower computational cost.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;The paper centers around the fact that a neural net of size N requires O(N^2)
computations to execute, which is problematic, as the ability of the network to
learn data is roughly O(N). The authors propose a method to conduct conditional
computation, which is a process in which different parts of the network are
activated depending on the sample, thereby saving significant computational
effort.&lt;/p&gt;

&lt;p&gt;Their results indicate that they achieved this- they achieve SOTA results on NMT
(WMT En -&amp;gt; Fr &amp;amp; En -&amp;gt; De, Wu et. al 2016) despite much less training (1/6th of
the time).&lt;/p&gt;

&lt;p&gt;Effectively, the paper presents a way to produce strong models while
significantly reducing computational complexity.&lt;/p&gt;
</description>
				<pubDate>Sat, 01 Jul 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/outrageously-large-networks/</link>
				<guid isPermaLink="true">http://localhost:4000/outrageously-large-networks/</guid>
			</item>
		
	</channel>
</rss>

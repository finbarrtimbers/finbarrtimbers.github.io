<?xml version="1.0" encoding="UTF-8"?>
<!-- Template from here: https://github.com/diverso/jekyll-rss-feeds -->
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
		<title>Finbarr Timbers</title>
		<description>Personal website for Finbarr Timbers</description>
		<link>http://localhost:4000</link>
		<atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Attention is all you need</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art BLEU
score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the
training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;This paper is focused on .
This is a problem as&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Strong paper, effectively lays out how training deep networks can be scaled
effectively. This sort of yeoman's work is needed in the field.&lt;/p&gt;

&lt;p&gt;Concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Continues the trend of papers that rely on resources only available at a
handful of industrial labs. No academic researcher that's not affiliated with
a large tech company would be able to muster the 256 GPUs required to reproduce
this work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The amount of proprietary code required for this is a bit insane; you have to
have an infrastructure that can support the communication between GPUs required
here. Similar to my first point. Reproducibility suffers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Wed, 21 Jun 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/attention/</link>
				<guid isPermaLink="true">http://localhost:4000/attention/</guid>
			</item>
		
			<item>
				<title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://research.fb.com/wp-content/uploads/2017/06/imagenet1kin1h5.pdf?&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Deep learning thrives with large neural networks and large datasets. However,
larger networks and larger datasets result in longer training times that impede
research and development progress. Distributed synchronous SGD offers a
potential solution to this problem by dividing SGD minibatches over a pool of
parallel workers. Yet to make this scheme efficient, the per-worker workload
must be large, which implies nontrivial growth in the SGD minibatch size. In
this paper, we empirically show that on the ImageNet dataset large minibatches
cause optimization dif- ficulties, but when these are addressed the trained
networks exhibit good generalization. Specifically, we show no loss of accuracy
when training with large minibatch sizes up to 8192 images. To achieve this
result, we adopt a linear scaling rule for adjusting learning rates as a
function of minibatch size and develop a new warmup scheme that overcomes
optimization challenges early in training. With these simple techniques, our
Caffe2-based system trains ResNet- 50 with a minibatch size of 8192 on 256 GPUs
in one hour, while matching small minibatch accuracy. Using commodity hardware,
our implementation achieves ∼90% scaling efficiency when moving from 8 to 256
GPUs. This system enables us to train visual recognition models on internetscale
data with high efficiency.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;This paper is focused on parallelizing model training across multiple GPUs. This
is a problem as it is currently typically quite difficult to get reasonable
speedups when using multiple GPUs to train a model (by difficult, I mean that
you get significantly less than linear speedups).&lt;/p&gt;

&lt;p&gt;In this paper, by Facebook Research, the authors are able to get &lt;em&gt;almost&lt;/em&gt; linear
speedups moving from 8 to 256 GPUs (0.9x), which is quite good. Using 256 GPUs,
the authors are able to train the ResNet-50 model in 1 hour (to give you an
idea of how impressive this is, the ImageNet dataset consists of 750 GB of
data). AlexNet, which was the breakthrough paper that was a large
contributor to Deep Learning's current popularity, took 5-6 days to train for 90
epochs on two NVIDIA GTX 580s, which is equivalent to 288 GPU hours &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&quot;&gt;1&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The trick that they use to do this is to use a large minibatch size consisting
of 8192 images. Using a large minibatch size makes it much easier to fully
exploit the GPUs, and hence makes training run faster, however, it makes each
gradient update noisier, making the training potentially take longer to
converge (or making it so that training might converge to a &lt;em&gt;wrong&lt;/em&gt;,
i.e. non-optimal, answer). Additionally, if you are using multiple GPUs, you
have to synchronize the weights after each minibatch update, so having smaller
minibatches causes the required communication overhead to drastically increase.&lt;/p&gt;

&lt;p&gt;To compensate for the noise introduced by the large minibatch size used here,
the authors used a &quot;Linear Scaling Rule&quot;, where they multiplied the learning
rate by &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; when they used a minibatch size of &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;. This allowed the authors
to match the accuracy between small and large minibatches.&lt;/p&gt;

&lt;p&gt;The authors note that the linear scaling rule is nice theoretically as, after
&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; iterations of SGD with a learning rate of &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; and a minibatch of
&lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt;, the weight vector is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
w_{t+k} = w_t - \eta \frac{1}{n} \sum \limits_{j &lt; k} \sum
\limits_{x \in \mathcal{B}_j} \grad l(x, w_{t + j}) %]]&gt;&lt;/script&gt;

&lt;p&gt;when taking a single step with a minibatch &lt;script type=&quot;math/tex&quot;&gt;\cup_j \mathcal{B}_j&lt;/script&gt; of size
&lt;script type=&quot;math/tex&quot;&gt;kn&lt;/script&gt; and learning rate &lt;script type=&quot;math/tex&quot;&gt;\eta'&lt;/script&gt;, the weight vector is instead&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
w_{t+1}' = w_t - \eta' \frac{1}{n} \sum \limits_{j &lt; k} \sum
\limits_{x \in \mathcal{B}_j} \grad l(x, w_t), %]]&gt;&lt;/script&gt;

&lt;p&gt;which is different. However, if &lt;script type=&quot;math/tex&quot;&gt;\Delta l(x, w_t)&lt;/script&gt; is close in value to
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\Delta l(x, w_{t+j})$ for $j &lt; k$, then setting %]]&gt;&lt;/script&gt;\eta' = kn&lt;script type=&quot;math/tex&quot;&gt;makes it so
that&lt;/script&gt;w_{t+1} \approx w_{t+k}$$ (I would imagine that you could formalize this
with an epsilon-delta proof). The paper notes that the two updates can only
be similar when the linear scaling rule is used; in other words, the linear
scaling rule is necessary, but not sufficient.&lt;/p&gt;

&lt;p&gt;The authors note that the assumption that the two gradients are similar doesn't
hold during the initial training, when the weights are rapidly changing, and
that the results hold only for a large, but finite, range of minibatch sizes
(which for ImageNet is as large as 8192). The authors use a &quot;warmup&quot; phase to
mitigate the problems with divergence during initial training, where the model
uses less aggressive learning rates, and then switches to the linear scaling
rule after 5 epochs. That didn't work, and instead, they used a gradual warmup
that brought the learning rate to increase at a constant rate per iteration
so that it reached &lt;script type=&quot;math/tex&quot;&gt;\eta' = k \eta&lt;/script&gt; after 5 epochs, which worked better.&lt;/p&gt;

&lt;p&gt;The authors then go on to discuss results, namely that they were able to train
ResNet-50 in one hour while still getting state of the art results.&lt;/p&gt;

&lt;p&gt;What's novel about this is the size of the parallelization; presumably there's
nothing special about using 256 GPUs, and if someone had the resources available
(&lt;em&gt;cough&lt;/em&gt; Google &lt;em&gt;cough&lt;/em&gt;), one could scale this further. Given that GPUs seem to
be following Moore's law and doubling in performance every 18 months, this paper
seems to be important; if we can train a state of the art model in one hour
using 256 GPUs today, then within 3 years, we could train one in 15 minutes. If
someone wanted to scale the number of GPUs higher, they could train the model in
under 10 minutes.&lt;/p&gt;

&lt;p&gt;Conversely, researchers currently tolerate several weeks of
training time (Mozilla's implementation of
&lt;a href=&quot;https://github.com/mozilla/DeepSpeech&quot;&gt;Baidu's DeepSpeech&lt;/a&gt; model takes
Mozilla roughly 10 days to train on 4 high end GPUs &lt;a href=&quot;https://github.com/mozilla/DeepSpeech/issues/630&quot;&gt;2&lt;/a&gt;); if the amount of model
that can be trained in that time drastically increases, all of a sudden
researchers are able to consider datasets that are radically larger, and can
start approaching even higher performance levels.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Strong paper, effectively lays out how training deep networks can be scaled
effectively. This sort of yeoman's work is needed in the field.&lt;/p&gt;

&lt;p&gt;Concerns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Continues the trend of papers that rely on resources only available at a
handful of industrial labs. No academic researcher that's not affiliated with
a large tech company would be able to muster the 256 GPUs required to reproduce
this work.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The amount of proprietary code required for this is a bit insane; you have to
have an infrastructure that can support the communication between GPUs required
here. Similar to my first point. Reproducibility suffers.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

</description>
				<pubDate>Tue, 20 Jun 2017 00:00:00 -0600</pubDate>
				<link>http://localhost:4000/imagenet-1-hour/</link>
				<guid isPermaLink="true">http://localhost:4000/imagenet-1-hour/</guid>
			</item>
		
			<item>
				<title>Tests make you write down your assumptions</title>
				<description>&lt;p&gt;I've been fighting with recurring errors the whole time I've been working on
&lt;a href=&quot;http://www.bugdedupe.com&quot;&gt;BugDedupe&lt;/a&gt;. I keep changing some aspect of the
frontend, and inadvertently break the site. The way to prevent this, of course,
is by having a comprehensive test suite. I know that I should have tests, and I do, but not
at anywhere near the coverage that I need (I'm currently at 29% code
coverage).&lt;/p&gt;

&lt;p&gt;The reason for the abysmal amount of code coverage is that I don't know how to
write the tests that I need. For instance, to test that BugDedupe is merging
rows correctly, I need to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Set up a test database.&lt;/li&gt;
  &lt;li&gt;Set up a test Github account.&lt;/li&gt;
  &lt;li&gt;Mock the &lt;code class=&quot;highlighter-rouge&quot;&gt;POST&lt;/code&gt; request for Flask.&lt;/li&gt;
  &lt;li&gt;Figure out how Flask's app environments work so that I can get the correct
context for &lt;code class=&quot;highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; and the &lt;code class=&quot;highlighter-rouge&quot;&gt;user&lt;/code&gt; objects that are used throughout the routes.
&lt;code class=&quot;highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;user&lt;/code&gt; are objects that can be called at any point in the application
context for Flask without you having to explicitly set &lt;code class=&quot;highlighter-rouge&quot;&gt;user = ...&lt;/code&gt;. This is
good- it makes it really easy to use them- but it's bad as I don't &lt;em&gt;really&lt;/em&gt;
know how they work.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The reason I've been avoiding writing the tests is that it's really difficult
to write tests when you don't know &lt;em&gt;exactly&lt;/em&gt; what your code is doing, and you
don't have a clear understanding of how the framework you're using works.
However, it turns out that it's really difficult to write code that works
correctly when you don't have a clear understanding of how your framework works.
So I'm taking the time to figure out exactly what's going on, and so far, it's
definitely been worth it.&lt;/p&gt;

&lt;p&gt;For instance, I found out how &lt;code class=&quot;highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; works- it turns out that Flask creates
multiple &lt;a href=&quot;http://flask.pocoo.org/docs/0.12/appcontext/&quot;&gt;contexts&lt;/a&gt; that store
data that are needed on a per-request basis. &lt;code class=&quot;highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; stores data on the
application context, so it makes data available to different functions
during one request. It's effectively a super-global variable. You can't just use
a global variable to replace &lt;code class=&quot;highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; as then it would break in threaded
environments, which are necessary when you're trying to serve many users. That's
cool. I wouldn't have learned that today if I hadn't been writing tests
that needed to call &lt;code class=&quot;highlighter-rouge&quot;&gt;flask.g&lt;/code&gt; and store data there. I would only have learned it
when I introduced some nasty bug.&lt;/p&gt;

&lt;p&gt;In short, if you don't know exactly what's going on in your code, then you
should write tests and formalize your knowledge.&lt;/p&gt;
</description>
				<pubDate>Sun, 05 Mar 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/testing/</link>
				<guid isPermaLink="true">http://localhost:4000/testing/</guid>
			</item>
		
			<item>
				<title>Random Search for Hyper-Parameter Optimization</title>
				<description>&lt;h1 id=&quot;abstract&quot;&gt;&lt;a href=&quot;http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf&quot;&gt;Abstract&lt;/a&gt;&lt;/h1&gt;

&lt;p&gt;Grid search and manual search are the most widely used strategies for hyper-parameter optimization.
This paper shows empirically and theoretically that randomly chosen trials are more efficient
for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison
with a large previous study that used grid search and manual search to configure neural networks
and deep belief networks. Compared with neural networks configured by a pure grid search,
we find that random search over the same domain is able to find models that are as good or better
within a small fraction of the computation time. Granting random search the same computational
budget, random search finds better models by effectively searching a larger, less promising con-
figuration space. Compared with deep belief networks configured by a thoughtful combination of
manual search and grid search, purely random search over the same 32-dimensional configuration
space found statistically equal performance on four of seven data sets, and superior performance
on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation
set performance reveals that for most data sets only a few of the hyper-parameters really matter,
but that different hyper-parameters are important on different data sets. This phenomenon makes
grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some
light on why recent “High Throughput” methods achieve surprising success—they appear to search
through a large number of hyper-parameters because most hyper-parameters do not matter much.
We anticipate that growing interest in large hierarchical models will place an increasing burden on
techniques for hyper-parameter optimization; this work shows that random search is a natural baseline
against which to judge progress in the development of adaptive (sequential) hyper-parameter
optimization algorithms.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A learning algorithm $\mathcal{A}$ can be thought of as a functional (a function
that operates on functions) that maps a data set $\mathcal{X}^{\text{train}}$ to
a function $f$. We can think of $\mathcal{A}$ as a function itself, and write
it as $\mathcal{A}(\mathcal{X}^{\text{train}}, \lambda)$, where $\lambda$ is a
vector of so-called &quot;hyper-parameters&quot;, which change how the algorithm operates.
An example is $\alpha$, the $L_1$ penalty in
&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html&quot;&gt;Lasso&lt;/a&gt;.
Finding $\lambda$ is called the &quot;hyper-parameter optimization problem&quot;, which
consists of finding $\lambda^\star$ that minimizes the expected error of the
algorithm over the set of all possible training sets. Said expected error
cannot be characterized, and as a result, solutions to the hyper-parameter
optimization problem take two forms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The manual approach, where a researcher tries a number of different
parameters and uses the best one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Grid search, where all of the different combinations of parameters are tried.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Approach 2 is guaranteed to find the optimal combination, but it is extremely
computationally expensive, growing at a rate of O($p^n$), where $p$ is the
number of different values each parameter can take, and $n$ is the number of
different parameters. Typically, manual search is used to minimize the number
of possible values that each parameter can take, and then a grid search is
performed over the remaining values.&lt;/p&gt;

&lt;p&gt;Manual search has advantages and disadvantages; on the one hand, it can work
well, and it can give researchers insight into how the algorithm works. On the
flip side, it's not reproducible, and has no guarantee of success, particularly
in higher dimension spaces.&lt;/p&gt;

&lt;p&gt;Consequently, the authors present a randomized variant of grid search that
randomly searches the space of all possible hyper-parameters. Random search
ends up being more practical than grid search as it can be applied using a
cluster of unreliable computers, and new trials can easily be added to the
search as all trials are i.i.d.&lt;/p&gt;

&lt;h2 id=&quot;random-vs-grid-for-neural-networks&quot;&gt;Random vs. Grid for neural networks&lt;/h2&gt;

&lt;p&gt;This part of the paper is heavily inspired by Larochelle (2007) &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1273556&quot;&gt;1&lt;/a&gt;.
The authors use a variety of classification datasets, including a number of
variants of MNIST, to perform hyper-parameter optimization on a series of
neural networks.The authors note that the variation of the hyper-parameter
optimization varies significantly with the datasets; for MNIST basic,
experiments with 4 or 8 trials often had the same performance as much bigger
trials, while even with 16 or 32 trials, MNIST rotated background images were
still exhibiting significant variation.&lt;/p&gt;

&lt;p&gt;The authors use these results to note that in many cases, the effective
dimensionality of $\psi$ ,the hyper-parameter space, is much lower than the
possible dimensionality of $\psi$. In other words, many of the parameters only
have a small number of possible values that are useful.&lt;/p&gt;

&lt;h2 id=&quot;random-vs-sequential-manual-optimization&quot;&gt;Random vs. sequential manual optimization&lt;/h2&gt;

&lt;p&gt;The authors discuss an experiment by &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1273556&quot;&gt;1&lt;/a&gt; comparing randomized grid search with having a
researcher conduct a sequential manual search. The authors quote &lt;a href=&quot;https://dl.acm.org/citation.cfm?id=1273556&quot;&gt;1&lt;/a&gt; on how to
effectively conduct sequential manual optimization, which is quite insightful.
The setting used in the experiment is one with 32 different hyper parameters,
which, if each parameter had two possible values, would create a parameter space
with $2^32$ members- far too large to evaluate with a grid search. In the
experiment, random search performed well, but not as well as with the neural
networks, finding a better model than manual search in 1 data set, an equally
good model in 4 data sets, and an inferior model in 3 data sets.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The authors suggest using randomized search instead of grid search in almost
every scenario, noting that although more complicated approaches are better
(e.g. adaptive search algorithms), they're more complicated, while a randomized
grid search is a much cheaper way of evaluating more of the search space. The
randomized search, similar to the grid search, is trivially parallelizable, and
can be scaled much more rapidly than an adaptive search, and can stopped,
started, and scaled without difficulty.&lt;/p&gt;

&lt;h1 id=&quot;comments&quot;&gt;Comments&lt;/h1&gt;

&lt;p&gt;The paper makes a lot of sense, and I'm persuaded to swap out grid search for a
randomized search in my own work. I'd like to see some sort of sequential
randomized grid search that works iteratively, alternating between performing a
randomized grid search over a subset of the parameter space, and then selecting
a new, smaller subset to search over (in effect, performing gradient descent
over the parameter space). Perhaps that exists and I need to find a paper
discussing that.&lt;/p&gt;

</description>
				<pubDate>Wed, 01 Mar 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/random-search-hyper-parameter-optimization/</link>
				<guid isPermaLink="true">http://localhost:4000/random-search-hyper-parameter-optimization/</guid>
			</item>
		
			<item>
				<title>How I designed my machine learning app</title>
				<description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Approximately ten thousand years ago, during undergrad, I made what was probably
the best decision of my academic career and took
&lt;a href=&quot;https://webdocs.cs.ualberta.ca/~greiner/&quot;&gt;Russ Greiner's&lt;/a&gt; CMPUT 466 class at
the University of Alberta. As a math major, it took some convincing to let the
CS department allow me into the course, as I was missing approximately every
prerequisite for the class, but convince them I did, and I was introduced to the
body of techniques known as machine learning. The course required me to complete
a group project, and my group worked on the problem of bug report deduplication.&lt;/p&gt;

&lt;h2 id=&quot;inspiration&quot;&gt;Inspiration&lt;/h2&gt;

&lt;p&gt;Managing the flood of bug reports that pour in is a problem for any sizable
software project. Android has some
&lt;a href=&quot;https://code.google.com/p/android/issues/list?num=100&amp;amp;start=0&quot;&gt;53 636&lt;/a&gt;  bug
reports as of the time of writing, and if you search &quot;bug report triage&quot; in
&lt;a href=&quot;https://scholar.google.ca/scholar?q=bug+report+triage&amp;amp;btnG=&amp;amp;hl=en&amp;amp;as_sdt=0%2C5&quot;&gt;Google Scholar&lt;/a&gt;,
you get almost 4000 results for academic work. The cost of managing bug reports
is huge, too— if each bug report takes a quarter of an hour to administer (read,
compare to previous bug reports, link them, and assign to a developer), then the
Android community has spent 13 409 hours, or 32 work years managing bug reports.
That ignores the number of internal bug reports that Google's QA staff would
have found and reported. If we assume a typical Software Engineer costs
$100 000 annually, that's $3.2 million that the Android community has dedicated
to bug triage.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://finbarr.ca/dedup/&quot;&gt;project&lt;/a&gt; that my team worked on focused on
analysing the text of the bug reports and using features generated from those to
classify the bug reports. We developed a series of features that used reference
material (textbooks, manuals, documentation) to create subject specific word
lists, from which we generated numerical comparison scores. From this, we were
able to get a series of subject scores (e.g. Android security: 0.2). The work
proved remarkably successful, almost matching the performance of word lists that
were manually extracted. With very little tuning, we were able to correctly
classify 97% of bug reports, and I suspect that with some modelling effort
(hyperparameter optimization, investigating some more complicated models) that
could be improved on.&lt;/p&gt;

&lt;p&gt;That was several years ago. The code that performed the bug report deduplication has
been sitting around on Github for a few years, untouched. It seemed to me that
bug report deduplication was a problem with a clear solution that hadn't been
implemented by anyone. I've been looking for a side project to do that will let
me learn more about product development and deploying machine learning in
production environments, instead of just academic experimental settings, so I
took it upon myself to turn the academic code into a web app. I was able to
recruit a few of my friends to help, and we built
&lt;a href=&quot;http://www.bugdedupe.com&quot;&gt;BugDedupe&lt;/a&gt;. It's still in a very early stage, so I'm
looking for feedback and feature requests. The goal is to have the site be free
for open source repositories and small (private) side projects, charging larger
repositories to recoup my costs. If you have feedback or feature requests, or
just want to chat, shoot me an email at finbarrtimbers@gmail.com.&lt;/p&gt;

&lt;p&gt;When I started building BugDedupe, I hadn't seen many posts about how teams
designed their machine learning web apps, so I wanted to write about how we
approached ours, to get feedback and to help others doing the same.&lt;/p&gt;

&lt;h2 id=&quot;myself&quot;&gt;Myself&lt;/h2&gt;

&lt;p&gt;I'm a stats/ML guy who does a lot of number crunching in Python at my day job
(Numpy/sklearn/SciPy). I've been wanting to learn more about the whole Python
stack, and particularly web dev, so that I can learn how to make products from
end-to-end. As a consultant, I get all of my projects to the MVP stage and then
have to start again on the next one for a new client. I wanted to work on
something that I could polish and grow, and given my stats background,
BugDedupe seemed like a great opportunity.&lt;/p&gt;

&lt;h2 id=&quot;background&quot;&gt;Background:&lt;/h2&gt;

&lt;p&gt;We developed an automated &lt;a href=&quot;http://finbarr.ca/dedup&quot;&gt;method&lt;/a&gt; of predicting
whether or not two bug reports are duplicates of each other. We did this by
analysing the text of the bug report and comparing it to each other, and to
reference texts (e.g. we had bug reports for a Java project, and we compared
them to different chapters of a Java textbook to get subject scores— allowing
us to say that a given report is 30\% cryptography and 45\% networking). This
gave us a number of features that we could run through a machine learning
classifier. We used a number of different ones and got really high results—
97\%. The method worked, and we tested it on real world data— the bug reports
from Android, and Eclipse, among others. The only remaining problem was figuring
out how to make the service available online.&lt;/p&gt;

&lt;h2 id=&quot;layout&quot;&gt;Layout&lt;/h2&gt;

&lt;p&gt;At the start of the project, I was pretty confused about how to develop the
site. I've been a fan of functional programming for a long time, and try to
develop all of my projects in a functional manner.In that light, I decided to
use a stateless architecture for the app. All of the state of the app (users,
data, etc.) would be stored in the MySQL database, and the server would exist
only to render it onto the web; similarly, the machine learning processes would
interact uniquely with the database. As a result, we can have anything except
the database crash at any time, and we won't lose any data. The database is
regularly backed up, and as we're using Google Cloud SQL, it's in good hands.
If, god willing, we run into scaling problems, we feel that our architecture will
also allow us to focus only on optimizing the specific parts that are
bottlenecking our performance, as everything is logically separated.&lt;/p&gt;

&lt;h2 id=&quot;hosting&quot;&gt;Hosting&lt;/h2&gt;

&lt;p&gt;I've been using Docker at work and like it a lot. I decided to encapsulate each
separate component in a container, and run them on Google Cloud Platform, as
I like what Google's doing with &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;. Once I
had the components in Docker, it was straightforwardt to launch them on Google
Container Engine.&lt;/p&gt;

&lt;p&gt;I found it surprisingly easy. It sucked getting started, but now that
everything's set up, it's super easy to work with, and kubernetes makes a lot of
the administrative tasks go away (e.g. managing secrets/environment variables,
restarting Python, scaling).&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;While some things surprised me (particularly the latency of APIs- I was using
Stormpath for authentication, and it started adding up to 6 seconds per
request), overall, I'm extremely happy with how the program turned out. I think
that having everything interact only with the database and itself managed to
reduce complexity significantly, so that I only had to think about how the
current component interacted with the database, instead of having to worry about
how it fit in with all of the other components of the stack. The test now is to
get more users, and see how we perform at scale. The more users we get, the
more accurate our classifier will be, and the more useful our service will be,
so we need to get that flywheel rolling as fast as we can.&lt;/p&gt;

&lt;p&gt;If you found this useful, or if you have any feedback, please send me an email
at finbarrtimbers@gmail.com. I'd love to hear what you think of our
architecture.&lt;/p&gt;
</description>
				<pubDate>Tue, 21 Feb 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/Bugdedupe/</link>
				<guid isPermaLink="true">http://localhost:4000/Bugdedupe/</guid>
			</item>
		
			<item>
				<title>Securing yourself against online tracking</title>
				<description>&lt;p&gt;I've had a few people email me asking how they can protect themselves online,
and what combination of tools they should use. I thought I'd write up my advice
here so that others can beenfit.&lt;/p&gt;

&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;/h1&gt;

&lt;p&gt;Online security is weird. You start taking reasonable
measures, and all of a sudden, you're wearing a tinfoil hat and air-gapping
all of your computers. You can go crazy with security, so it's important to
be realistic about the types of threats that you face, and choose security
options based on the threats you're worried about.&lt;/p&gt;

&lt;p&gt;There's some stuff that you definitely &lt;em&gt;should&lt;/em&gt; do, and then after that, you
start getting deeper and deeper into paranoia territory. How deep you want to
go depends on how worried you are (and remember— you're not paranoid if they
really &lt;em&gt;are&lt;/em&gt; out to get you).&lt;/p&gt;

&lt;p&gt;You should definitely do three things:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Install uBlock Origin &amp;amp; Privacy Badger.&lt;/li&gt;
  &lt;li&gt;Use a password manager like LastPass to generate secure passwords.&lt;/li&gt;
  &lt;li&gt;Encrypt all of your hard drives.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After that, it depends on how worried you are. If you're concerned that people
might be trying to log into your accounts, or access your computers illicitly,
you should look at installing two-factor authentication on your email accounts,
password manager, and computers.&lt;/p&gt;

&lt;h2 id=&quot;ublock-origin&quot;&gt;uBlock Origin&lt;/h2&gt;

&lt;p&gt;uBlock Origin is an ad blocker that's super lightweight, and just works (there's
been some weird controversies about ad blockers… Google 'Adblock Plus' if
you're interested). It blocks requests by default, so that, e.g., Google
Analytics can't track your visits to sites. This might break some sites, but you
can disable it for those sites.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/ublock-origin/cjpalhdlnbpafiamejdnhcphjbkeiagm?hl=en&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;privacy-badger&quot;&gt;Privacy Badger&lt;/h2&gt;

&lt;p&gt;Privacy Badger is a tool made by the EFF (a non-profit that focuses on digital
rights) that blocks spying ads and invisible trackers. This would, for instance,
block the images that BananaTags uses to track forwards from loading.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://chrome.google.com/webstore/detail/privacy-badger/pkehgijcmpdhfbdbbnkijodmdjhbjlgp&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;password-managers&quot;&gt;Password managers&lt;/h2&gt;

&lt;p&gt;Password Managers are tools that manage your passwords for you. This allows you
to come up much more complex passwords that are randomly generated, and to share
them securely (no more emailing passwords, or sharing a Google Sheet).&lt;/p&gt;

&lt;p&gt;LastPass is the one that I'm most familiar with, and it has a free tier that
satisfies most of your needs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.lastpass.com/&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;less-crucial-security-options&quot;&gt;Less crucial security options&lt;/h1&gt;

&lt;h2 id=&quot;two-factor-authentication-2fa&quot;&gt;Two Factor Authentication (2FA):&lt;/h2&gt;

&lt;p&gt;2FA is a system where you are required to prove your identity in a second,
independent way. As a result, having 2FA enabled guarantees that merely having
your password isn't enough to access your account. If John Podesta had 2FA
enabled, Russia wouldn't have been able to hack his emails.&lt;/p&gt;

&lt;p&gt;You can enable 2FA on Gmail, Lastpass, and to login to Windows. There are
multiple ways of using 2FA:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Have a code sent by SMS to your phone&lt;/li&gt;
  &lt;li&gt;Use an app on your phone that generates codes&lt;/li&gt;
  &lt;li&gt;Use a physical stick that generates a code (so you don't have to type
anything- just push a button).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I'm currently demoing a Yubikey (one of the physical 2FA keys) for work; I'll
update this with a review afterwards.&lt;/p&gt;

&lt;h2 id=&quot;iba-opt-out&quot;&gt;IBA Opt-out&lt;/h2&gt;

&lt;p&gt;IBA Opt-out opts you out of Google's interest-based ads, which makes it so that
other sites can't track you through the ads. This is Google only, so it's less
useful than the other tools.&lt;/p&gt;

&lt;p&gt;https://chrome.google.com/webstore/detail/iba-opt-out-by-google/gbiekjoijknlhijdjbaadobpkdhmoebb?hl=en&lt;/p&gt;

&lt;h2 id=&quot;https-everywhere&quot;&gt;HTTPS Everywhere&lt;/h2&gt;

&lt;p&gt;Also by the EFF, HTTPS Everywhere makes it so that all of your pages use HTTPS
if supported. It's useful to defend against sites that try to inject ads or
unwanted material (e.g. on Greyhound, the bus wifi automatically takes over the
ads). Unsecured connections can be used for man-in-the-middle (MITM) attacks,
where a page steals your data by snooping on your traffic. That shouldn't be a
problem as every site you use should use HTTPS, but there's no harm in using
HTTPS.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.eff.org/https-everywhere%20&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 04 Feb 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/defense/</link>
				<guid isPermaLink="true">http://localhost:4000/defense/</guid>
			</item>
		
			<item>
				<title>Useful Bash One-liners</title>
				<description>&lt;p&gt;I have a file in my home folder that contains Bash oneliners that I use
regularly (I'm a huge nerd, naturally).  I found most of them elsewhere online;
I wrote very few of these from scratch.&lt;/p&gt;

&lt;h2 id=&quot;download-a-page-and-all-linked-pagesdocuments&quot;&gt;Download a page and all linked pages/documents:&lt;/h2&gt;

&lt;p&gt;Download &lt;code class=&quot;highlighter-rouge&quot;&gt;$PAGE&lt;/code&gt; and all linked pages/documents, to a depth of &lt;code class=&quot;highlighter-rouge&quot;&gt;$NUM&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget -r &quot;$NUM&quot; &quot;$PAGE&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;e.g.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ wget -r 1 \
    https://courses.cs.washington.edu/courses/cse455/14au/notes/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you only want PDF files (e.g. if you're downloading course notes), then you
can add the flag &lt;code class=&quot;highlighter-rouge&quot;&gt;---accept &quot;*.pdf&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Taken from &lt;a href=&quot;http://superuser.com/questions/274414/how-to-save-all-the-webpages-linked-from-one&quot;&gt;Stack Overflow&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;recursively-unrar-files&quot;&gt;Recursively unrar files&lt;/h2&gt;

&lt;p&gt;You can replace &lt;code class=&quot;highlighter-rouge&quot;&gt;unrar e&lt;/code&gt; with any other command as well (e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;unzip&lt;/code&gt;).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ find ./ -name '*.rar' -execdir unrar e {} \;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;As a data scientist, I often get a dump of data from a client. This command lets
me process them all at once.&lt;/p&gt;

&lt;h2 id=&quot;turn-white-backgrounds-transparent&quot;&gt;Turn white backgrounds transparent&lt;/h2&gt;

&lt;p&gt;I use this ALL THE TIME when I'm giving talks (particularly when I'm teaching). I
found it on the
&lt;a href=&quot;http://www.imagemagick.org/discourse-server/viewtopic.php?t=12619&quot;&gt;Imagemagick&lt;/a&gt;
forums.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ convert image.gif -transparent white result.gif (or use result.png)
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Alternately, if the image has an off-white background:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ convert image.gif -fuzz XX% -transparent white result.gif
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;where the smaller the %, the closer to true white or conversely, the larger the
%, the more variation from white is allowed to become transparent.&lt;/p&gt;

&lt;h2 id=&quot;diff-contents-of-two-folders&quot;&gt;Diff contents of two folders&lt;/h2&gt;

&lt;p&gt;Checks which files are different between the folders &lt;code class=&quot;highlighter-rouge&quot;&gt;dir1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;dir2&lt;/code&gt;. I've
used this to track down bugs when I'm installing our software on client sites
to make sure that their data is exactly the same as my copy of it.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ diff -qr dir1 dir2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can also use some sort of checksum by zipping up both folders and comparing
the results, e.g. with&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ md5 dir1.zip
$ md5 dir2.zip
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
				<pubDate>Fri, 20 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/Useful-oneliners/</link>
				<guid isPermaLink="true">http://localhost:4000/Useful-oneliners/</guid>
			</item>
		
			<item>
				<title>Including web fonts in RMarkdown</title>
				<description>&lt;p&gt;I didn't see this anywhere online, so I thought I'd quickly write up how to add
web fonts to a RMarkdown presentation.&lt;/p&gt;

&lt;p&gt;You have a RMarkdown presentation using ioslides:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
title: &quot;Best Presentation Ever&quot;
author: &quot;Finbarr Timbers&quot;
date: &quot;January 19, 2017&quot;
output: ioslides_presentation
css: assets/styles.css
logo: assets/logo.png
incremental: true
widescreen: true
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Create a file &lt;code class=&quot;highlighter-rouge&quot;&gt;header.html&lt;/code&gt; including a link to the fonts you want to use in the
same folder as your RMarkdown document&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;link href=&quot;https://fonts.googleapis.com/css?family=Open+Sans&quot; rel=&quot;stylesheet&quot;&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Then, add an &lt;code class=&quot;highlighter-rouge&quot;&gt;includes&lt;/code&gt; section to the header of your RMarkdown document:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
title: &quot;Best Presentation Ever&quot;
author: &quot;Finbarr Timbers&quot;
date: &quot;January 19, 2017&quot;
output:
    ioslides_presentation:
        includes: header.html
css: assets/styles.css
logo: assets/logo.png
incremental: true
widescreen: true
---
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You're done! Should work properly.&lt;/p&gt;
</description>
				<pubDate>Thu, 19 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/web-fonts-rmd/</link>
				<guid isPermaLink="true">http://localhost:4000/web-fonts-rmd/</guid>
			</item>
		
			<item>
				<title>A Deep Hierarchical Approach to Lifelong Learning in Minecraft</title>
				<description>&lt;h2 id=&quot;abstract&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1604.07255&quot;&gt;Abstract&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;We propose a lifelong learning system that has the ability to reuse and transfer
knowledge from one task to another while efficiently retaining the previously
learned knowledge-base. Knowledge is transferred by learning reusable skills to
solve tasks in Minecraft, a popular video game which is an unsolved and
high-dimensional lifelong learning problem. These reusable skills, which we
refer to as Deep Skill Networks, are then incorporated into our novel
Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture using two
techniques: (1) a deep skill array and (2) skill distillation, our novel
variation of policy distillation (Rusu et. al. 2015) for learning skills. Skill
distillation enables the HDRLN to efficiently retain knowledge and therefore
scale in lifelong learning, by accumulating knowledge and encapsulating multiple
reusable skills into a single distilled network. The H-DRLN exhibits superior
performance and lower learning sample complexity compared to the regular Deep Q
Network (Mnih et. al. 2015) in sub-domains of Minecraft.&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;Paper discusses how previously trained neural networks (called skills) can be
used to create an adaptable, more general AI that can respond to varied
scenarios.&lt;/p&gt;
</description>
				<pubDate>Tue, 03 Jan 2017 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/minecraft/</link>
				<guid isPermaLink="true">http://localhost:4000/minecraft/</guid>
			</item>
		
			<item>
				<title>Larry Ellison on consulting costs</title>
				<description>&lt;p&gt;I'm currently reading &lt;a href=&quot;https://www.amazon.ca/Softwar-Intimate-Portrait-Ellison-Oracle/dp/0743225058&quot;&gt;Softwar&lt;/a&gt;,
a book about Oracle's rise. The book is brilliant, and it descibes at length
Larry Ellison's sales process. There was a passage describing a meeting that
Larry had that explains far more about enterprise sales than it should:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/larry-ellison-softwar-academic.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Tue, 06 Dec 2016 00:00:00 -0700</pubDate>
				<link>http://localhost:4000/softwar-academic/</link>
				<guid isPermaLink="true">http://localhost:4000/softwar-academic/</guid>
			</item>
		
	</channel>
</rss>
